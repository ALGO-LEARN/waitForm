{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextCNN_20220506.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCoO1lP38GHm",
        "outputId": "c52adeb7-9f73-49b7-e274-3cc4a856ff71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3262\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "final_data = pd.read_csv(\"./words_data.csv\", encoding='utf-8')\n",
        "final_data = final_data = final_data.loc[:,['header', 'detail']]\n",
        "final_data\n",
        "\n",
        "test_data = pd.read_csv(\"./programmers_data.csv\", encoding='utf-8')\n",
        "print(len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 본 코드\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "from tensorflow import flags\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cnn_tool as tool"
      ],
      "metadata": {
        "id": "vrj4VwOD9aY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(sys.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRcr4eDI9cSy",
        "outputId": "3b908984-cbd5-4bea-c25e-fb2c6654ad91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.7.13 (default, Apr 24 2022, 01:04:09) \n",
            "[GCC 7.5.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow==1.15"
      ],
      "metadata": {
        "id": "Puw2zdkf9hY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install flags"
      ],
      "metadata": {
        "id": "wYkKBpnF904s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextCNN(object):\n",
        "    \"\"\"\n",
        "    A CNN for text classification.\n",
        "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
        "    <Parameters>\n",
        "        - sequence_length: 최대 문장 길이\n",
        "        - num_classes: 클래스 개수\n",
        "        - vocab_size: 등장 단어 수\n",
        "        - embedding_size: 각 단어에 해당되는 임베디드 벡터의 차원\n",
        "        - filter_sizes: convolutional filter들의 사이즈 (= 각 filter가 몇 개의 단어를 볼 것인가?) (예: \"3, 4, 5\")\n",
        "        - num_filters: 각 filter size 별 filter 수\n",
        "        - l2_reg_lambda: 각 weights, biases에 대한 l2 regularization 정도\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self, sequence_length, num_classes, vocab_size,\n",
        "            embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
        "        # Placeholders for input, output and dropout\n",
        "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
        "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
        "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
        "\n",
        "        # Keeping track of l2 regularization loss (optional)\n",
        "        l2_loss = tf.constant(0.0)\n",
        "\n",
        "        # Embedding layer\n",
        "        \"\"\"\n",
        "        <Variable>\n",
        "            - W: 각 단어의 임베디드 벡터의 성분을 랜덤하게 할당\n",
        "        \"\"\"\n",
        "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
        "        #with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
        "            W = tf.Variable(\n",
        "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
        "                name=\"W\")\n",
        "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
        "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
        "\n",
        "        # Create a convolution + maxpool layer for each filter size\n",
        "        pooled_outputs = []\n",
        "        for i, filter_size in enumerate(filter_sizes):\n",
        "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
        "                # Convolution Layer\n",
        "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
        "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
        "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
        "                conv = tf.nn.conv2d(\n",
        "                    self.embedded_chars_expanded,\n",
        "                    W,\n",
        "                    strides=[1, 1, 1, 1],\n",
        "                    padding=\"VALID\",\n",
        "                    name=\"conv\")\n",
        "                # Apply nonlinearity\n",
        "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
        "                # Maxpooling over the outputs\n",
        "                pooled = tf.nn.max_pool(\n",
        "                    h,\n",
        "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
        "                    strides=[1, 1, 1, 1],\n",
        "                    padding='VALID',\n",
        "                    name=\"pool\")\n",
        "                pooled_outputs.append(pooled)\n",
        "\n",
        "        # Combine all the pooled features\n",
        "        num_filters_total = num_filters * len(filter_sizes)\n",
        "        self.h_pool = tf.concat(pooled_outputs,3)\n",
        "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
        "\n",
        "        # Add dropout\n",
        "        with tf.name_scope(\"dropout\"):\n",
        "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
        "\n",
        "        # Final (unnormalized) scores and predictions\n",
        "        with tf.name_scope(\"output\"):\n",
        "            W = tf.get_variable(\n",
        "                \"W\",\n",
        "                shape=[num_filters_total, num_classes],\n",
        "                initializer=tf.contrib.layers.xavier_initializer())\n",
        "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
        "            l2_loss += tf.nn.l2_loss(W)\n",
        "            l2_loss += tf.nn.l2_loss(b)\n",
        "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
        "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
        "\n",
        "        # Calculate Mean cross-entropy loss\n",
        "        with tf.name_scope(\"loss\"):\n",
        "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
        "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
        "\n",
        "        # Accuracy\n",
        "        with tf.name_scope(\"accuracy\"):\n",
        "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
        "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
      ],
      "metadata": {
        "id": "kbrGDoOZ-S5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data loading\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"./words_data.csv\", encoding='utf-8')\n",
        "df = df.loc[:,['header', 'detail']]\n"
      ],
      "metadata": {
        "id": "peZ-KmPN_06h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contents = df['detail']\n",
        "points = df['header']\n",
        "contents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8Mer8zM_20W",
        "outputId": "36e4cf07-1274-4374-d84d-92fe12ce4e07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        ['webgl', 'canvas', '활용', 'd', 'd', '모델', '인터'...\n",
              "1        ['ruby', 'on', 'rails', '이용', 'rest', 'api', '...\n",
              "2        ['한국', '미국', '하루', '평균', '사용', '블라인드', '서비스', ...\n",
              "3        ['fts', 'fulfillment', 'and', 'transportation'...\n",
              "4        ['agile', '골', '라라', '개발', '문화', '백엔드', '개발', ...\n",
              "                               ...                        \n",
              "46216    ['자체', '개발', 'web', 'logging', 'framework', '이...\n",
              "46217    ['ai', '를', '기반', '개인', '취향', '최적화', '음악', '추천...\n",
              "46218    ['data', 'engineer', 'python', '클라우드', '환경', '...\n",
              "46219    ['senior', 'frontend', 'engineer', 'mlops', '제...\n",
              "46220    ['국내', '유명', 'ai', '기반', '오디오', '기술', '스타트업', ...\n",
              "Name: detail, Length: 46221, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_output(points):\n",
        "  results = np.zeros((len(points),10))\n",
        "  for idx, point in enumerate(points):\n",
        "    results[idx, point] = 1\n",
        "  return results"
      ],
      "metadata": {
        "id": "TN9zPBrg_8As"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tranform document to vector\n",
        "max_document_length = 200\n",
        "x, vocabulary, vocab_size = tool.make_input(contents,max_document_length)\n",
        "print('사전단어수 : %s' % (vocab_size))\n",
        "y = make_output(points)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z4BYm_K_9tp",
        "outputId": "de5d9f5e-6a51-4789-f3d2-bbe4997f0f2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/cnn_tool.py:43: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n",
            "사전단어수 : 36112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# divide dataset into train/test set\n",
        "x_train, x_test, y_train, y_test = tool.divide(x,y,train_prop=0.8)"
      ],
      "metadata": {
        "id": "8RhJZ4By__X6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Hyperparameters\n",
        "flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of embedded vector (default: 128)\")\n",
        "flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
        "flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
        "flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
        "flags.DEFINE_float(\"l2_reg_lambda\", 0.1, \"L2 regularization lambda (default: 0.0)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGrvQhBPAFkg",
        "outputId": "9534795e-b57b-4d8a-9429-cc5d22bdffd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<absl.flags._flagvalues.FlagHolder at 0x7f39cf514c10>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
        "flags.DEFINE_integer(\"num_epochs\", 10, \"Number of training epochs (default: 200)\")\n",
        "flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
        "flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
        "flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5Palwz-AIlm",
        "outputId": "f2741790-f946-4884-e7fd-ed827318d195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<absl.flags._flagvalues.FlagHolder at 0x7f39cf5148d0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Misc Parameters\n",
        "flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
        "flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfTcFzM8AQM5",
        "outputId": "8e8aee88-1dfd-4e61-ccd5-5491e56f874f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<absl.flags._flagvalues.FlagHolder at 0x7f39cf4ed150>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flags.DEFINE_string(\"f\", \"\", \"kernel\")\n",
        "\n",
        "FLAGS = tf.flags.FLAGS\n",
        "#FLAGS._parse_flags()\n",
        "\n",
        "import sys\n",
        "FLAGS(sys.argv)\n",
        "\n",
        "print(\"\\nParameters:\")\n",
        "for attr, value in sorted(FLAGS.__flags.items()):\n",
        "    print(\"{}={}\".format(attr.upper(), value))\n",
        "print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_McP_fIARRd",
        "outputId": "f863fef0-fb03-49a5-886e-35e6c49e5ad6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Parameters:\n",
            "ALLOW_SOFT_PLACEMENT=<absl.flags._flag.BooleanFlag object at 0x7f39cf4eda10>\n",
            "ALSOLOGTOSTDERR=<absl.flags._flag.BooleanFlag object at 0x7f39e9ef2090>\n",
            "BATCH_SIZE=<absl.flags._flag.Flag object at 0x7f39cf514590>\n",
            "CHECKPOINT_EVERY=<absl.flags._flag.Flag object at 0x7f39cf514510>\n",
            "DROPOUT_KEEP_PROB=<absl.flags._flag.Flag object at 0x7f39cf514610>\n",
            "EMBEDDING_DIM=<absl.flags._flag.Flag object at 0x7f39cf514a50>\n",
            "EVALUATE_EVERY=<absl.flags._flag.Flag object at 0x7f39cf514b90>\n",
            "F=<absl.flags._flag.Flag object at 0x7f39cf4f0e10>\n",
            "FILTER_SIZES=<absl.flags._flag.Flag object at 0x7f39cf514c90>\n",
            "L2_REG_LAMBDA=<absl.flags._flag.Flag object at 0x7f39cf514850>\n",
            "LOG_DEVICE_PLACEMENT=<absl.flags._flag.BooleanFlag object at 0x7f39cf4ed990>\n",
            "LOG_DIR=<absl.flags._flag.Flag object at 0x7f39e9ef2150>\n",
            "LOGGER_LEVELS=<absl.logging._LoggerLevelsFlag object at 0x7f39e9ef22d0>\n",
            "LOGTOSTDERR=<absl.flags._flag.BooleanFlag object at 0x7f39e9f6b9d0>\n",
            "NUM_CHECKPOINTS=<absl.flags._flag.Flag object at 0x7f39cf514190>\n",
            "NUM_EPOCHS=<absl.flags._flag.Flag object at 0x7f39cf514210>\n",
            "NUM_FILTERS=<absl.flags._flag.Flag object at 0x7f39cf514d50>\n",
            "ONLY_CHECK_ARGS=<absl.flags._flag.BooleanFlag object at 0x7f39e9ef2c10>\n",
            "OP_CONVERSION_FALLBACK_TO_WHILE_LOOP=<absl.flags._flag.BooleanFlag object at 0x7f39e83f6210>\n",
            "PDB=<absl.flags._defines.DEFINE_alias.<locals>._FlagAlias object at 0x7f39e9fdb550>\n",
            "PDB_POST_MORTEM=<absl.flags._flag.BooleanFlag object at 0x7f39e9fb55d0>\n",
            "PROFILE_FILE=<absl.flags._flag.Flag object at 0x7f39e9f65c10>\n",
            "RUN_WITH_PDB=<absl.flags._flag.BooleanFlag object at 0x7f39e9fb54d0>\n",
            "RUN_WITH_PROFILING=<absl.flags._flag.BooleanFlag object at 0x7f39e9f4ce10>\n",
            "SHOWPREFIXFORINFO=<absl.flags._flag.BooleanFlag object at 0x7f39e9ef25d0>\n",
            "STDERRTHRESHOLD=<absl.logging._StderrthresholdFlag object at 0x7f39e9ef24d0>\n",
            "TEST_RANDOM_SEED=<absl.flags._flag.Flag object at 0x7f39db95fdd0>\n",
            "TEST_RANDOMIZE_ORDERING_SEED=<absl.flags._flag.Flag object at 0x7f39db9640d0>\n",
            "TEST_SRCDIR=<absl.flags._flag.Flag object at 0x7f39db98eed0>\n",
            "TEST_TMPDIR=<absl.flags._flag.Flag object at 0x7f39db959910>\n",
            "USE_CPROFILE_FOR_PROFILING=<absl.flags._flag.BooleanFlag object at 0x7f39e9ef2bd0>\n",
            "V=<absl.logging._VerbosityFlag object at 0x7f39e9ef21d0>\n",
            "VERBOSITY=<absl.logging._VerbosityFlag object at 0x7f39e9ef21d0>\n",
            "XML_OUTPUT_FILE=<absl.flags._flag.Flag object at 0x7f39db964a90>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(y_train.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eU5d2_iAS01",
        "outputId": "13ead5ab-c65b-45f2-fba3-e402201b45d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(36977, 200)\n",
            "(36977, 10)\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. train the model and test\n",
        "with tf.Graph().as_default():\n",
        "    sess = tf.Session()\n",
        "    with sess.as_default():\n",
        "        cnn = TextCNN(sequence_length=x_train.shape[1],\n",
        "                      num_classes=y_train.shape[1],\n",
        "                      vocab_size=vocab_size,\n",
        "                      embedding_size=FLAGS.embedding_dim,\n",
        "                      filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
        "                      num_filters=FLAGS.num_filters,\n",
        "                      l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
        "\n",
        "        # Define Training procedure\n",
        "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
        "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
        "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
        "\n",
        "        # Keep track of gradient values and sparsity (optional)\n",
        "        grad_summaries = []\n",
        "        for g, v in grads_and_vars:\n",
        "            if g is not None:\n",
        "                grad_hist_summary = tf.summary.histogram(\"{}\".format(v.name), g)\n",
        "                sparsity_summary = tf.summary.scalar(\"{}\".format(v.name), tf.nn.zero_fraction(g))\n",
        "                grad_summaries.append(grad_hist_summary)\n",
        "                grad_summaries.append(sparsity_summary)\n",
        "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
        "\n",
        "        # Output directory for models and summaries\n",
        "        timestamp = str(int(time.time()))\n",
        "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
        "        print(\"Writing to {}\\n\".format(out_dir))\n",
        "\n",
        "        # Summaries for loss and accuracy\n",
        "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
        "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
        "\n",
        "        # Train Summaries\n",
        "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
        "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
        "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
        "\n",
        "        # Dev summaries\n",
        "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
        "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
        "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
        "\n",
        "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
        "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
        "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
        "        if not os.path.exists(checkpoint_dir):\n",
        "            os.makedirs(checkpoint_dir)\n",
        "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
        "\n",
        "        # Initialize all variables\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        def train_step(x_batch, y_batch):\n",
        "            \"\"\"\n",
        "            A single training step\n",
        "            \"\"\"\n",
        "            feed_dict = {\n",
        "                cnn.input_x: x_batch,\n",
        "                cnn.input_y: y_batch,\n",
        "                cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
        "            }\n",
        "            _, step, summaries, loss, accuracy = sess.run(\n",
        "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
        "                feed_dict)\n",
        "            time_str = datetime.datetime.now().isoformat()\n",
        "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
        "            train_summary_writer.add_summary(summaries, step)\n",
        "\n",
        "\n",
        "        def dev_step(x_batch, y_batch, writer=None):\n",
        "            \"\"\"\n",
        "            Evaluates model on a dev set\n",
        "            \"\"\"\n",
        "            feed_dict = {\n",
        "                cnn.input_x: x_batch,\n",
        "                cnn.input_y: y_batch,\n",
        "                cnn.dropout_keep_prob: 1.0\n",
        "            }\n",
        "            step, summaries, loss, accuracy = sess.run(\n",
        "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
        "                feed_dict)\n",
        "            time_str = datetime.datetime.now().isoformat()\n",
        "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
        "            if writer:\n",
        "                writer.add_summary(summaries, step)\n",
        "\n",
        "\n",
        "        def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
        "            \"\"\"\n",
        "            Generates a batch iterator for a dataset.\n",
        "            \"\"\"\n",
        "            data = np.array(data)\n",
        "            data_size = len(data)\n",
        "            num_batches_per_epoch = int((len(data) - 1) / batch_size) + 1\n",
        "            for epoch in range(num_epochs):\n",
        "                # Shuffle the data at each epoch\n",
        "                if shuffle:\n",
        "                    shuffle_indices = np.random.permutation(np.arange(data_size))\n",
        "                    shuffled_data = data[shuffle_indices]\n",
        "                else:\n",
        "                    shuffled_data = data\n",
        "                for batch_num in range(num_batches_per_epoch):\n",
        "                    start_index = batch_num * batch_size\n",
        "                    end_index = min((batch_num + 1) * batch_size, data_size)\n",
        "                    yield shuffled_data[start_index:end_index]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhuIt8biAjfG",
        "outputId": "fd3eb5a3-b6ed-45af-9a27-b70f911a7de8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-d8581bf790d1>:71: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-6-d8581bf790d1>:87: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "INFO:tensorflow:Summary name embedding/W:0 is illegal; using embedding/W_0 instead.\n",
            "INFO:tensorflow:Summary name embedding/W:0 is illegal; using embedding/W_0 instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-3/W:0 is illegal; using conv-maxpool-3/W_0 instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-3/W:0 is illegal; using conv-maxpool-3/W_0 instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-3/b:0 is illegal; using conv-maxpool-3/b_0 instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-3/b:0 is illegal; using conv-maxpool-3/b_0 instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-4/W:0 is illegal; using conv-maxpool-4/W_0 instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-4/W:0 is illegal; using conv-maxpool-4/W_0 instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-4/b:0 is illegal; using conv-maxpool-4/b_0 instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-4/b:0 is illegal; using conv-maxpool-4/b_0 instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-5/W:0 is illegal; using conv-maxpool-5/W_0 instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-5/W:0 is illegal; using conv-maxpool-5/W_0 instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-5/b:0 is illegal; using conv-maxpool-5/b_0 instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-5/b:0 is illegal; using conv-maxpool-5/b_0 instead.\n",
            "INFO:tensorflow:Summary name W:0 is illegal; using W_0 instead.\n",
            "INFO:tensorflow:Summary name W:0 is illegal; using W_0 instead.\n",
            "INFO:tensorflow:Summary name output/b:0 is illegal; using output/b_0 instead.\n",
            "INFO:tensorflow:Summary name output/b:0 is illegal; using output/b_0 instead.\n",
            "Writing to /content/runs/1651775402\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "        # Generate batches\n",
        "        batches = batch_iter(\n",
        "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
        "\n",
        "        testpoint = 0\n",
        "        # Training loop. For each batch...\n",
        "        for batch in batches:\n",
        "            x_batch, y_batch = zip(*batch)\n",
        "            train_step(x_batch, y_batch)\n",
        "            current_step = tf.train.global_step(sess, global_step)\n",
        "            if current_step % FLAGS.evaluate_every == 0:\n",
        "                if testpoint + 100 < len(x_test):\n",
        "                    testpoint += 100\n",
        "                else:\n",
        "                    testpoint = 0\n",
        "                print(\"\\nEvaluation:\")\n",
        "                dev_step(x_test[testpoint:testpoint+100], y_test[testpoint:testpoint+100], writer=dev_summary_writer)\n",
        "                print(\"\")\n",
        "            if current_step % FLAGS.checkpoint_every == 0:\n",
        "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
        "                print(\"Saved model checkpoint to {}\\n\".format(path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XF861yYdAlh4",
        "outputId": "b8a3fdec-57a9-4ab4-cae2-2987e1ec2d53"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:97: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "2022-05-05T21:12:50.298964: step 6843, loss 1.55651, acc 0.4375\n",
            "2022-05-05T21:12:50.936559: step 6844, loss 1.62934, acc 0.453125\n",
            "2022-05-05T21:12:51.563908: step 6845, loss 1.47682, acc 0.46875\n",
            "2022-05-05T21:12:52.241867: step 6846, loss 1.6152, acc 0.40625\n",
            "2022-05-05T21:12:52.960129: step 6847, loss 1.42992, acc 0.46875\n",
            "2022-05-05T21:12:53.667031: step 6848, loss 1.3361, acc 0.515625\n",
            "2022-05-05T21:12:54.369841: step 6849, loss 1.28052, acc 0.5625\n",
            "2022-05-05T21:12:55.040329: step 6850, loss 1.35787, acc 0.5\n",
            "2022-05-05T21:12:55.705224: step 6851, loss 1.55394, acc 0.421875\n",
            "2022-05-05T21:12:56.342097: step 6852, loss 1.41548, acc 0.546875\n",
            "2022-05-05T21:12:56.968606: step 6853, loss 1.41225, acc 0.421875\n",
            "2022-05-05T21:12:57.592130: step 6854, loss 1.35006, acc 0.578125\n",
            "2022-05-05T21:12:58.216177: step 6855, loss 1.34686, acc 0.5\n",
            "2022-05-05T21:12:58.824930: step 6856, loss 1.62614, acc 0.484375\n",
            "2022-05-05T21:12:59.474362: step 6857, loss 1.33005, acc 0.546875\n",
            "2022-05-05T21:13:00.153295: step 6858, loss 1.3974, acc 0.546875\n",
            "2022-05-05T21:13:00.829766: step 6859, loss 1.40842, acc 0.53125\n",
            "2022-05-05T21:13:01.478763: step 6860, loss 1.50912, acc 0.46875\n",
            "2022-05-05T21:13:02.131108: step 6861, loss 1.63405, acc 0.46875\n",
            "2022-05-05T21:13:02.801183: step 6862, loss 1.29087, acc 0.65625\n",
            "2022-05-05T21:13:03.461953: step 6863, loss 1.55917, acc 0.484375\n",
            "2022-05-05T21:13:04.090304: step 6864, loss 1.44377, acc 0.53125\n",
            "2022-05-05T21:13:04.703479: step 6865, loss 1.55865, acc 0.53125\n",
            "2022-05-05T21:13:05.379246: step 6866, loss 1.38116, acc 0.578125\n",
            "2022-05-05T21:13:06.018215: step 6867, loss 1.4966, acc 0.453125\n",
            "2022-05-05T21:13:06.648879: step 6868, loss 1.35157, acc 0.546875\n",
            "2022-05-05T21:13:07.274839: step 6869, loss 1.42728, acc 0.5625\n",
            "2022-05-05T21:13:07.894216: step 6870, loss 1.54613, acc 0.4375\n",
            "2022-05-05T21:13:08.525527: step 6871, loss 1.42551, acc 0.484375\n",
            "2022-05-05T21:13:09.150472: step 6872, loss 1.71279, acc 0.359375\n",
            "2022-05-05T21:13:09.765748: step 6873, loss 1.59036, acc 0.5\n",
            "2022-05-05T21:13:10.416405: step 6874, loss 1.56625, acc 0.421875\n",
            "2022-05-05T21:13:11.090949: step 6875, loss 1.53937, acc 0.453125\n",
            "2022-05-05T21:13:11.775219: step 6876, loss 1.75902, acc 0.359375\n",
            "2022-05-05T21:13:12.431377: step 6877, loss 1.45358, acc 0.390625\n",
            "2022-05-05T21:13:13.117202: step 6878, loss 1.58479, acc 0.46875\n",
            "2022-05-05T21:13:13.774603: step 6879, loss 1.7103, acc 0.421875\n",
            "2022-05-05T21:13:14.400160: step 6880, loss 1.47157, acc 0.4375\n",
            "2022-05-05T21:13:15.016238: step 6881, loss 1.43107, acc 0.53125\n",
            "2022-05-05T21:13:15.627556: step 6882, loss 1.60177, acc 0.421875\n",
            "2022-05-05T21:13:16.229836: step 6883, loss 1.64294, acc 0.453125\n",
            "2022-05-05T21:13:16.848382: step 6884, loss 1.56939, acc 0.40625\n",
            "2022-05-05T21:13:17.461086: step 6885, loss 1.25967, acc 0.515625\n",
            "2022-05-05T21:13:18.076781: step 6886, loss 1.66781, acc 0.453125\n",
            "2022-05-05T21:13:18.718454: step 6887, loss 1.69686, acc 0.484375\n",
            "2022-05-05T21:13:19.353562: step 6888, loss 1.63578, acc 0.390625\n",
            "2022-05-05T21:13:20.039560: step 6889, loss 1.49899, acc 0.375\n",
            "2022-05-05T21:13:20.716316: step 6890, loss 1.63774, acc 0.421875\n",
            "2022-05-05T21:13:21.336034: step 6891, loss 1.48122, acc 0.421875\n",
            "2022-05-05T21:13:21.967929: step 6892, loss 1.33685, acc 0.59375\n",
            "2022-05-05T21:13:22.665799: step 6893, loss 1.36284, acc 0.484375\n",
            "2022-05-05T21:13:23.362172: step 6894, loss 1.40927, acc 0.515625\n",
            "2022-05-05T21:13:24.035768: step 6895, loss 1.3067, acc 0.546875\n",
            "2022-05-05T21:13:24.714729: step 6896, loss 1.57149, acc 0.421875\n",
            "2022-05-05T21:13:25.410237: step 6897, loss 1.44018, acc 0.46875\n",
            "2022-05-05T21:13:26.121299: step 6898, loss 1.49689, acc 0.484375\n",
            "2022-05-05T21:13:26.835326: step 6899, loss 1.43689, acc 0.5625\n",
            "2022-05-05T21:13:27.499225: step 6900, loss 1.56288, acc 0.4375\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:13:27.655344: step 6900, loss 1.71226, acc 0.35\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-6900\n",
            "\n",
            "2022-05-05T21:13:28.555979: step 6901, loss 1.71275, acc 0.40625\n",
            "2022-05-05T21:13:29.246883: step 6902, loss 1.67677, acc 0.390625\n",
            "2022-05-05T21:13:29.983403: step 6903, loss 1.57445, acc 0.4375\n",
            "2022-05-05T21:13:30.659680: step 6904, loss 1.43292, acc 0.53125\n",
            "2022-05-05T21:13:31.301158: step 6905, loss 1.40259, acc 0.5\n",
            "2022-05-05T21:13:31.974703: step 6906, loss 1.43619, acc 0.453125\n",
            "2022-05-05T21:13:32.631312: step 6907, loss 1.59365, acc 0.421875\n",
            "2022-05-05T21:13:33.277689: step 6908, loss 1.66221, acc 0.515625\n",
            "2022-05-05T21:13:33.927067: step 6909, loss 1.69639, acc 0.3125\n",
            "2022-05-05T21:13:34.548101: step 6910, loss 1.5854, acc 0.515625\n",
            "2022-05-05T21:13:35.183590: step 6911, loss 1.41966, acc 0.484375\n",
            "2022-05-05T21:13:35.825894: step 6912, loss 1.44354, acc 0.421875\n",
            "2022-05-05T21:13:36.459230: step 6913, loss 1.55349, acc 0.515625\n",
            "2022-05-05T21:13:37.084456: step 6914, loss 1.37019, acc 0.5625\n",
            "2022-05-05T21:13:37.757668: step 6915, loss 1.72349, acc 0.375\n",
            "2022-05-05T21:13:38.449585: step 6916, loss 1.29535, acc 0.546875\n",
            "2022-05-05T21:13:39.148052: step 6917, loss 1.56253, acc 0.453125\n",
            "2022-05-05T21:13:39.846735: step 6918, loss 1.45146, acc 0.5625\n",
            "2022-05-05T21:13:40.537787: step 6919, loss 1.56215, acc 0.390625\n",
            "2022-05-05T21:13:41.182233: step 6920, loss 1.607, acc 0.421875\n",
            "2022-05-05T21:13:41.797008: step 6921, loss 1.34937, acc 0.5625\n",
            "2022-05-05T21:13:42.409482: step 6922, loss 1.52167, acc 0.421875\n",
            "2022-05-05T21:13:43.009027: step 6923, loss 1.5523, acc 0.375\n",
            "2022-05-05T21:13:43.627168: step 6924, loss 1.62735, acc 0.46875\n",
            "2022-05-05T21:13:44.258571: step 6925, loss 1.43914, acc 0.546875\n",
            "2022-05-05T21:13:44.881482: step 6926, loss 1.45675, acc 0.59375\n",
            "2022-05-05T21:13:45.503043: step 6927, loss 1.43635, acc 0.53125\n",
            "2022-05-05T21:13:46.111236: step 6928, loss 1.60252, acc 0.46875\n",
            "2022-05-05T21:13:46.731026: step 6929, loss 1.62314, acc 0.40625\n",
            "2022-05-05T21:13:47.355804: step 6930, loss 1.34583, acc 0.53125\n",
            "2022-05-05T21:13:48.004676: step 6931, loss 1.46154, acc 0.4375\n",
            "2022-05-05T21:13:48.637333: step 6932, loss 1.55094, acc 0.40625\n",
            "2022-05-05T21:13:49.241852: step 6933, loss 1.55643, acc 0.53125\n",
            "2022-05-05T21:13:49.856935: step 6934, loss 1.69587, acc 0.34375\n",
            "2022-05-05T21:13:50.466603: step 6935, loss 1.52889, acc 0.46875\n",
            "2022-05-05T21:13:50.962702: step 6936, loss 1.40767, acc 0.55102\n",
            "2022-05-05T21:13:51.602740: step 6937, loss 1.31186, acc 0.53125\n",
            "2022-05-05T21:13:52.258614: step 6938, loss 1.27848, acc 0.640625\n",
            "2022-05-05T21:13:52.905034: step 6939, loss 1.36063, acc 0.515625\n",
            "2022-05-05T21:13:53.513731: step 6940, loss 1.16362, acc 0.640625\n",
            "2022-05-05T21:13:54.165547: step 6941, loss 1.42043, acc 0.484375\n",
            "2022-05-05T21:13:54.880086: step 6942, loss 1.20167, acc 0.59375\n",
            "2022-05-05T21:13:55.557749: step 6943, loss 1.31348, acc 0.5625\n",
            "2022-05-05T21:13:56.233440: step 6944, loss 1.42766, acc 0.578125\n",
            "2022-05-05T21:13:56.939428: step 6945, loss 1.54317, acc 0.5\n",
            "2022-05-05T21:13:57.605389: step 6946, loss 1.30875, acc 0.609375\n",
            "2022-05-05T21:13:58.285321: step 6947, loss 1.29658, acc 0.5625\n",
            "2022-05-05T21:13:58.964242: step 6948, loss 1.45865, acc 0.484375\n",
            "2022-05-05T21:13:59.622673: step 6949, loss 1.49033, acc 0.53125\n",
            "2022-05-05T21:14:00.281167: step 6950, loss 1.33305, acc 0.609375\n",
            "2022-05-05T21:14:00.902508: step 6951, loss 1.29556, acc 0.5\n",
            "2022-05-05T21:14:01.512903: step 6952, loss 1.45309, acc 0.421875\n",
            "2022-05-05T21:14:02.131579: step 6953, loss 1.38064, acc 0.53125\n",
            "2022-05-05T21:14:02.740444: step 6954, loss 1.50919, acc 0.484375\n",
            "2022-05-05T21:14:03.354214: step 6955, loss 1.30992, acc 0.53125\n",
            "2022-05-05T21:14:03.968301: step 6956, loss 1.4712, acc 0.546875\n",
            "2022-05-05T21:14:04.594945: step 6957, loss 1.4701, acc 0.53125\n",
            "2022-05-05T21:14:05.258722: step 6958, loss 1.42193, acc 0.546875\n",
            "2022-05-05T21:14:05.864532: step 6959, loss 1.47092, acc 0.515625\n",
            "2022-05-05T21:14:06.483150: step 6960, loss 1.30495, acc 0.6875\n",
            "2022-05-05T21:14:07.095874: step 6961, loss 1.39312, acc 0.515625\n",
            "2022-05-05T21:14:07.714577: step 6962, loss 1.23446, acc 0.625\n",
            "2022-05-05T21:14:08.330766: step 6963, loss 1.56535, acc 0.453125\n",
            "2022-05-05T21:14:08.938087: step 6964, loss 1.39055, acc 0.5\n",
            "2022-05-05T21:14:09.569632: step 6965, loss 1.47868, acc 0.515625\n",
            "2022-05-05T21:14:10.252105: step 6966, loss 1.3617, acc 0.59375\n",
            "2022-05-05T21:14:10.937239: step 6967, loss 1.36404, acc 0.609375\n",
            "2022-05-05T21:14:11.637414: step 6968, loss 1.47524, acc 0.421875\n",
            "2022-05-05T21:14:12.330659: step 6969, loss 1.41745, acc 0.515625\n",
            "2022-05-05T21:14:13.016766: step 6970, loss 1.29028, acc 0.515625\n",
            "2022-05-05T21:14:13.695841: step 6971, loss 1.39087, acc 0.484375\n",
            "2022-05-05T21:14:14.372835: step 6972, loss 1.14693, acc 0.625\n",
            "2022-05-05T21:14:15.001647: step 6973, loss 1.49123, acc 0.453125\n",
            "2022-05-05T21:14:15.659236: step 6974, loss 1.38797, acc 0.5\n",
            "2022-05-05T21:14:16.266435: step 6975, loss 1.47523, acc 0.484375\n",
            "2022-05-05T21:14:16.882472: step 6976, loss 1.50859, acc 0.453125\n",
            "2022-05-05T21:14:17.498812: step 6977, loss 1.49297, acc 0.5\n",
            "2022-05-05T21:14:18.107812: step 6978, loss 1.46403, acc 0.515625\n",
            "2022-05-05T21:14:18.743750: step 6979, loss 1.12418, acc 0.625\n",
            "2022-05-05T21:14:19.364322: step 6980, loss 1.41177, acc 0.578125\n",
            "2022-05-05T21:14:19.968768: step 6981, loss 1.45774, acc 0.46875\n",
            "2022-05-05T21:14:20.580156: step 6982, loss 1.24254, acc 0.609375\n",
            "2022-05-05T21:14:21.210194: step 6983, loss 1.47762, acc 0.46875\n",
            "2022-05-05T21:14:21.901241: step 6984, loss 1.41992, acc 0.515625\n",
            "2022-05-05T21:14:22.624488: step 6985, loss 1.30064, acc 0.59375\n",
            "2022-05-05T21:14:23.306244: step 6986, loss 1.29315, acc 0.609375\n",
            "2022-05-05T21:14:24.015813: step 6987, loss 1.4588, acc 0.453125\n",
            "2022-05-05T21:14:24.697645: step 6988, loss 1.3089, acc 0.625\n",
            "2022-05-05T21:14:25.400293: step 6989, loss 1.16775, acc 0.625\n",
            "2022-05-05T21:14:26.102632: step 6990, loss 1.22149, acc 0.578125\n",
            "2022-05-05T21:14:26.770401: step 6991, loss 1.3234, acc 0.5625\n",
            "2022-05-05T21:14:27.403906: step 6992, loss 1.3766, acc 0.546875\n",
            "2022-05-05T21:14:28.042441: step 6993, loss 1.69613, acc 0.421875\n",
            "2022-05-05T21:14:28.671246: step 6994, loss 1.54021, acc 0.53125\n",
            "2022-05-05T21:14:29.324386: step 6995, loss 1.48985, acc 0.453125\n",
            "2022-05-05T21:14:29.944058: step 6996, loss 1.29914, acc 0.578125\n",
            "2022-05-05T21:14:30.587102: step 6997, loss 1.54504, acc 0.46875\n",
            "2022-05-05T21:14:31.257780: step 6998, loss 1.45855, acc 0.515625\n",
            "2022-05-05T21:14:31.971744: step 6999, loss 1.22025, acc 0.703125\n",
            "2022-05-05T21:14:32.654916: step 7000, loss 1.23167, acc 0.65625\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:14:32.817289: step 7000, loss 1.85829, acc 0.34\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-7000\n",
            "\n",
            "2022-05-05T21:14:33.755382: step 7001, loss 1.54079, acc 0.46875\n",
            "2022-05-05T21:14:34.477370: step 7002, loss 1.43835, acc 0.453125\n",
            "2022-05-05T21:14:35.167832: step 7003, loss 1.18758, acc 0.59375\n",
            "2022-05-05T21:14:35.871749: step 7004, loss 1.50246, acc 0.4375\n",
            "2022-05-05T21:14:36.563737: step 7005, loss 1.45724, acc 0.546875\n",
            "2022-05-05T21:14:37.276450: step 7006, loss 1.22557, acc 0.53125\n",
            "2022-05-05T21:14:37.971171: step 7007, loss 1.24621, acc 0.625\n",
            "2022-05-05T21:14:38.678158: step 7008, loss 1.3348, acc 0.609375\n",
            "2022-05-05T21:14:39.350237: step 7009, loss 1.38417, acc 0.515625\n",
            "2022-05-05T21:14:40.013106: step 7010, loss 1.40804, acc 0.5\n",
            "2022-05-05T21:14:40.665317: step 7011, loss 1.47109, acc 0.484375\n",
            "2022-05-05T21:14:41.301342: step 7012, loss 1.38249, acc 0.5\n",
            "2022-05-05T21:14:41.948220: step 7013, loss 1.3962, acc 0.53125\n",
            "2022-05-05T21:14:42.580312: step 7014, loss 1.50296, acc 0.453125\n",
            "2022-05-05T21:14:43.183129: step 7015, loss 1.29769, acc 0.640625\n",
            "2022-05-05T21:14:43.819254: step 7016, loss 1.38733, acc 0.5\n",
            "2022-05-05T21:14:44.437494: step 7017, loss 1.24158, acc 0.671875\n",
            "2022-05-05T21:14:45.056176: step 7018, loss 1.28429, acc 0.578125\n",
            "2022-05-05T21:14:45.677168: step 7019, loss 1.3108, acc 0.640625\n",
            "2022-05-05T21:14:46.408859: step 7020, loss 1.51568, acc 0.484375\n",
            "2022-05-05T21:14:47.056125: step 7021, loss 1.42697, acc 0.5\n",
            "2022-05-05T21:14:47.668665: step 7022, loss 1.33086, acc 0.5\n",
            "2022-05-05T21:14:48.267310: step 7023, loss 1.43099, acc 0.40625\n",
            "2022-05-05T21:14:48.879797: step 7024, loss 1.55898, acc 0.5\n",
            "2022-05-05T21:14:49.488181: step 7025, loss 1.49482, acc 0.453125\n",
            "2022-05-05T21:14:50.077094: step 7026, loss 1.27215, acc 0.53125\n",
            "2022-05-05T21:14:50.681269: step 7027, loss 1.53404, acc 0.46875\n",
            "2022-05-05T21:14:51.272048: step 7028, loss 1.42821, acc 0.453125\n",
            "2022-05-05T21:14:51.884929: step 7029, loss 1.43732, acc 0.484375\n",
            "2022-05-05T21:14:52.492924: step 7030, loss 1.37776, acc 0.484375\n",
            "2022-05-05T21:14:53.107226: step 7031, loss 1.44998, acc 0.453125\n",
            "2022-05-05T21:14:53.728197: step 7032, loss 1.46161, acc 0.53125\n",
            "2022-05-05T21:14:54.357906: step 7033, loss 1.26766, acc 0.609375\n",
            "2022-05-05T21:14:54.990266: step 7034, loss 1.25813, acc 0.5625\n",
            "2022-05-05T21:14:55.704590: step 7035, loss 1.49468, acc 0.4375\n",
            "2022-05-05T21:14:56.424891: step 7036, loss 1.57784, acc 0.390625\n",
            "2022-05-05T21:14:57.113045: step 7037, loss 1.46866, acc 0.484375\n",
            "2022-05-05T21:14:57.769222: step 7038, loss 1.40574, acc 0.578125\n",
            "2022-05-05T21:14:58.414429: step 7039, loss 1.3394, acc 0.578125\n",
            "2022-05-05T21:14:59.090932: step 7040, loss 1.22231, acc 0.640625\n",
            "2022-05-05T21:14:59.787509: step 7041, loss 1.47259, acc 0.546875\n",
            "2022-05-05T21:15:00.475387: step 7042, loss 1.33617, acc 0.515625\n",
            "2022-05-05T21:15:01.177117: step 7043, loss 1.35047, acc 0.546875\n",
            "2022-05-05T21:15:01.856447: step 7044, loss 1.39246, acc 0.5\n",
            "2022-05-05T21:15:02.520588: step 7045, loss 1.43616, acc 0.53125\n",
            "2022-05-05T21:15:03.188365: step 7046, loss 1.384, acc 0.59375\n",
            "2022-05-05T21:15:03.819162: step 7047, loss 1.33533, acc 0.515625\n",
            "2022-05-05T21:15:04.444186: step 7048, loss 1.35541, acc 0.59375\n",
            "2022-05-05T21:15:05.059090: step 7049, loss 1.29637, acc 0.5625\n",
            "2022-05-05T21:15:05.671083: step 7050, loss 1.33386, acc 0.546875\n",
            "2022-05-05T21:15:06.285060: step 7051, loss 1.12196, acc 0.609375\n",
            "2022-05-05T21:15:06.918943: step 7052, loss 1.40955, acc 0.484375\n",
            "2022-05-05T21:15:07.552089: step 7053, loss 1.50175, acc 0.515625\n",
            "2022-05-05T21:15:08.175580: step 7054, loss 1.47609, acc 0.515625\n",
            "2022-05-05T21:15:08.793368: step 7055, loss 1.26114, acc 0.546875\n",
            "2022-05-05T21:15:09.429674: step 7056, loss 1.59348, acc 0.515625\n",
            "2022-05-05T21:15:10.092106: step 7057, loss 1.57739, acc 0.4375\n",
            "2022-05-05T21:15:10.731761: step 7058, loss 1.55849, acc 0.453125\n",
            "2022-05-05T21:15:11.352273: step 7059, loss 1.31467, acc 0.515625\n",
            "2022-05-05T21:15:11.997253: step 7060, loss 1.25514, acc 0.515625\n",
            "2022-05-05T21:15:12.669170: step 7061, loss 1.63064, acc 0.40625\n",
            "2022-05-05T21:15:13.361299: step 7062, loss 1.47117, acc 0.453125\n",
            "2022-05-05T21:15:14.041449: step 7063, loss 1.40187, acc 0.453125\n",
            "2022-05-05T21:15:14.729546: step 7064, loss 1.27213, acc 0.5625\n",
            "2022-05-05T21:15:15.438320: step 7065, loss 1.25305, acc 0.609375\n",
            "2022-05-05T21:15:16.122158: step 7066, loss 1.29555, acc 0.59375\n",
            "2022-05-05T21:15:16.809635: step 7067, loss 1.23762, acc 0.640625\n",
            "2022-05-05T21:15:17.530663: step 7068, loss 1.4112, acc 0.484375\n",
            "2022-05-05T21:15:18.202641: step 7069, loss 1.39337, acc 0.46875\n",
            "2022-05-05T21:15:18.900831: step 7070, loss 1.46892, acc 0.5\n",
            "2022-05-05T21:15:19.572280: step 7071, loss 1.51178, acc 0.484375\n",
            "2022-05-05T21:15:20.248755: step 7072, loss 1.68796, acc 0.4375\n",
            "2022-05-05T21:15:20.935664: step 7073, loss 1.47376, acc 0.515625\n",
            "2022-05-05T21:15:21.622753: step 7074, loss 1.52828, acc 0.40625\n",
            "2022-05-05T21:15:22.313594: step 7075, loss 1.22796, acc 0.5625\n",
            "2022-05-05T21:15:23.006443: step 7076, loss 1.40132, acc 0.484375\n",
            "2022-05-05T21:15:23.695033: step 7077, loss 1.55352, acc 0.421875\n",
            "2022-05-05T21:15:24.365672: step 7078, loss 1.3726, acc 0.5625\n",
            "2022-05-05T21:15:25.036688: step 7079, loss 1.48962, acc 0.46875\n",
            "2022-05-05T21:15:25.709205: step 7080, loss 1.35833, acc 0.5\n",
            "2022-05-05T21:15:26.390447: step 7081, loss 1.55724, acc 0.5\n",
            "2022-05-05T21:15:27.024113: step 7082, loss 1.53906, acc 0.453125\n",
            "2022-05-05T21:15:27.678408: step 7083, loss 1.55153, acc 0.453125\n",
            "2022-05-05T21:15:28.332538: step 7084, loss 1.34681, acc 0.53125\n",
            "2022-05-05T21:15:29.035267: step 7085, loss 1.36814, acc 0.53125\n",
            "2022-05-05T21:15:29.724868: step 7086, loss 1.33937, acc 0.578125\n",
            "2022-05-05T21:15:30.455831: step 7087, loss 1.36699, acc 0.46875\n",
            "2022-05-05T21:15:31.171906: step 7088, loss 1.42566, acc 0.515625\n",
            "2022-05-05T21:15:31.895629: step 7089, loss 1.26933, acc 0.625\n",
            "2022-05-05T21:15:32.607593: step 7090, loss 1.1906, acc 0.609375\n",
            "2022-05-05T21:15:33.331453: step 7091, loss 1.37496, acc 0.515625\n",
            "2022-05-05T21:15:34.073131: step 7092, loss 1.56204, acc 0.40625\n",
            "2022-05-05T21:15:34.776524: step 7093, loss 1.39471, acc 0.5\n",
            "2022-05-05T21:15:35.498326: step 7094, loss 1.27807, acc 0.5625\n",
            "2022-05-05T21:15:36.215046: step 7095, loss 1.62313, acc 0.484375\n",
            "2022-05-05T21:15:36.905444: step 7096, loss 1.31806, acc 0.59375\n",
            "2022-05-05T21:15:37.654567: step 7097, loss 1.3729, acc 0.546875\n",
            "2022-05-05T21:15:38.367427: step 7098, loss 1.36205, acc 0.578125\n",
            "2022-05-05T21:15:39.097396: step 7099, loss 1.47927, acc 0.515625\n",
            "2022-05-05T21:15:39.797305: step 7100, loss 1.41312, acc 0.421875\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:15:39.949599: step 7100, loss 1.82009, acc 0.33\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-7100\n",
            "\n",
            "2022-05-05T21:15:40.844973: step 7101, loss 1.50587, acc 0.4375\n",
            "2022-05-05T21:15:41.507216: step 7102, loss 1.2823, acc 0.53125\n",
            "2022-05-05T21:15:42.142072: step 7103, loss 1.46166, acc 0.46875\n",
            "2022-05-05T21:15:42.801930: step 7104, loss 1.37028, acc 0.5625\n",
            "2022-05-05T21:15:43.435967: step 7105, loss 1.42228, acc 0.484375\n",
            "2022-05-05T21:15:44.058978: step 7106, loss 1.12292, acc 0.625\n",
            "2022-05-05T21:15:44.717927: step 7107, loss 1.31414, acc 0.53125\n",
            "2022-05-05T21:15:45.340314: step 7108, loss 1.34723, acc 0.578125\n",
            "2022-05-05T21:15:45.962960: step 7109, loss 1.42266, acc 0.5625\n",
            "2022-05-05T21:15:46.663402: step 7110, loss 1.29356, acc 0.53125\n",
            "2022-05-05T21:15:47.380518: step 7111, loss 1.35178, acc 0.5625\n",
            "2022-05-05T21:15:48.067709: step 7112, loss 1.36045, acc 0.578125\n",
            "2022-05-05T21:15:48.782809: step 7113, loss 1.37615, acc 0.546875\n",
            "2022-05-05T21:15:49.504499: step 7114, loss 1.43913, acc 0.484375\n",
            "2022-05-05T21:15:50.191852: step 7115, loss 1.44761, acc 0.5\n",
            "2022-05-05T21:15:50.895623: step 7116, loss 1.56622, acc 0.4375\n",
            "2022-05-05T21:15:51.576882: step 7117, loss 1.3134, acc 0.5625\n",
            "2022-05-05T21:15:52.264158: step 7118, loss 1.17478, acc 0.59375\n",
            "2022-05-05T21:15:52.960652: step 7119, loss 1.32654, acc 0.5625\n",
            "2022-05-05T21:15:53.647177: step 7120, loss 1.20689, acc 0.65625\n",
            "2022-05-05T21:15:54.312180: step 7121, loss 1.52736, acc 0.4375\n",
            "2022-05-05T21:15:54.985692: step 7122, loss 1.30083, acc 0.53125\n",
            "2022-05-05T21:15:55.670320: step 7123, loss 1.0841, acc 0.6875\n",
            "2022-05-05T21:15:56.323916: step 7124, loss 1.48607, acc 0.453125\n",
            "2022-05-05T21:15:57.019963: step 7125, loss 1.51909, acc 0.5\n",
            "2022-05-05T21:15:57.710872: step 7126, loss 1.23156, acc 0.609375\n",
            "2022-05-05T21:15:58.394784: step 7127, loss 1.32815, acc 0.53125\n",
            "2022-05-05T21:15:59.027699: step 7128, loss 1.16195, acc 0.578125\n",
            "2022-05-05T21:15:59.664199: step 7129, loss 1.42349, acc 0.515625\n",
            "2022-05-05T21:16:00.299252: step 7130, loss 1.21786, acc 0.609375\n",
            "2022-05-05T21:16:00.935113: step 7131, loss 1.55622, acc 0.46875\n",
            "2022-05-05T21:16:01.626116: step 7132, loss 1.37571, acc 0.59375\n",
            "2022-05-05T21:16:02.298702: step 7133, loss 1.39365, acc 0.515625\n",
            "2022-05-05T21:16:02.961590: step 7134, loss 1.4678, acc 0.46875\n",
            "2022-05-05T21:16:03.592205: step 7135, loss 1.16394, acc 0.640625\n",
            "2022-05-05T21:16:04.228631: step 7136, loss 1.44868, acc 0.484375\n",
            "2022-05-05T21:16:04.878700: step 7137, loss 1.4084, acc 0.546875\n",
            "2022-05-05T21:16:05.535964: step 7138, loss 1.27194, acc 0.609375\n",
            "2022-05-05T21:16:06.169955: step 7139, loss 1.38772, acc 0.53125\n",
            "2022-05-05T21:16:06.786250: step 7140, loss 1.58553, acc 0.4375\n",
            "2022-05-05T21:16:07.408525: step 7141, loss 1.27488, acc 0.609375\n",
            "2022-05-05T21:16:08.019561: step 7142, loss 1.57773, acc 0.453125\n",
            "2022-05-05T21:16:08.665102: step 7143, loss 1.3471, acc 0.53125\n",
            "2022-05-05T21:16:09.280613: step 7144, loss 1.46754, acc 0.5\n",
            "2022-05-05T21:16:09.896188: step 7145, loss 1.31799, acc 0.515625\n",
            "2022-05-05T21:16:10.525547: step 7146, loss 1.2115, acc 0.59375\n",
            "2022-05-05T21:16:11.130972: step 7147, loss 1.57079, acc 0.484375\n",
            "2022-05-05T21:16:11.743817: step 7148, loss 1.35452, acc 0.515625\n",
            "2022-05-05T21:16:12.363430: step 7149, loss 1.30852, acc 0.609375\n",
            "2022-05-05T21:16:12.964502: step 7150, loss 1.36518, acc 0.5\n",
            "2022-05-05T21:16:13.578251: step 7151, loss 1.39622, acc 0.515625\n",
            "2022-05-05T21:16:14.186240: step 7152, loss 1.39602, acc 0.484375\n",
            "2022-05-05T21:16:14.821018: step 7153, loss 1.15462, acc 0.625\n",
            "2022-05-05T21:16:15.440176: step 7154, loss 1.40487, acc 0.5\n",
            "2022-05-05T21:16:16.073840: step 7155, loss 1.42313, acc 0.578125\n",
            "2022-05-05T21:16:16.693304: step 7156, loss 1.47637, acc 0.53125\n",
            "2022-05-05T21:16:17.294326: step 7157, loss 1.41383, acc 0.53125\n",
            "2022-05-05T21:16:17.909932: step 7158, loss 1.40977, acc 0.4375\n",
            "2022-05-05T21:16:18.515420: step 7159, loss 1.36614, acc 0.515625\n",
            "2022-05-05T21:16:19.138289: step 7160, loss 1.34116, acc 0.59375\n",
            "2022-05-05T21:16:19.742823: step 7161, loss 1.52167, acc 0.375\n",
            "2022-05-05T21:16:20.343524: step 7162, loss 1.47014, acc 0.46875\n",
            "2022-05-05T21:16:20.953332: step 7163, loss 1.49902, acc 0.46875\n",
            "2022-05-05T21:16:21.568605: step 7164, loss 1.33125, acc 0.484375\n",
            "2022-05-05T21:16:22.167217: step 7165, loss 1.2717, acc 0.671875\n",
            "2022-05-05T21:16:22.780668: step 7166, loss 1.53559, acc 0.484375\n",
            "2022-05-05T21:16:23.373223: step 7167, loss 1.40221, acc 0.484375\n",
            "2022-05-05T21:16:23.994250: step 7168, loss 1.45494, acc 0.53125\n",
            "2022-05-05T21:16:24.615939: step 7169, loss 1.5939, acc 0.40625\n",
            "2022-05-05T21:16:25.223935: step 7170, loss 1.24192, acc 0.546875\n",
            "2022-05-05T21:16:25.860847: step 7171, loss 1.47512, acc 0.46875\n",
            "2022-05-05T21:16:26.480448: step 7172, loss 1.29144, acc 0.546875\n",
            "2022-05-05T21:16:27.115316: step 7173, loss 1.41957, acc 0.515625\n",
            "2022-05-05T21:16:27.731958: step 7174, loss 1.25621, acc 0.546875\n",
            "2022-05-05T21:16:28.331659: step 7175, loss 1.31873, acc 0.5625\n",
            "2022-05-05T21:16:28.951026: step 7176, loss 1.54133, acc 0.546875\n",
            "2022-05-05T21:16:29.583105: step 7177, loss 1.54927, acc 0.4375\n",
            "2022-05-05T21:16:30.197022: step 7178, loss 1.58099, acc 0.390625\n",
            "2022-05-05T21:16:30.800965: step 7179, loss 1.3325, acc 0.546875\n",
            "2022-05-05T21:16:31.399038: step 7180, loss 1.59189, acc 0.453125\n",
            "2022-05-05T21:16:32.015435: step 7181, loss 1.28365, acc 0.609375\n",
            "2022-05-05T21:16:32.618333: step 7182, loss 1.33729, acc 0.5625\n",
            "2022-05-05T21:16:33.239671: step 7183, loss 1.39522, acc 0.421875\n",
            "2022-05-05T21:16:33.849286: step 7184, loss 1.34508, acc 0.546875\n",
            "2022-05-05T21:16:34.452424: step 7185, loss 1.40186, acc 0.578125\n",
            "2022-05-05T21:16:35.070803: step 7186, loss 1.5432, acc 0.453125\n",
            "2022-05-05T21:16:35.669923: step 7187, loss 1.14235, acc 0.59375\n",
            "2022-05-05T21:16:36.283880: step 7188, loss 1.40232, acc 0.4375\n",
            "2022-05-05T21:16:36.894825: step 7189, loss 1.45479, acc 0.46875\n",
            "2022-05-05T21:16:37.536948: step 7190, loss 1.22892, acc 0.609375\n",
            "2022-05-05T21:16:38.174886: step 7191, loss 1.28581, acc 0.53125\n",
            "2022-05-05T21:16:38.780846: step 7192, loss 1.46773, acc 0.5\n",
            "2022-05-05T21:16:39.407797: step 7193, loss 1.39835, acc 0.46875\n",
            "2022-05-05T21:16:40.034427: step 7194, loss 1.39901, acc 0.5625\n",
            "2022-05-05T21:16:40.652535: step 7195, loss 1.14923, acc 0.671875\n",
            "2022-05-05T21:16:41.287829: step 7196, loss 1.58521, acc 0.5\n",
            "2022-05-05T21:16:41.898916: step 7197, loss 1.42802, acc 0.5\n",
            "2022-05-05T21:16:42.533287: step 7198, loss 1.22897, acc 0.578125\n",
            "2022-05-05T21:16:43.169034: step 7199, loss 1.26502, acc 0.609375\n",
            "2022-05-05T21:16:43.801315: step 7200, loss 1.43211, acc 0.453125\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:16:43.945832: step 7200, loss 1.88608, acc 0.32\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-7200\n",
            "\n",
            "2022-05-05T21:16:44.793783: step 7201, loss 1.36033, acc 0.515625\n",
            "2022-05-05T21:16:45.434608: step 7202, loss 1.57767, acc 0.453125\n",
            "2022-05-05T21:16:46.068659: step 7203, loss 1.55279, acc 0.421875\n",
            "2022-05-05T21:16:46.743847: step 7204, loss 1.46073, acc 0.46875\n",
            "2022-05-05T21:16:47.399535: step 7205, loss 1.38752, acc 0.546875\n",
            "2022-05-05T21:16:48.031049: step 7206, loss 1.49308, acc 0.5\n",
            "2022-05-05T21:16:48.668818: step 7207, loss 1.36295, acc 0.578125\n",
            "2022-05-05T21:16:49.290093: step 7208, loss 1.3666, acc 0.484375\n",
            "2022-05-05T21:16:49.927365: step 7209, loss 1.25989, acc 0.59375\n",
            "2022-05-05T21:16:50.544464: step 7210, loss 1.47916, acc 0.53125\n",
            "2022-05-05T21:16:51.144571: step 7211, loss 1.39994, acc 0.59375\n",
            "2022-05-05T21:16:51.790762: step 7212, loss 1.58171, acc 0.4375\n",
            "2022-05-05T21:16:52.417686: step 7213, loss 1.23753, acc 0.609375\n",
            "2022-05-05T21:16:53.019620: step 7214, loss 1.52032, acc 0.53125\n",
            "2022-05-05T21:16:53.635698: step 7215, loss 1.36905, acc 0.515625\n",
            "2022-05-05T21:16:54.236281: step 7216, loss 1.20208, acc 0.546875\n",
            "2022-05-05T21:16:54.872840: step 7217, loss 1.52011, acc 0.5625\n",
            "2022-05-05T21:16:55.478619: step 7218, loss 1.32424, acc 0.484375\n",
            "2022-05-05T21:16:56.093012: step 7219, loss 1.6359, acc 0.421875\n",
            "2022-05-05T21:16:56.723335: step 7220, loss 1.54628, acc 0.46875\n",
            "2022-05-05T21:16:57.337617: step 7221, loss 1.42814, acc 0.578125\n",
            "2022-05-05T21:16:57.953279: step 7222, loss 1.52243, acc 0.53125\n",
            "2022-05-05T21:16:58.560948: step 7223, loss 1.35115, acc 0.515625\n",
            "2022-05-05T21:16:59.181740: step 7224, loss 1.53833, acc 0.515625\n",
            "2022-05-05T21:16:59.820328: step 7225, loss 1.21126, acc 0.625\n",
            "2022-05-05T21:17:00.482665: step 7226, loss 1.46105, acc 0.546875\n",
            "2022-05-05T21:17:01.136476: step 7227, loss 1.47312, acc 0.5\n",
            "2022-05-05T21:17:01.793658: step 7228, loss 1.29531, acc 0.59375\n",
            "2022-05-05T21:17:02.442210: step 7229, loss 1.46286, acc 0.4375\n",
            "2022-05-05T21:17:03.090408: step 7230, loss 1.38014, acc 0.609375\n",
            "2022-05-05T21:17:03.711419: step 7231, loss 1.44497, acc 0.53125\n",
            "2022-05-05T21:17:04.349231: step 7232, loss 1.21754, acc 0.59375\n",
            "2022-05-05T21:17:04.994790: step 7233, loss 1.52934, acc 0.515625\n",
            "2022-05-05T21:17:05.630724: step 7234, loss 1.54626, acc 0.5\n",
            "2022-05-05T21:17:06.297681: step 7235, loss 1.34998, acc 0.515625\n",
            "2022-05-05T21:17:06.935917: step 7236, loss 1.38297, acc 0.546875\n",
            "2022-05-05T21:17:07.550873: step 7237, loss 1.59367, acc 0.453125\n",
            "2022-05-05T21:17:08.176544: step 7238, loss 1.4969, acc 0.53125\n",
            "2022-05-05T21:17:08.806645: step 7239, loss 1.46079, acc 0.484375\n",
            "2022-05-05T21:17:09.443946: step 7240, loss 1.35656, acc 0.578125\n",
            "2022-05-05T21:17:10.068863: step 7241, loss 1.62508, acc 0.5\n",
            "2022-05-05T21:17:10.705913: step 7242, loss 1.29207, acc 0.515625\n",
            "2022-05-05T21:17:11.328715: step 7243, loss 1.48016, acc 0.46875\n",
            "2022-05-05T21:17:11.929228: step 7244, loss 1.50061, acc 0.421875\n",
            "2022-05-05T21:17:12.543295: step 7245, loss 1.78101, acc 0.375\n",
            "2022-05-05T21:17:13.167207: step 7246, loss 1.38053, acc 0.578125\n",
            "2022-05-05T21:17:13.785668: step 7247, loss 1.59704, acc 0.421875\n",
            "2022-05-05T21:17:14.402921: step 7248, loss 1.51906, acc 0.5\n",
            "2022-05-05T21:17:15.005741: step 7249, loss 1.36762, acc 0.546875\n",
            "2022-05-05T21:17:15.616865: step 7250, loss 1.4451, acc 0.453125\n",
            "2022-05-05T21:17:16.228760: step 7251, loss 1.19329, acc 0.59375\n",
            "2022-05-05T21:17:16.843825: step 7252, loss 1.53803, acc 0.421875\n",
            "2022-05-05T21:17:17.459443: step 7253, loss 1.48241, acc 0.5\n",
            "2022-05-05T21:17:18.080632: step 7254, loss 1.49964, acc 0.453125\n",
            "2022-05-05T21:17:18.706640: step 7255, loss 1.48312, acc 0.5\n",
            "2022-05-05T21:17:19.339649: step 7256, loss 1.3538, acc 0.484375\n",
            "2022-05-05T21:17:19.976766: step 7257, loss 1.37656, acc 0.53125\n",
            "2022-05-05T21:17:20.614354: step 7258, loss 1.34439, acc 0.5625\n",
            "2022-05-05T21:17:21.263824: step 7259, loss 1.37491, acc 0.5\n",
            "2022-05-05T21:17:21.898284: step 7260, loss 1.51867, acc 0.453125\n",
            "2022-05-05T21:17:22.527204: step 7261, loss 1.42268, acc 0.515625\n",
            "2022-05-05T21:17:23.142503: step 7262, loss 1.41025, acc 0.5625\n",
            "2022-05-05T21:17:23.770399: step 7263, loss 1.44145, acc 0.421875\n",
            "2022-05-05T21:17:24.378341: step 7264, loss 1.24609, acc 0.5\n",
            "2022-05-05T21:17:25.006120: step 7265, loss 1.4742, acc 0.484375\n",
            "2022-05-05T21:17:25.627567: step 7266, loss 1.32017, acc 0.5625\n",
            "2022-05-05T21:17:26.245940: step 7267, loss 1.50369, acc 0.53125\n",
            "2022-05-05T21:17:26.880103: step 7268, loss 1.41417, acc 0.546875\n",
            "2022-05-05T21:17:27.491656: step 7269, loss 1.44408, acc 0.453125\n",
            "2022-05-05T21:17:28.114331: step 7270, loss 1.51238, acc 0.484375\n",
            "2022-05-05T21:17:28.745432: step 7271, loss 1.52105, acc 0.4375\n",
            "2022-05-05T21:17:29.360648: step 7272, loss 1.47681, acc 0.5\n",
            "2022-05-05T21:17:29.987743: step 7273, loss 1.42744, acc 0.484375\n",
            "2022-05-05T21:17:30.594554: step 7274, loss 1.72557, acc 0.375\n",
            "2022-05-05T21:17:31.243916: step 7275, loss 1.39759, acc 0.53125\n",
            "2022-05-05T21:17:31.882188: step 7276, loss 1.53189, acc 0.421875\n",
            "2022-05-05T21:17:32.490373: step 7277, loss 1.305, acc 0.625\n",
            "2022-05-05T21:17:33.127944: step 7278, loss 1.28319, acc 0.625\n",
            "2022-05-05T21:17:33.798167: step 7279, loss 1.37163, acc 0.515625\n",
            "2022-05-05T21:17:34.441590: step 7280, loss 1.4696, acc 0.453125\n",
            "2022-05-05T21:17:35.086551: step 7281, loss 1.50285, acc 0.53125\n",
            "2022-05-05T21:17:35.698833: step 7282, loss 1.19906, acc 0.578125\n",
            "2022-05-05T21:17:36.318097: step 7283, loss 1.48056, acc 0.453125\n",
            "2022-05-05T21:17:36.952303: step 7284, loss 1.21098, acc 0.546875\n",
            "2022-05-05T21:17:37.552022: step 7285, loss 1.3965, acc 0.53125\n",
            "2022-05-05T21:17:38.168301: step 7286, loss 1.40021, acc 0.421875\n",
            "2022-05-05T21:17:38.767026: step 7287, loss 1.41945, acc 0.515625\n",
            "2022-05-05T21:17:39.384358: step 7288, loss 1.4734, acc 0.453125\n",
            "2022-05-05T21:17:39.995649: step 7289, loss 1.40749, acc 0.5\n",
            "2022-05-05T21:17:40.599431: step 7290, loss 1.55117, acc 0.46875\n",
            "2022-05-05T21:17:41.223771: step 7291, loss 1.43896, acc 0.5\n",
            "2022-05-05T21:17:41.862633: step 7292, loss 1.56154, acc 0.421875\n",
            "2022-05-05T21:17:42.482970: step 7293, loss 1.47779, acc 0.5\n",
            "2022-05-05T21:17:43.101393: step 7294, loss 1.4361, acc 0.484375\n",
            "2022-05-05T21:17:43.703923: step 7295, loss 1.46672, acc 0.46875\n",
            "2022-05-05T21:17:44.326286: step 7296, loss 1.40078, acc 0.484375\n",
            "2022-05-05T21:17:44.933617: step 7297, loss 1.41672, acc 0.515625\n",
            "2022-05-05T21:17:45.543111: step 7298, loss 1.34803, acc 0.546875\n",
            "2022-05-05T21:17:46.165010: step 7299, loss 1.39109, acc 0.53125\n",
            "2022-05-05T21:17:46.776001: step 7300, loss 1.45913, acc 0.4375\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:17:46.919762: step 7300, loss 2.02561, acc 0.31\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-7300\n",
            "\n",
            "2022-05-05T21:17:47.728293: step 7301, loss 1.37076, acc 0.46875\n",
            "2022-05-05T21:17:48.345375: step 7302, loss 1.36468, acc 0.46875\n",
            "2022-05-05T21:17:48.950168: step 7303, loss 1.41173, acc 0.515625\n",
            "2022-05-05T21:17:49.564286: step 7304, loss 1.37481, acc 0.546875\n",
            "2022-05-05T21:17:50.173729: step 7305, loss 1.39443, acc 0.484375\n",
            "2022-05-05T21:17:50.794910: step 7306, loss 1.413, acc 0.515625\n",
            "2022-05-05T21:17:51.415065: step 7307, loss 1.37945, acc 0.53125\n",
            "2022-05-05T21:17:52.064762: step 7308, loss 1.54703, acc 0.40625\n",
            "2022-05-05T21:17:52.695968: step 7309, loss 1.45875, acc 0.53125\n",
            "2022-05-05T21:17:53.319371: step 7310, loss 1.54421, acc 0.515625\n",
            "2022-05-05T21:17:53.937522: step 7311, loss 1.80685, acc 0.3125\n",
            "2022-05-05T21:17:54.549059: step 7312, loss 1.51586, acc 0.484375\n",
            "2022-05-05T21:17:55.159755: step 7313, loss 1.33966, acc 0.53125\n",
            "2022-05-05T21:17:55.793374: step 7314, loss 1.35429, acc 0.5625\n",
            "2022-05-05T21:17:56.417099: step 7315, loss 1.52536, acc 0.40625\n",
            "2022-05-05T21:17:57.039029: step 7316, loss 1.41718, acc 0.515625\n",
            "2022-05-05T21:17:57.681809: step 7317, loss 1.47685, acc 0.46875\n",
            "2022-05-05T21:17:58.308666: step 7318, loss 1.60985, acc 0.453125\n",
            "2022-05-05T21:17:58.947374: step 7319, loss 1.50196, acc 0.515625\n",
            "2022-05-05T21:17:59.558083: step 7320, loss 1.39774, acc 0.53125\n",
            "2022-05-05T21:18:00.167924: step 7321, loss 1.49365, acc 0.453125\n",
            "2022-05-05T21:18:00.790786: step 7322, loss 1.26731, acc 0.609375\n",
            "2022-05-05T21:18:01.404885: step 7323, loss 1.4156, acc 0.546875\n",
            "2022-05-05T21:18:02.039857: step 7324, loss 1.19556, acc 0.609375\n",
            "2022-05-05T21:18:02.656365: step 7325, loss 1.49961, acc 0.515625\n",
            "2022-05-05T21:18:03.262927: step 7326, loss 1.43383, acc 0.515625\n",
            "2022-05-05T21:18:03.907720: step 7327, loss 1.54556, acc 0.421875\n",
            "2022-05-05T21:18:04.526488: step 7328, loss 1.54994, acc 0.421875\n",
            "2022-05-05T21:18:05.145149: step 7329, loss 1.67698, acc 0.390625\n",
            "2022-05-05T21:18:05.763647: step 7330, loss 1.56044, acc 0.421875\n",
            "2022-05-05T21:18:06.364212: step 7331, loss 1.51642, acc 0.484375\n",
            "2022-05-05T21:18:06.990073: step 7332, loss 1.2734, acc 0.578125\n",
            "2022-05-05T21:18:07.590731: step 7333, loss 1.33438, acc 0.53125\n",
            "2022-05-05T21:18:08.205197: step 7334, loss 1.41307, acc 0.46875\n",
            "2022-05-05T21:18:08.817216: step 7335, loss 1.49404, acc 0.40625\n",
            "2022-05-05T21:18:09.423209: step 7336, loss 1.68199, acc 0.421875\n",
            "2022-05-05T21:18:10.040405: step 7337, loss 1.2951, acc 0.515625\n",
            "2022-05-05T21:18:10.647836: step 7338, loss 1.36124, acc 0.515625\n",
            "2022-05-05T21:18:11.263111: step 7339, loss 1.29482, acc 0.546875\n",
            "2022-05-05T21:18:11.866710: step 7340, loss 1.63476, acc 0.375\n",
            "2022-05-05T21:18:12.507164: step 7341, loss 1.50608, acc 0.40625\n",
            "2022-05-05T21:18:13.120404: step 7342, loss 1.20325, acc 0.578125\n",
            "2022-05-05T21:18:13.729774: step 7343, loss 1.44614, acc 0.5625\n",
            "2022-05-05T21:18:14.340893: step 7344, loss 1.51798, acc 0.421875\n",
            "2022-05-05T21:18:14.946252: step 7345, loss 1.3209, acc 0.578125\n",
            "2022-05-05T21:18:15.577628: step 7346, loss 1.25342, acc 0.609375\n",
            "2022-05-05T21:18:16.199409: step 7347, loss 1.62206, acc 0.484375\n",
            "2022-05-05T21:18:16.795124: step 7348, loss 1.58443, acc 0.515625\n",
            "2022-05-05T21:18:17.411270: step 7349, loss 1.43656, acc 0.515625\n",
            "2022-05-05T21:18:18.024507: step 7350, loss 1.49335, acc 0.5\n",
            "2022-05-05T21:18:18.648476: step 7351, loss 1.52599, acc 0.453125\n",
            "2022-05-05T21:18:19.266118: step 7352, loss 1.39764, acc 0.5\n",
            "2022-05-05T21:18:19.879980: step 7353, loss 1.43107, acc 0.5\n",
            "2022-05-05T21:18:20.495795: step 7354, loss 1.27974, acc 0.546875\n",
            "2022-05-05T21:18:21.107597: step 7355, loss 1.42026, acc 0.46875\n",
            "2022-05-05T21:18:21.722202: step 7356, loss 1.37226, acc 0.53125\n",
            "2022-05-05T21:18:22.488473: step 7357, loss 1.34846, acc 0.578125\n",
            "2022-05-05T21:18:23.303515: step 7358, loss 1.44705, acc 0.5\n",
            "2022-05-05T21:18:23.941900: step 7359, loss 1.37134, acc 0.5\n",
            "2022-05-05T21:18:24.560303: step 7360, loss 1.34604, acc 0.453125\n",
            "2022-05-05T21:18:25.178660: step 7361, loss 1.51644, acc 0.46875\n",
            "2022-05-05T21:18:25.825587: step 7362, loss 1.62323, acc 0.40625\n",
            "2022-05-05T21:18:26.453392: step 7363, loss 1.54489, acc 0.328125\n",
            "2022-05-05T21:18:27.067344: step 7364, loss 1.5744, acc 0.4375\n",
            "2022-05-05T21:18:27.690860: step 7365, loss 1.75825, acc 0.421875\n",
            "2022-05-05T21:18:28.299593: step 7366, loss 1.54723, acc 0.46875\n",
            "2022-05-05T21:18:28.922660: step 7367, loss 1.40243, acc 0.5\n",
            "2022-05-05T21:18:29.536825: step 7368, loss 1.44356, acc 0.515625\n",
            "2022-05-05T21:18:30.147186: step 7369, loss 1.40084, acc 0.4375\n",
            "2022-05-05T21:18:30.773705: step 7370, loss 1.41765, acc 0.484375\n",
            "2022-05-05T21:18:31.377649: step 7371, loss 1.51096, acc 0.453125\n",
            "2022-05-05T21:18:31.995854: step 7372, loss 1.45382, acc 0.5\n",
            "2022-05-05T21:18:32.613406: step 7373, loss 1.6244, acc 0.390625\n",
            "2022-05-05T21:18:33.257883: step 7374, loss 1.47203, acc 0.515625\n",
            "2022-05-05T21:18:33.896914: step 7375, loss 1.49493, acc 0.453125\n",
            "2022-05-05T21:18:34.509519: step 7376, loss 1.36913, acc 0.515625\n",
            "2022-05-05T21:18:35.129459: step 7377, loss 1.62652, acc 0.40625\n",
            "2022-05-05T21:18:35.753621: step 7378, loss 1.56759, acc 0.46875\n",
            "2022-05-05T21:18:36.368274: step 7379, loss 1.57014, acc 0.5625\n",
            "2022-05-05T21:18:36.981749: step 7380, loss 1.31036, acc 0.5\n",
            "2022-05-05T21:18:37.593121: step 7381, loss 1.47956, acc 0.46875\n",
            "2022-05-05T21:18:38.211116: step 7382, loss 1.32603, acc 0.546875\n",
            "2022-05-05T21:18:38.829316: step 7383, loss 1.58306, acc 0.390625\n",
            "2022-05-05T21:18:39.443106: step 7384, loss 1.22636, acc 0.625\n",
            "2022-05-05T21:18:40.061946: step 7385, loss 1.2792, acc 0.59375\n",
            "2022-05-05T21:18:40.666519: step 7386, loss 1.51576, acc 0.4375\n",
            "2022-05-05T21:18:41.282936: step 7387, loss 1.37258, acc 0.546875\n",
            "2022-05-05T21:18:41.883811: step 7388, loss 1.42968, acc 0.5\n",
            "2022-05-05T21:18:42.492944: step 7389, loss 1.39099, acc 0.546875\n",
            "2022-05-05T21:18:43.111086: step 7390, loss 1.72539, acc 0.375\n",
            "2022-05-05T21:18:43.733021: step 7391, loss 1.32469, acc 0.546875\n",
            "2022-05-05T21:18:44.348086: step 7392, loss 1.34282, acc 0.578125\n",
            "2022-05-05T21:18:44.953486: step 7393, loss 1.27799, acc 0.484375\n",
            "2022-05-05T21:18:45.575485: step 7394, loss 1.35724, acc 0.53125\n",
            "2022-05-05T21:18:46.197381: step 7395, loss 1.23922, acc 0.578125\n",
            "2022-05-05T21:18:46.804418: step 7396, loss 1.41908, acc 0.609375\n",
            "2022-05-05T21:18:47.436757: step 7397, loss 1.4361, acc 0.5625\n",
            "2022-05-05T21:18:48.051843: step 7398, loss 1.45317, acc 0.46875\n",
            "2022-05-05T21:18:48.670220: step 7399, loss 1.27756, acc 0.484375\n",
            "2022-05-05T21:18:49.284708: step 7400, loss 1.39027, acc 0.515625\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:18:49.433534: step 7400, loss 2.05879, acc 0.26\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-7400\n",
            "\n",
            "2022-05-05T21:18:50.245374: step 7401, loss 1.36351, acc 0.5\n",
            "2022-05-05T21:18:50.847702: step 7402, loss 1.42645, acc 0.484375\n",
            "2022-05-05T21:18:51.462718: step 7403, loss 1.3543, acc 0.53125\n",
            "2022-05-05T21:18:52.064215: step 7404, loss 1.55086, acc 0.4375\n",
            "2022-05-05T21:18:52.681697: step 7405, loss 1.32317, acc 0.65625\n",
            "2022-05-05T21:18:53.299433: step 7406, loss 1.4214, acc 0.609375\n",
            "2022-05-05T21:18:53.925259: step 7407, loss 1.38914, acc 0.515625\n",
            "2022-05-05T21:18:54.538325: step 7408, loss 1.57649, acc 0.421875\n",
            "2022-05-05T21:18:55.163462: step 7409, loss 1.38551, acc 0.53125\n",
            "2022-05-05T21:18:55.781858: step 7410, loss 1.46546, acc 0.484375\n",
            "2022-05-05T21:18:56.402015: step 7411, loss 1.28773, acc 0.59375\n",
            "2022-05-05T21:18:56.995973: step 7412, loss 1.54435, acc 0.53125\n",
            "2022-05-05T21:18:57.626639: step 7413, loss 1.45474, acc 0.453125\n",
            "2022-05-05T21:18:58.224510: step 7414, loss 1.42096, acc 0.546875\n",
            "2022-05-05T21:18:58.860587: step 7415, loss 1.40949, acc 0.53125\n",
            "2022-05-05T21:18:59.481726: step 7416, loss 1.36855, acc 0.53125\n",
            "2022-05-05T21:19:00.084867: step 7417, loss 1.49132, acc 0.453125\n",
            "2022-05-05T21:19:00.709173: step 7418, loss 1.42566, acc 0.578125\n",
            "2022-05-05T21:19:01.326551: step 7419, loss 1.53544, acc 0.46875\n",
            "2022-05-05T21:19:01.942264: step 7420, loss 1.43883, acc 0.53125\n",
            "2022-05-05T21:19:02.553626: step 7421, loss 1.52789, acc 0.5\n",
            "2022-05-05T21:19:03.170921: step 7422, loss 1.42235, acc 0.5625\n",
            "2022-05-05T21:19:03.788328: step 7423, loss 1.66278, acc 0.4375\n",
            "2022-05-05T21:19:04.428392: step 7424, loss 1.35795, acc 0.59375\n",
            "2022-05-05T21:19:05.044705: step 7425, loss 1.57261, acc 0.5\n",
            "2022-05-05T21:19:05.686048: step 7426, loss 1.59701, acc 0.4375\n",
            "2022-05-05T21:19:06.287043: step 7427, loss 1.62514, acc 0.40625\n",
            "2022-05-05T21:19:06.904293: step 7428, loss 1.58059, acc 0.484375\n",
            "2022-05-05T21:19:07.513872: step 7429, loss 1.38344, acc 0.53125\n",
            "2022-05-05T21:19:08.156651: step 7430, loss 1.80585, acc 0.375\n",
            "2022-05-05T21:19:08.777291: step 7431, loss 1.43749, acc 0.46875\n",
            "2022-05-05T21:19:09.388194: step 7432, loss 1.53013, acc 0.359375\n",
            "2022-05-05T21:19:10.004685: step 7433, loss 1.53698, acc 0.453125\n",
            "2022-05-05T21:19:10.628054: step 7434, loss 1.26891, acc 0.5625\n",
            "2022-05-05T21:19:11.240686: step 7435, loss 1.42801, acc 0.484375\n",
            "2022-05-05T21:19:11.860494: step 7436, loss 1.60231, acc 0.390625\n",
            "2022-05-05T21:19:12.468080: step 7437, loss 1.44052, acc 0.484375\n",
            "2022-05-05T21:19:13.100106: step 7438, loss 1.53208, acc 0.4375\n",
            "2022-05-05T21:19:13.713127: step 7439, loss 1.45613, acc 0.46875\n",
            "2022-05-05T21:19:14.352405: step 7440, loss 1.40115, acc 0.515625\n",
            "2022-05-05T21:19:14.980925: step 7441, loss 1.50551, acc 0.375\n",
            "2022-05-05T21:19:15.589184: step 7442, loss 1.56781, acc 0.546875\n",
            "2022-05-05T21:19:16.196226: step 7443, loss 1.47186, acc 0.53125\n",
            "2022-05-05T21:19:16.807132: step 7444, loss 1.58743, acc 0.46875\n",
            "2022-05-05T21:19:17.419955: step 7445, loss 1.3228, acc 0.46875\n",
            "2022-05-05T21:19:18.054259: step 7446, loss 1.29029, acc 0.59375\n",
            "2022-05-05T21:19:18.671182: step 7447, loss 1.57485, acc 0.453125\n",
            "2022-05-05T21:19:19.294536: step 7448, loss 1.42217, acc 0.4375\n",
            "2022-05-05T21:19:19.912308: step 7449, loss 1.48232, acc 0.46875\n",
            "2022-05-05T21:19:20.536340: step 7450, loss 1.45524, acc 0.453125\n",
            "2022-05-05T21:19:21.148297: step 7451, loss 1.47214, acc 0.5\n",
            "2022-05-05T21:19:21.752915: step 7452, loss 1.25976, acc 0.5625\n",
            "2022-05-05T21:19:22.369178: step 7453, loss 1.42132, acc 0.5625\n",
            "2022-05-05T21:19:22.979823: step 7454, loss 1.52869, acc 0.5\n",
            "2022-05-05T21:19:23.600233: step 7455, loss 1.4461, acc 0.5625\n",
            "2022-05-05T21:19:24.211631: step 7456, loss 1.39597, acc 0.53125\n",
            "2022-05-05T21:19:24.849465: step 7457, loss 1.53201, acc 0.515625\n",
            "2022-05-05T21:19:25.476493: step 7458, loss 1.37915, acc 0.484375\n",
            "2022-05-05T21:19:26.091386: step 7459, loss 1.44529, acc 0.5\n",
            "2022-05-05T21:19:26.727030: step 7460, loss 1.24054, acc 0.640625\n",
            "2022-05-05T21:19:27.358113: step 7461, loss 1.33303, acc 0.5625\n",
            "2022-05-05T21:19:27.975033: step 7462, loss 1.65241, acc 0.375\n",
            "2022-05-05T21:19:28.606472: step 7463, loss 1.75299, acc 0.453125\n",
            "2022-05-05T21:19:29.221050: step 7464, loss 1.47088, acc 0.5625\n",
            "2022-05-05T21:19:29.846633: step 7465, loss 1.33025, acc 0.578125\n",
            "2022-05-05T21:19:30.462826: step 7466, loss 1.58095, acc 0.46875\n",
            "2022-05-05T21:19:31.073377: step 7467, loss 1.58868, acc 0.4375\n",
            "2022-05-05T21:19:31.699356: step 7468, loss 1.34404, acc 0.46875\n",
            "2022-05-05T21:19:32.314585: step 7469, loss 1.63658, acc 0.4375\n",
            "2022-05-05T21:19:32.952767: step 7470, loss 1.56615, acc 0.34375\n",
            "2022-05-05T21:19:33.582537: step 7471, loss 1.58515, acc 0.421875\n",
            "2022-05-05T21:19:34.202515: step 7472, loss 1.43398, acc 0.453125\n",
            "2022-05-05T21:19:34.831663: step 7473, loss 1.47952, acc 0.515625\n",
            "2022-05-05T21:19:35.466525: step 7474, loss 1.73069, acc 0.421875\n",
            "2022-05-05T21:19:36.090121: step 7475, loss 1.52782, acc 0.4375\n",
            "2022-05-05T21:19:36.724337: step 7476, loss 1.60679, acc 0.4375\n",
            "2022-05-05T21:19:37.332684: step 7477, loss 1.5811, acc 0.375\n",
            "2022-05-05T21:19:37.968276: step 7478, loss 1.55269, acc 0.46875\n",
            "2022-05-05T21:19:38.572118: step 7479, loss 1.59203, acc 0.46875\n",
            "2022-05-05T21:19:39.191831: step 7480, loss 1.32587, acc 0.46875\n",
            "2022-05-05T21:19:39.813281: step 7481, loss 1.39071, acc 0.59375\n",
            "2022-05-05T21:19:40.420939: step 7482, loss 1.32997, acc 0.515625\n",
            "2022-05-05T21:19:41.038936: step 7483, loss 1.37587, acc 0.515625\n",
            "2022-05-05T21:19:41.641082: step 7484, loss 1.66434, acc 0.40625\n",
            "2022-05-05T21:19:42.259029: step 7485, loss 1.68506, acc 0.359375\n",
            "2022-05-05T21:19:42.875107: step 7486, loss 1.50311, acc 0.453125\n",
            "2022-05-05T21:19:43.487793: step 7487, loss 1.60546, acc 0.4375\n",
            "2022-05-05T21:19:44.104079: step 7488, loss 1.38537, acc 0.609375\n",
            "2022-05-05T21:19:44.714695: step 7489, loss 1.53961, acc 0.46875\n",
            "2022-05-05T21:19:45.347750: step 7490, loss 1.29354, acc 0.59375\n",
            "2022-05-05T21:19:45.966971: step 7491, loss 1.34265, acc 0.546875\n",
            "2022-05-05T21:19:46.590357: step 7492, loss 1.56159, acc 0.390625\n",
            "2022-05-05T21:19:47.222083: step 7493, loss 1.22422, acc 0.546875\n",
            "2022-05-05T21:19:47.851474: step 7494, loss 1.48566, acc 0.40625\n",
            "2022-05-05T21:19:48.472079: step 7495, loss 1.44174, acc 0.53125\n",
            "2022-05-05T21:19:49.099189: step 7496, loss 1.324, acc 0.484375\n",
            "2022-05-05T21:19:49.707921: step 7497, loss 1.45711, acc 0.5\n",
            "2022-05-05T21:19:50.323033: step 7498, loss 1.65618, acc 0.359375\n",
            "2022-05-05T21:19:50.933778: step 7499, loss 1.43688, acc 0.4375\n",
            "2022-05-05T21:19:51.566120: step 7500, loss 1.39692, acc 0.484375\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:19:51.710847: step 7500, loss 1.77557, acc 0.37\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-7500\n",
            "\n",
            "2022-05-05T21:19:52.533220: step 7501, loss 1.64249, acc 0.4375\n",
            "2022-05-05T21:19:53.169688: step 7502, loss 1.47895, acc 0.4375\n",
            "2022-05-05T21:19:53.770923: step 7503, loss 1.5083, acc 0.515625\n",
            "2022-05-05T21:19:54.388746: step 7504, loss 1.45277, acc 0.5\n",
            "2022-05-05T21:19:54.999452: step 7505, loss 1.51149, acc 0.578125\n",
            "2022-05-05T21:19:55.643960: step 7506, loss 1.37313, acc 0.53125\n",
            "2022-05-05T21:19:56.254382: step 7507, loss 1.45896, acc 0.515625\n",
            "2022-05-05T21:19:56.890971: step 7508, loss 1.28368, acc 0.578125\n",
            "2022-05-05T21:19:57.518116: step 7509, loss 1.49909, acc 0.46875\n",
            "2022-05-05T21:19:58.143653: step 7510, loss 1.58784, acc 0.484375\n",
            "2022-05-05T21:19:58.769549: step 7511, loss 1.57667, acc 0.34375\n",
            "2022-05-05T21:19:59.396100: step 7512, loss 1.62897, acc 0.390625\n",
            "2022-05-05T21:20:00.012416: step 7513, loss 1.36691, acc 0.546875\n",
            "2022-05-05T21:20:00.518053: step 7514, loss 1.57973, acc 0.44898\n",
            "2022-05-05T21:20:01.145257: step 7515, loss 1.32243, acc 0.59375\n",
            "2022-05-05T21:20:01.780655: step 7516, loss 1.35276, acc 0.5\n",
            "2022-05-05T21:20:02.409170: step 7517, loss 1.27436, acc 0.53125\n",
            "2022-05-05T21:20:03.024593: step 7518, loss 1.46314, acc 0.4375\n",
            "2022-05-05T21:20:03.664167: step 7519, loss 1.38964, acc 0.5\n",
            "2022-05-05T21:20:04.284495: step 7520, loss 1.23815, acc 0.640625\n",
            "2022-05-05T21:20:04.913506: step 7521, loss 1.54617, acc 0.40625\n",
            "2022-05-05T21:20:05.537378: step 7522, loss 1.34654, acc 0.53125\n",
            "2022-05-05T21:20:06.176397: step 7523, loss 1.44011, acc 0.484375\n",
            "2022-05-05T21:20:06.804925: step 7524, loss 1.26861, acc 0.5625\n",
            "2022-05-05T21:20:07.418623: step 7525, loss 1.41187, acc 0.515625\n",
            "2022-05-05T21:20:08.051982: step 7526, loss 1.34282, acc 0.515625\n",
            "2022-05-05T21:20:08.672262: step 7527, loss 1.33223, acc 0.5625\n",
            "2022-05-05T21:20:09.287592: step 7528, loss 1.21693, acc 0.5625\n",
            "2022-05-05T21:20:09.922180: step 7529, loss 1.3935, acc 0.578125\n",
            "2022-05-05T21:20:10.532392: step 7530, loss 1.08035, acc 0.671875\n",
            "2022-05-05T21:20:11.161796: step 7531, loss 1.24433, acc 0.5625\n",
            "2022-05-05T21:20:11.785349: step 7532, loss 1.34754, acc 0.59375\n",
            "2022-05-05T21:20:12.403745: step 7533, loss 1.23843, acc 0.59375\n",
            "2022-05-05T21:20:13.021448: step 7534, loss 1.56872, acc 0.4375\n",
            "2022-05-05T21:20:13.633924: step 7535, loss 1.33076, acc 0.484375\n",
            "2022-05-05T21:20:14.250510: step 7536, loss 1.28631, acc 0.53125\n",
            "2022-05-05T21:20:14.862614: step 7537, loss 1.22965, acc 0.671875\n",
            "2022-05-05T21:20:15.475178: step 7538, loss 1.02037, acc 0.71875\n",
            "2022-05-05T21:20:16.098864: step 7539, loss 1.461, acc 0.453125\n",
            "2022-05-05T21:20:16.728883: step 7540, loss 1.25423, acc 0.625\n",
            "2022-05-05T21:20:17.342163: step 7541, loss 1.35364, acc 0.546875\n",
            "2022-05-05T21:20:17.955235: step 7542, loss 1.22912, acc 0.546875\n",
            "2022-05-05T21:20:18.564919: step 7543, loss 1.31553, acc 0.5\n",
            "2022-05-05T21:20:19.200041: step 7544, loss 1.28258, acc 0.546875\n",
            "2022-05-05T21:20:19.876103: step 7545, loss 1.241, acc 0.640625\n",
            "2022-05-05T21:20:20.560719: step 7546, loss 1.34832, acc 0.515625\n",
            "2022-05-05T21:20:21.243229: step 7547, loss 1.34097, acc 0.5\n",
            "2022-05-05T21:20:21.898351: step 7548, loss 1.47531, acc 0.515625\n",
            "2022-05-05T21:20:22.572876: step 7549, loss 1.22636, acc 0.53125\n",
            "2022-05-05T21:20:23.267169: step 7550, loss 1.28961, acc 0.59375\n",
            "2022-05-05T21:20:23.943508: step 7551, loss 1.50751, acc 0.5\n",
            "2022-05-05T21:20:24.626619: step 7552, loss 1.16333, acc 0.609375\n",
            "2022-05-05T21:20:25.299270: step 7553, loss 1.30928, acc 0.5625\n",
            "2022-05-05T21:20:25.966517: step 7554, loss 1.34175, acc 0.53125\n",
            "2022-05-05T21:20:26.695416: step 7555, loss 1.13703, acc 0.625\n",
            "2022-05-05T21:20:27.383738: step 7556, loss 1.17598, acc 0.53125\n",
            "2022-05-05T21:20:28.009731: step 7557, loss 1.40151, acc 0.46875\n",
            "2022-05-05T21:20:28.635817: step 7558, loss 1.36597, acc 0.546875\n",
            "2022-05-05T21:20:29.269082: step 7559, loss 1.13905, acc 0.59375\n",
            "2022-05-05T21:20:29.879234: step 7560, loss 1.22485, acc 0.5625\n",
            "2022-05-05T21:20:30.500086: step 7561, loss 1.4897, acc 0.4375\n",
            "2022-05-05T21:20:31.108753: step 7562, loss 1.45158, acc 0.5625\n",
            "2022-05-05T21:20:31.741926: step 7563, loss 1.25776, acc 0.515625\n",
            "2022-05-05T21:20:32.377219: step 7564, loss 1.29832, acc 0.546875\n",
            "2022-05-05T21:20:32.987683: step 7565, loss 1.27193, acc 0.59375\n",
            "2022-05-05T21:20:33.614719: step 7566, loss 1.3897, acc 0.484375\n",
            "2022-05-05T21:20:34.237984: step 7567, loss 1.22977, acc 0.671875\n",
            "2022-05-05T21:20:34.874533: step 7568, loss 1.41085, acc 0.5\n",
            "2022-05-05T21:20:35.499718: step 7569, loss 1.37342, acc 0.546875\n",
            "2022-05-05T21:20:36.122371: step 7570, loss 1.48052, acc 0.453125\n",
            "2022-05-05T21:20:36.759961: step 7571, loss 1.21042, acc 0.65625\n",
            "2022-05-05T21:20:37.401105: step 7572, loss 1.46563, acc 0.484375\n",
            "2022-05-05T21:20:38.033574: step 7573, loss 1.24719, acc 0.484375\n",
            "2022-05-05T21:20:38.661904: step 7574, loss 1.28834, acc 0.59375\n",
            "2022-05-05T21:20:39.271982: step 7575, loss 1.60724, acc 0.484375\n",
            "2022-05-05T21:20:39.909330: step 7576, loss 1.39045, acc 0.4375\n",
            "2022-05-05T21:20:40.525846: step 7577, loss 1.40944, acc 0.40625\n",
            "2022-05-05T21:20:41.152437: step 7578, loss 1.42551, acc 0.4375\n",
            "2022-05-05T21:20:41.775745: step 7579, loss 1.32457, acc 0.625\n",
            "2022-05-05T21:20:42.381236: step 7580, loss 1.37569, acc 0.5625\n",
            "2022-05-05T21:20:43.007900: step 7581, loss 1.37358, acc 0.5\n",
            "2022-05-05T21:20:43.627125: step 7582, loss 1.22292, acc 0.59375\n",
            "2022-05-05T21:20:44.255209: step 7583, loss 1.32515, acc 0.5625\n",
            "2022-05-05T21:20:44.880387: step 7584, loss 1.27253, acc 0.546875\n",
            "2022-05-05T21:20:45.486264: step 7585, loss 1.41575, acc 0.53125\n",
            "2022-05-05T21:20:46.098550: step 7586, loss 1.27108, acc 0.453125\n",
            "2022-05-05T21:20:46.715009: step 7587, loss 1.48616, acc 0.546875\n",
            "2022-05-05T21:20:47.373103: step 7588, loss 1.42, acc 0.546875\n",
            "2022-05-05T21:20:48.002492: step 7589, loss 1.28025, acc 0.484375\n",
            "2022-05-05T21:20:48.632138: step 7590, loss 1.1952, acc 0.59375\n",
            "2022-05-05T21:20:49.263767: step 7591, loss 1.31993, acc 0.5\n",
            "2022-05-05T21:20:49.881949: step 7592, loss 1.13222, acc 0.640625\n",
            "2022-05-05T21:20:50.521608: step 7593, loss 1.26157, acc 0.578125\n",
            "2022-05-05T21:20:51.143773: step 7594, loss 1.4596, acc 0.5\n",
            "2022-05-05T21:20:51.761720: step 7595, loss 1.48229, acc 0.453125\n",
            "2022-05-05T21:20:52.411841: step 7596, loss 1.19077, acc 0.640625\n",
            "2022-05-05T21:20:53.057078: step 7597, loss 1.3519, acc 0.53125\n",
            "2022-05-05T21:20:53.694849: step 7598, loss 1.32862, acc 0.46875\n",
            "2022-05-05T21:20:54.353583: step 7599, loss 1.46715, acc 0.34375\n",
            "2022-05-05T21:20:54.969096: step 7600, loss 1.32499, acc 0.546875\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:20:55.135629: step 7600, loss 2.02074, acc 0.31\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-7600\n",
            "\n",
            "2022-05-05T21:20:55.988370: step 7601, loss 1.27889, acc 0.484375\n",
            "2022-05-05T21:20:56.624526: step 7602, loss 1.52023, acc 0.5\n",
            "2022-05-05T21:20:57.280370: step 7603, loss 1.38245, acc 0.421875\n",
            "2022-05-05T21:20:57.995937: step 7604, loss 1.11935, acc 0.625\n",
            "2022-05-05T21:20:58.677417: step 7605, loss 1.38164, acc 0.484375\n",
            "2022-05-05T21:20:59.358631: step 7606, loss 1.35417, acc 0.421875\n",
            "2022-05-05T21:20:59.988436: step 7607, loss 1.36684, acc 0.53125\n",
            "2022-05-05T21:21:00.624521: step 7608, loss 1.51127, acc 0.40625\n",
            "2022-05-05T21:21:01.239152: step 7609, loss 1.45215, acc 0.546875\n",
            "2022-05-05T21:21:01.867546: step 7610, loss 1.38139, acc 0.515625\n",
            "2022-05-05T21:21:02.498741: step 7611, loss 1.18232, acc 0.5625\n",
            "2022-05-05T21:21:03.119515: step 7612, loss 1.2932, acc 0.5\n",
            "2022-05-05T21:21:03.784949: step 7613, loss 1.30379, acc 0.578125\n",
            "2022-05-05T21:21:04.422121: step 7614, loss 1.31924, acc 0.53125\n",
            "2022-05-05T21:21:05.034514: step 7615, loss 1.21059, acc 0.578125\n",
            "2022-05-05T21:21:05.661843: step 7616, loss 1.32354, acc 0.53125\n",
            "2022-05-05T21:21:06.271968: step 7617, loss 1.38616, acc 0.5\n",
            "2022-05-05T21:21:06.898352: step 7618, loss 1.33201, acc 0.640625\n",
            "2022-05-05T21:21:07.527674: step 7619, loss 1.53149, acc 0.484375\n",
            "2022-05-05T21:21:08.156162: step 7620, loss 1.36957, acc 0.53125\n",
            "2022-05-05T21:21:08.796829: step 7621, loss 1.29294, acc 0.578125\n",
            "2022-05-05T21:21:09.400114: step 7622, loss 1.49213, acc 0.40625\n",
            "2022-05-05T21:21:10.020594: step 7623, loss 1.41735, acc 0.53125\n",
            "2022-05-05T21:21:10.639332: step 7624, loss 1.31538, acc 0.546875\n",
            "2022-05-05T21:21:11.247168: step 7625, loss 1.57426, acc 0.453125\n",
            "2022-05-05T21:21:11.870473: step 7626, loss 1.30817, acc 0.609375\n",
            "2022-05-05T21:21:12.482651: step 7627, loss 1.24773, acc 0.5625\n",
            "2022-05-05T21:21:13.123536: step 7628, loss 1.19243, acc 0.59375\n",
            "2022-05-05T21:21:13.750912: step 7629, loss 1.45692, acc 0.421875\n",
            "2022-05-05T21:21:14.369546: step 7630, loss 1.29861, acc 0.59375\n",
            "2022-05-05T21:21:15.003744: step 7631, loss 1.23237, acc 0.546875\n",
            "2022-05-05T21:21:15.623134: step 7632, loss 1.33125, acc 0.53125\n",
            "2022-05-05T21:21:16.236949: step 7633, loss 1.18278, acc 0.59375\n",
            "2022-05-05T21:21:16.855000: step 7634, loss 1.18909, acc 0.609375\n",
            "2022-05-05T21:21:17.463660: step 7635, loss 1.3334, acc 0.515625\n",
            "2022-05-05T21:21:18.081955: step 7636, loss 1.47407, acc 0.546875\n",
            "2022-05-05T21:21:18.726655: step 7637, loss 1.33782, acc 0.5625\n",
            "2022-05-05T21:21:19.356758: step 7638, loss 1.20834, acc 0.609375\n",
            "2022-05-05T21:21:19.963402: step 7639, loss 1.16115, acc 0.53125\n",
            "2022-05-05T21:21:20.577265: step 7640, loss 1.47198, acc 0.484375\n",
            "2022-05-05T21:21:21.194676: step 7641, loss 1.4063, acc 0.421875\n",
            "2022-05-05T21:21:21.793207: step 7642, loss 1.41975, acc 0.46875\n",
            "2022-05-05T21:21:22.419233: step 7643, loss 1.65725, acc 0.390625\n",
            "2022-05-05T21:21:23.033937: step 7644, loss 1.32314, acc 0.578125\n",
            "2022-05-05T21:21:23.651909: step 7645, loss 1.47242, acc 0.515625\n",
            "2022-05-05T21:21:24.267445: step 7646, loss 1.23121, acc 0.53125\n",
            "2022-05-05T21:21:24.871058: step 7647, loss 1.20525, acc 0.578125\n",
            "2022-05-05T21:21:25.503920: step 7648, loss 1.74341, acc 0.359375\n",
            "2022-05-05T21:21:26.125251: step 7649, loss 1.35144, acc 0.515625\n",
            "2022-05-05T21:21:26.747409: step 7650, loss 1.20979, acc 0.546875\n",
            "2022-05-05T21:21:27.372503: step 7651, loss 1.33137, acc 0.515625\n",
            "2022-05-05T21:21:27.995591: step 7652, loss 1.23609, acc 0.609375\n",
            "2022-05-05T21:21:28.643166: step 7653, loss 1.42848, acc 0.484375\n",
            "2022-05-05T21:21:29.258237: step 7654, loss 1.65745, acc 0.421875\n",
            "2022-05-05T21:21:29.870024: step 7655, loss 1.34583, acc 0.484375\n",
            "2022-05-05T21:21:30.486842: step 7656, loss 1.39346, acc 0.59375\n",
            "2022-05-05T21:21:31.093378: step 7657, loss 1.18589, acc 0.609375\n",
            "2022-05-05T21:21:31.724294: step 7658, loss 1.19032, acc 0.578125\n",
            "2022-05-05T21:21:32.361532: step 7659, loss 1.47333, acc 0.46875\n",
            "2022-05-05T21:21:32.980634: step 7660, loss 1.31082, acc 0.5625\n",
            "2022-05-05T21:21:33.607485: step 7661, loss 1.39094, acc 0.484375\n",
            "2022-05-05T21:21:34.222407: step 7662, loss 1.26272, acc 0.5625\n",
            "2022-05-05T21:21:34.854326: step 7663, loss 1.22214, acc 0.59375\n",
            "2022-05-05T21:21:35.481051: step 7664, loss 1.53684, acc 0.4375\n",
            "2022-05-05T21:21:36.093807: step 7665, loss 1.48076, acc 0.484375\n",
            "2022-05-05T21:21:36.719124: step 7666, loss 1.25923, acc 0.609375\n",
            "2022-05-05T21:21:37.339051: step 7667, loss 1.40662, acc 0.546875\n",
            "2022-05-05T21:21:37.958549: step 7668, loss 1.31636, acc 0.578125\n",
            "2022-05-05T21:21:38.584413: step 7669, loss 1.46645, acc 0.5\n",
            "2022-05-05T21:21:39.230546: step 7670, loss 1.15415, acc 0.578125\n",
            "2022-05-05T21:21:39.858696: step 7671, loss 1.42256, acc 0.53125\n",
            "2022-05-05T21:21:40.489165: step 7672, loss 1.25745, acc 0.5\n",
            "2022-05-05T21:21:41.103909: step 7673, loss 1.45916, acc 0.484375\n",
            "2022-05-05T21:21:41.734108: step 7674, loss 1.54291, acc 0.46875\n",
            "2022-05-05T21:21:42.353392: step 7675, loss 1.44444, acc 0.546875\n",
            "2022-05-05T21:21:42.977214: step 7676, loss 1.49007, acc 0.453125\n",
            "2022-05-05T21:21:43.611478: step 7677, loss 1.54694, acc 0.5\n",
            "2022-05-05T21:21:44.243366: step 7678, loss 1.52399, acc 0.4375\n",
            "2022-05-05T21:21:44.882140: step 7679, loss 1.1826, acc 0.625\n",
            "2022-05-05T21:21:45.487462: step 7680, loss 1.23679, acc 0.5\n",
            "2022-05-05T21:21:46.126116: step 7681, loss 1.29265, acc 0.578125\n",
            "2022-05-05T21:21:46.744474: step 7682, loss 1.41362, acc 0.53125\n",
            "2022-05-05T21:21:47.347469: step 7683, loss 1.56151, acc 0.40625\n",
            "2022-05-05T21:21:47.971638: step 7684, loss 1.439, acc 0.515625\n",
            "2022-05-05T21:21:48.582064: step 7685, loss 1.57552, acc 0.46875\n",
            "2022-05-05T21:21:49.233417: step 7686, loss 1.44567, acc 0.5\n",
            "2022-05-05T21:21:49.860210: step 7687, loss 1.216, acc 0.609375\n",
            "2022-05-05T21:21:50.474397: step 7688, loss 1.36429, acc 0.5625\n",
            "2022-05-05T21:21:51.104921: step 7689, loss 1.34759, acc 0.59375\n",
            "2022-05-05T21:21:51.718439: step 7690, loss 1.40559, acc 0.46875\n",
            "2022-05-05T21:21:52.352740: step 7691, loss 1.3492, acc 0.546875\n",
            "2022-05-05T21:21:52.980105: step 7692, loss 1.56032, acc 0.53125\n",
            "2022-05-05T21:21:53.598600: step 7693, loss 1.25613, acc 0.59375\n",
            "2022-05-05T21:21:54.213595: step 7694, loss 1.4185, acc 0.515625\n",
            "2022-05-05T21:21:54.835562: step 7695, loss 1.46623, acc 0.453125\n",
            "2022-05-05T21:21:55.457343: step 7696, loss 1.31904, acc 0.5625\n",
            "2022-05-05T21:21:56.071758: step 7697, loss 1.35084, acc 0.515625\n",
            "2022-05-05T21:21:56.688791: step 7698, loss 1.25016, acc 0.59375\n",
            "2022-05-05T21:21:57.317319: step 7699, loss 1.58857, acc 0.453125\n",
            "2022-05-05T21:21:57.943376: step 7700, loss 1.31005, acc 0.546875\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:21:58.102264: step 7700, loss 2.30519, acc 0.26\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-7700\n",
            "\n",
            "2022-05-05T21:21:59.157424: step 7701, loss 1.36649, acc 0.578125\n",
            "2022-05-05T21:21:59.908929: step 7702, loss 1.38432, acc 0.5625\n",
            "2022-05-05T21:22:00.685425: step 7703, loss 1.32131, acc 0.546875\n",
            "2022-05-05T21:22:01.384848: step 7704, loss 1.42529, acc 0.453125\n",
            "2022-05-05T21:22:02.056894: step 7705, loss 1.29031, acc 0.53125\n",
            "2022-05-05T21:22:02.763915: step 7706, loss 1.2902, acc 0.5\n",
            "2022-05-05T21:22:03.462838: step 7707, loss 1.43192, acc 0.484375\n",
            "2022-05-05T21:22:04.192503: step 7708, loss 1.51734, acc 0.453125\n",
            "2022-05-05T21:22:05.073640: step 7709, loss 1.24705, acc 0.53125\n",
            "2022-05-05T21:22:05.977943: step 7710, loss 1.39512, acc 0.53125\n",
            "2022-05-05T21:22:07.008589: step 7711, loss 1.44628, acc 0.359375\n",
            "2022-05-05T21:22:07.909057: step 7712, loss 1.15348, acc 0.609375\n",
            "2022-05-05T21:22:08.515870: step 7713, loss 1.51426, acc 0.515625\n",
            "2022-05-05T21:22:09.149074: step 7714, loss 1.51799, acc 0.421875\n",
            "2022-05-05T21:22:09.781440: step 7715, loss 1.40466, acc 0.5625\n",
            "2022-05-05T21:22:10.423474: step 7716, loss 1.13396, acc 0.625\n",
            "2022-05-05T21:22:11.041735: step 7717, loss 1.27513, acc 0.578125\n",
            "2022-05-05T21:22:11.653463: step 7718, loss 1.04851, acc 0.625\n",
            "2022-05-05T21:22:12.262807: step 7719, loss 1.25167, acc 0.625\n",
            "2022-05-05T21:22:12.881788: step 7720, loss 1.29313, acc 0.59375\n",
            "2022-05-05T21:22:13.499075: step 7721, loss 1.60009, acc 0.515625\n",
            "2022-05-05T21:22:14.147155: step 7722, loss 1.37522, acc 0.484375\n",
            "2022-05-05T21:22:14.760938: step 7723, loss 1.57979, acc 0.46875\n",
            "2022-05-05T21:22:15.415060: step 7724, loss 1.39469, acc 0.484375\n",
            "2022-05-05T21:22:16.036822: step 7725, loss 1.44462, acc 0.453125\n",
            "2022-05-05T21:22:16.694336: step 7726, loss 1.56969, acc 0.46875\n",
            "2022-05-05T21:22:17.338455: step 7727, loss 1.3669, acc 0.5625\n",
            "2022-05-05T21:22:17.997082: step 7728, loss 1.207, acc 0.53125\n",
            "2022-05-05T21:22:18.624147: step 7729, loss 1.31821, acc 0.609375\n",
            "2022-05-05T21:22:19.242954: step 7730, loss 1.30102, acc 0.546875\n",
            "2022-05-05T21:22:19.909127: step 7731, loss 1.48575, acc 0.46875\n",
            "2022-05-05T21:22:20.558304: step 7732, loss 1.41815, acc 0.453125\n",
            "2022-05-05T21:22:21.183579: step 7733, loss 1.63814, acc 0.46875\n",
            "2022-05-05T21:22:21.800075: step 7734, loss 1.46652, acc 0.5\n",
            "2022-05-05T21:22:22.444409: step 7735, loss 1.55311, acc 0.484375\n",
            "2022-05-05T21:22:23.123174: step 7736, loss 1.3883, acc 0.453125\n",
            "2022-05-05T21:22:23.803715: step 7737, loss 1.47669, acc 0.484375\n",
            "2022-05-05T21:22:24.434344: step 7738, loss 1.53263, acc 0.421875\n",
            "2022-05-05T21:22:25.052260: step 7739, loss 1.46222, acc 0.390625\n",
            "2022-05-05T21:22:25.684117: step 7740, loss 1.48473, acc 0.484375\n",
            "2022-05-05T21:22:26.309939: step 7741, loss 1.38089, acc 0.578125\n",
            "2022-05-05T21:22:26.922896: step 7742, loss 1.41713, acc 0.53125\n",
            "2022-05-05T21:22:27.532925: step 7743, loss 1.33027, acc 0.578125\n",
            "2022-05-05T21:22:28.149809: step 7744, loss 1.41264, acc 0.5\n",
            "2022-05-05T21:22:28.839028: step 7745, loss 1.36577, acc 0.5\n",
            "2022-05-05T21:22:29.557395: step 7746, loss 1.39056, acc 0.515625\n",
            "2022-05-05T21:22:30.216565: step 7747, loss 1.28671, acc 0.59375\n",
            "2022-05-05T21:22:30.862457: step 7748, loss 1.3461, acc 0.5\n",
            "2022-05-05T21:22:31.496973: step 7749, loss 1.19457, acc 0.546875\n",
            "2022-05-05T21:22:32.133254: step 7750, loss 1.20633, acc 0.53125\n",
            "2022-05-05T21:22:32.769796: step 7751, loss 1.50504, acc 0.421875\n",
            "2022-05-05T21:22:33.438092: step 7752, loss 1.38284, acc 0.484375\n",
            "2022-05-05T21:22:34.063710: step 7753, loss 1.16185, acc 0.53125\n",
            "2022-05-05T21:22:34.704772: step 7754, loss 1.37321, acc 0.5\n",
            "2022-05-05T21:22:35.319491: step 7755, loss 1.15214, acc 0.546875\n",
            "2022-05-05T21:22:35.938711: step 7756, loss 1.22816, acc 0.5625\n",
            "2022-05-05T21:22:36.568488: step 7757, loss 1.50416, acc 0.53125\n",
            "2022-05-05T21:22:37.179292: step 7758, loss 1.36652, acc 0.546875\n",
            "2022-05-05T21:22:37.800787: step 7759, loss 1.60785, acc 0.453125\n",
            "2022-05-05T21:22:38.407542: step 7760, loss 1.56159, acc 0.390625\n",
            "2022-05-05T21:22:39.088406: step 7761, loss 1.3929, acc 0.515625\n",
            "2022-05-05T21:22:39.788478: step 7762, loss 1.46584, acc 0.46875\n",
            "2022-05-05T21:22:40.481670: step 7763, loss 1.2778, acc 0.5625\n",
            "2022-05-05T21:22:41.189664: step 7764, loss 1.48864, acc 0.53125\n",
            "2022-05-05T21:22:41.856020: step 7765, loss 1.37939, acc 0.546875\n",
            "2022-05-05T21:22:42.460347: step 7766, loss 1.89538, acc 0.375\n",
            "2022-05-05T21:22:43.082126: step 7767, loss 1.37189, acc 0.546875\n",
            "2022-05-05T21:22:43.691316: step 7768, loss 1.51093, acc 0.46875\n",
            "2022-05-05T21:22:44.318656: step 7769, loss 1.35076, acc 0.5625\n",
            "2022-05-05T21:22:44.963892: step 7770, loss 1.3888, acc 0.5625\n",
            "2022-05-05T21:22:45.621127: step 7771, loss 1.5509, acc 0.546875\n",
            "2022-05-05T21:22:46.240738: step 7772, loss 1.63797, acc 0.46875\n",
            "2022-05-05T21:22:46.906626: step 7773, loss 1.20417, acc 0.625\n",
            "2022-05-05T21:22:47.549879: step 7774, loss 1.42475, acc 0.5\n",
            "2022-05-05T21:22:48.212322: step 7775, loss 1.71386, acc 0.390625\n",
            "2022-05-05T21:22:48.864856: step 7776, loss 1.68354, acc 0.375\n",
            "2022-05-05T21:22:49.485694: step 7777, loss 1.41191, acc 0.515625\n",
            "2022-05-05T21:22:50.103578: step 7778, loss 1.36396, acc 0.515625\n",
            "2022-05-05T21:22:50.724210: step 7779, loss 1.28026, acc 0.625\n",
            "2022-05-05T21:22:51.384443: step 7780, loss 1.51333, acc 0.40625\n",
            "2022-05-05T21:22:51.997411: step 7781, loss 1.21826, acc 0.640625\n",
            "2022-05-05T21:22:52.614397: step 7782, loss 1.49933, acc 0.546875\n",
            "2022-05-05T21:22:53.242935: step 7783, loss 1.38245, acc 0.453125\n",
            "2022-05-05T21:22:53.853260: step 7784, loss 1.22418, acc 0.59375\n",
            "2022-05-05T21:22:54.475666: step 7785, loss 1.38292, acc 0.5\n",
            "2022-05-05T21:22:55.082178: step 7786, loss 1.46454, acc 0.515625\n",
            "2022-05-05T21:22:55.700736: step 7787, loss 1.34819, acc 0.59375\n",
            "2022-05-05T21:22:56.318625: step 7788, loss 1.57262, acc 0.421875\n",
            "2022-05-05T21:22:56.942756: step 7789, loss 1.3597, acc 0.5625\n",
            "2022-05-05T21:22:57.572647: step 7790, loss 1.56556, acc 0.53125\n",
            "2022-05-05T21:22:58.207912: step 7791, loss 1.25316, acc 0.5625\n",
            "2022-05-05T21:22:58.810263: step 7792, loss 1.44976, acc 0.546875\n",
            "2022-05-05T21:22:59.422676: step 7793, loss 1.46822, acc 0.5\n",
            "2022-05-05T21:23:00.036463: step 7794, loss 1.62482, acc 0.453125\n",
            "2022-05-05T21:23:00.653938: step 7795, loss 1.35094, acc 0.5\n",
            "2022-05-05T21:23:01.252696: step 7796, loss 1.31511, acc 0.546875\n",
            "2022-05-05T21:23:01.890638: step 7797, loss 1.4249, acc 0.46875\n",
            "2022-05-05T21:23:02.504284: step 7798, loss 1.58879, acc 0.40625\n",
            "2022-05-05T21:23:03.104534: step 7799, loss 1.35339, acc 0.453125\n",
            "2022-05-05T21:23:03.726152: step 7800, loss 1.41647, acc 0.515625\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:23:03.870201: step 7800, loss 1.87812, acc 0.39\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-7800\n",
            "\n",
            "2022-05-05T21:23:04.701085: step 7801, loss 1.59908, acc 0.40625\n",
            "2022-05-05T21:23:05.303610: step 7802, loss 1.46005, acc 0.484375\n",
            "2022-05-05T21:23:05.922122: step 7803, loss 1.60184, acc 0.421875\n",
            "2022-05-05T21:23:06.536018: step 7804, loss 1.41262, acc 0.484375\n",
            "2022-05-05T21:23:07.143350: step 7805, loss 1.34642, acc 0.578125\n",
            "2022-05-05T21:23:07.818340: step 7806, loss 1.34906, acc 0.53125\n",
            "2022-05-05T21:23:08.424330: step 7807, loss 1.67638, acc 0.375\n",
            "2022-05-05T21:23:09.042740: step 7808, loss 1.27133, acc 0.546875\n",
            "2022-05-05T21:23:09.680690: step 7809, loss 1.30005, acc 0.5625\n",
            "2022-05-05T21:23:10.299770: step 7810, loss 1.35768, acc 0.515625\n",
            "2022-05-05T21:23:10.929382: step 7811, loss 1.56627, acc 0.484375\n",
            "2022-05-05T21:23:11.542034: step 7812, loss 1.40867, acc 0.5625\n",
            "2022-05-05T21:23:12.199932: step 7813, loss 1.51173, acc 0.46875\n",
            "2022-05-05T21:23:12.831588: step 7814, loss 1.16266, acc 0.5625\n",
            "2022-05-05T21:23:13.463350: step 7815, loss 1.11476, acc 0.640625\n",
            "2022-05-05T21:23:14.084336: step 7816, loss 1.47989, acc 0.46875\n",
            "2022-05-05T21:23:14.709380: step 7817, loss 1.47421, acc 0.515625\n",
            "2022-05-05T21:23:15.320341: step 7818, loss 1.41402, acc 0.5\n",
            "2022-05-05T21:23:15.949332: step 7819, loss 1.58153, acc 0.4375\n",
            "2022-05-05T21:23:16.563006: step 7820, loss 1.33668, acc 0.515625\n",
            "2022-05-05T21:23:17.190397: step 7821, loss 1.18934, acc 0.703125\n",
            "2022-05-05T21:23:17.799771: step 7822, loss 1.3584, acc 0.546875\n",
            "2022-05-05T21:23:18.424602: step 7823, loss 1.33435, acc 0.53125\n",
            "2022-05-05T21:23:19.040821: step 7824, loss 1.36003, acc 0.5\n",
            "2022-05-05T21:23:19.669402: step 7825, loss 1.46985, acc 0.421875\n",
            "2022-05-05T21:23:20.301166: step 7826, loss 1.50883, acc 0.46875\n",
            "2022-05-05T21:23:20.912475: step 7827, loss 1.40603, acc 0.5\n",
            "2022-05-05T21:23:21.519719: step 7828, loss 1.33936, acc 0.546875\n",
            "2022-05-05T21:23:22.155098: step 7829, loss 1.58592, acc 0.390625\n",
            "2022-05-05T21:23:22.789398: step 7830, loss 1.48045, acc 0.484375\n",
            "2022-05-05T21:23:23.422898: step 7831, loss 1.59385, acc 0.546875\n",
            "2022-05-05T21:23:24.053815: step 7832, loss 1.30052, acc 0.484375\n",
            "2022-05-05T21:23:24.675296: step 7833, loss 1.5164, acc 0.515625\n",
            "2022-05-05T21:23:25.323341: step 7834, loss 1.42027, acc 0.5\n",
            "2022-05-05T21:23:25.963665: step 7835, loss 1.56709, acc 0.34375\n",
            "2022-05-05T21:23:26.618086: step 7836, loss 1.28349, acc 0.5\n",
            "2022-05-05T21:23:27.243622: step 7837, loss 1.23623, acc 0.53125\n",
            "2022-05-05T21:23:27.875034: step 7838, loss 1.31208, acc 0.546875\n",
            "2022-05-05T21:23:28.522531: step 7839, loss 1.34108, acc 0.53125\n",
            "2022-05-05T21:23:29.138719: step 7840, loss 1.28116, acc 0.578125\n",
            "2022-05-05T21:23:29.778377: step 7841, loss 1.3198, acc 0.46875\n",
            "2022-05-05T21:23:30.402227: step 7842, loss 1.61003, acc 0.359375\n",
            "2022-05-05T21:23:31.022200: step 7843, loss 1.43183, acc 0.46875\n",
            "2022-05-05T21:23:31.649482: step 7844, loss 1.44103, acc 0.578125\n",
            "2022-05-05T21:23:32.266112: step 7845, loss 1.23291, acc 0.515625\n",
            "2022-05-05T21:23:32.925324: step 7846, loss 1.32353, acc 0.4375\n",
            "2022-05-05T21:23:33.552903: step 7847, loss 1.3119, acc 0.546875\n",
            "2022-05-05T21:23:34.172499: step 7848, loss 1.43117, acc 0.5\n",
            "2022-05-05T21:23:34.809775: step 7849, loss 1.56526, acc 0.40625\n",
            "2022-05-05T21:23:35.429754: step 7850, loss 1.38091, acc 0.5\n",
            "2022-05-05T21:23:36.041791: step 7851, loss 1.33159, acc 0.5625\n",
            "2022-05-05T21:23:36.676585: step 7852, loss 1.44061, acc 0.5\n",
            "2022-05-05T21:23:37.302095: step 7853, loss 1.38009, acc 0.453125\n",
            "2022-05-05T21:23:37.944625: step 7854, loss 1.47936, acc 0.4375\n",
            "2022-05-05T21:23:38.565781: step 7855, loss 1.2203, acc 0.640625\n",
            "2022-05-05T21:23:39.176705: step 7856, loss 1.26705, acc 0.46875\n",
            "2022-05-05T21:23:39.798426: step 7857, loss 1.06973, acc 0.640625\n",
            "2022-05-05T21:23:40.413624: step 7858, loss 1.48333, acc 0.5625\n",
            "2022-05-05T21:23:41.044749: step 7859, loss 1.40702, acc 0.5625\n",
            "2022-05-05T21:23:41.666039: step 7860, loss 1.43559, acc 0.46875\n",
            "2022-05-05T21:23:42.270957: step 7861, loss 1.5136, acc 0.515625\n",
            "2022-05-05T21:23:42.905297: step 7862, loss 1.39164, acc 0.578125\n",
            "2022-05-05T21:23:43.524549: step 7863, loss 1.50941, acc 0.40625\n",
            "2022-05-05T21:23:44.146102: step 7864, loss 1.21469, acc 0.546875\n",
            "2022-05-05T21:23:44.772032: step 7865, loss 1.50689, acc 0.453125\n",
            "2022-05-05T21:23:45.385622: step 7866, loss 1.27143, acc 0.5625\n",
            "2022-05-05T21:23:46.002121: step 7867, loss 1.27269, acc 0.640625\n",
            "2022-05-05T21:23:46.623306: step 7868, loss 1.52906, acc 0.40625\n",
            "2022-05-05T21:23:47.247834: step 7869, loss 1.37546, acc 0.5\n",
            "2022-05-05T21:23:47.871745: step 7870, loss 1.57167, acc 0.515625\n",
            "2022-05-05T21:23:48.478746: step 7871, loss 1.39106, acc 0.53125\n",
            "2022-05-05T21:23:49.111352: step 7872, loss 1.31977, acc 0.625\n",
            "2022-05-05T21:23:49.727341: step 7873, loss 1.53522, acc 0.5\n",
            "2022-05-05T21:23:50.366612: step 7874, loss 1.50977, acc 0.421875\n",
            "2022-05-05T21:23:50.996820: step 7875, loss 1.44818, acc 0.5\n",
            "2022-05-05T21:23:51.619033: step 7876, loss 1.201, acc 0.625\n",
            "2022-05-05T21:23:52.236697: step 7877, loss 1.24886, acc 0.5625\n",
            "2022-05-05T21:23:52.854927: step 7878, loss 1.31482, acc 0.46875\n",
            "2022-05-05T21:23:53.507871: step 7879, loss 1.52711, acc 0.375\n",
            "2022-05-05T21:23:54.144718: step 7880, loss 1.52489, acc 0.4375\n",
            "2022-05-05T21:23:54.771562: step 7881, loss 1.55479, acc 0.375\n",
            "2022-05-05T21:23:55.401983: step 7882, loss 1.18401, acc 0.625\n",
            "2022-05-05T21:23:56.025227: step 7883, loss 1.40884, acc 0.515625\n",
            "2022-05-05T21:23:56.667299: step 7884, loss 1.54261, acc 0.390625\n",
            "2022-05-05T21:23:57.296752: step 7885, loss 1.46468, acc 0.5625\n",
            "2022-05-05T21:23:57.904530: step 7886, loss 1.39572, acc 0.609375\n",
            "2022-05-05T21:23:58.537249: step 7887, loss 1.28576, acc 0.515625\n",
            "2022-05-05T21:23:59.164837: step 7888, loss 1.30314, acc 0.5625\n",
            "2022-05-05T21:23:59.786832: step 7889, loss 1.34855, acc 0.578125\n",
            "2022-05-05T21:24:00.413938: step 7890, loss 1.65537, acc 0.421875\n",
            "2022-05-05T21:24:01.030945: step 7891, loss 1.26988, acc 0.609375\n",
            "2022-05-05T21:24:01.660136: step 7892, loss 1.563, acc 0.484375\n",
            "2022-05-05T21:24:02.274198: step 7893, loss 1.22958, acc 0.65625\n",
            "2022-05-05T21:24:02.888310: step 7894, loss 1.31945, acc 0.546875\n",
            "2022-05-05T21:24:03.519612: step 7895, loss 1.39795, acc 0.453125\n",
            "2022-05-05T21:24:04.126087: step 7896, loss 1.27272, acc 0.578125\n",
            "2022-05-05T21:24:04.742903: step 7897, loss 1.40864, acc 0.5\n",
            "2022-05-05T21:24:05.350299: step 7898, loss 1.43693, acc 0.515625\n",
            "2022-05-05T21:24:05.958616: step 7899, loss 1.33666, acc 0.515625\n",
            "2022-05-05T21:24:06.579376: step 7900, loss 1.17784, acc 0.53125\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:24:06.722938: step 7900, loss 2.24482, acc 0.25\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-7900\n",
            "\n",
            "2022-05-05T21:24:07.533494: step 7901, loss 1.43058, acc 0.4375\n",
            "2022-05-05T21:24:08.148460: step 7902, loss 1.29269, acc 0.578125\n",
            "2022-05-05T21:24:08.765205: step 7903, loss 1.36559, acc 0.484375\n",
            "2022-05-05T21:24:09.370693: step 7904, loss 1.46984, acc 0.484375\n",
            "2022-05-05T21:24:09.995126: step 7905, loss 1.38968, acc 0.625\n",
            "2022-05-05T21:24:10.623848: step 7906, loss 1.27515, acc 0.546875\n",
            "2022-05-05T21:24:11.229782: step 7907, loss 1.35096, acc 0.59375\n",
            "2022-05-05T21:24:11.841283: step 7908, loss 1.52931, acc 0.46875\n",
            "2022-05-05T21:24:12.441942: step 7909, loss 1.45803, acc 0.40625\n",
            "2022-05-05T21:24:13.070095: step 7910, loss 1.47045, acc 0.5\n",
            "2022-05-05T21:24:13.695251: step 7911, loss 1.43972, acc 0.546875\n",
            "2022-05-05T21:24:14.324721: step 7912, loss 1.36108, acc 0.5\n",
            "2022-05-05T21:24:14.941971: step 7913, loss 1.3768, acc 0.546875\n",
            "2022-05-05T21:24:15.614224: step 7914, loss 1.50363, acc 0.4375\n",
            "2022-05-05T21:24:16.237760: step 7915, loss 1.1838, acc 0.5625\n",
            "2022-05-05T21:24:16.871049: step 7916, loss 1.4256, acc 0.53125\n",
            "2022-05-05T21:24:17.493681: step 7917, loss 1.41048, acc 0.5625\n",
            "2022-05-05T21:24:18.140433: step 7918, loss 1.53921, acc 0.5\n",
            "2022-05-05T21:24:18.774599: step 7919, loss 1.52136, acc 0.390625\n",
            "2022-05-05T21:24:19.386255: step 7920, loss 1.5731, acc 0.421875\n",
            "2022-05-05T21:24:20.017321: step 7921, loss 1.39542, acc 0.546875\n",
            "2022-05-05T21:24:20.653290: step 7922, loss 1.38363, acc 0.546875\n",
            "2022-05-05T21:24:21.285011: step 7923, loss 1.23097, acc 0.59375\n",
            "2022-05-05T21:24:21.927901: step 7924, loss 1.32524, acc 0.625\n",
            "2022-05-05T21:24:22.555632: step 7925, loss 1.43004, acc 0.578125\n",
            "2022-05-05T21:24:23.185376: step 7926, loss 1.60718, acc 0.375\n",
            "2022-05-05T21:24:23.817974: step 7927, loss 1.33059, acc 0.546875\n",
            "2022-05-05T21:24:24.509613: step 7928, loss 1.32355, acc 0.5\n",
            "2022-05-05T21:24:25.153197: step 7929, loss 1.6313, acc 0.390625\n",
            "2022-05-05T21:24:25.793729: step 7930, loss 1.37023, acc 0.515625\n",
            "2022-05-05T21:24:26.439121: step 7931, loss 1.88814, acc 0.359375\n",
            "2022-05-05T21:24:27.072848: step 7932, loss 1.49495, acc 0.421875\n",
            "2022-05-05T21:24:27.684761: step 7933, loss 1.30773, acc 0.609375\n",
            "2022-05-05T21:24:28.335867: step 7934, loss 1.52306, acc 0.4375\n",
            "2022-05-05T21:24:28.957591: step 7935, loss 1.33046, acc 0.5\n",
            "2022-05-05T21:24:29.586645: step 7936, loss 1.50967, acc 0.5\n",
            "2022-05-05T21:24:30.212113: step 7937, loss 1.46275, acc 0.53125\n",
            "2022-05-05T21:24:30.837295: step 7938, loss 1.2278, acc 0.5625\n",
            "2022-05-05T21:24:31.476501: step 7939, loss 1.28468, acc 0.46875\n",
            "2022-05-05T21:24:32.129412: step 7940, loss 1.37655, acc 0.59375\n",
            "2022-05-05T21:24:32.757568: step 7941, loss 1.3251, acc 0.546875\n",
            "2022-05-05T21:24:33.382720: step 7942, loss 1.35906, acc 0.46875\n",
            "2022-05-05T21:24:34.017540: step 7943, loss 1.29113, acc 0.59375\n",
            "2022-05-05T21:24:34.681812: step 7944, loss 1.51217, acc 0.484375\n",
            "2022-05-05T21:24:35.312300: step 7945, loss 1.61075, acc 0.375\n",
            "2022-05-05T21:24:35.932410: step 7946, loss 1.2958, acc 0.59375\n",
            "2022-05-05T21:24:36.563009: step 7947, loss 1.50895, acc 0.40625\n",
            "2022-05-05T21:24:37.196423: step 7948, loss 1.63729, acc 0.4375\n",
            "2022-05-05T21:24:37.829960: step 7949, loss 1.32535, acc 0.5625\n",
            "2022-05-05T21:24:38.482257: step 7950, loss 1.52449, acc 0.421875\n",
            "2022-05-05T21:24:39.105319: step 7951, loss 1.31955, acc 0.515625\n",
            "2022-05-05T21:24:39.738837: step 7952, loss 1.34267, acc 0.515625\n",
            "2022-05-05T21:24:40.339597: step 7953, loss 1.29243, acc 0.53125\n",
            "2022-05-05T21:24:40.977485: step 7954, loss 1.47944, acc 0.453125\n",
            "2022-05-05T21:24:41.606895: step 7955, loss 1.42557, acc 0.515625\n",
            "2022-05-05T21:24:42.205028: step 7956, loss 1.49638, acc 0.484375\n",
            "2022-05-05T21:24:42.825038: step 7957, loss 1.37896, acc 0.484375\n",
            "2022-05-05T21:24:43.427851: step 7958, loss 1.46668, acc 0.53125\n",
            "2022-05-05T21:24:44.065082: step 7959, loss 1.34035, acc 0.484375\n",
            "2022-05-05T21:24:44.685340: step 7960, loss 1.44408, acc 0.484375\n",
            "2022-05-05T21:24:45.317084: step 7961, loss 1.47162, acc 0.5\n",
            "2022-05-05T21:24:45.929955: step 7962, loss 1.52702, acc 0.453125\n",
            "2022-05-05T21:24:46.545483: step 7963, loss 1.55662, acc 0.421875\n",
            "2022-05-05T21:24:47.165454: step 7964, loss 1.4842, acc 0.46875\n",
            "2022-05-05T21:24:47.794130: step 7965, loss 1.59388, acc 0.40625\n",
            "2022-05-05T21:24:48.411143: step 7966, loss 1.3615, acc 0.515625\n",
            "2022-05-05T21:24:49.027001: step 7967, loss 1.24474, acc 0.640625\n",
            "2022-05-05T21:24:49.647653: step 7968, loss 1.21977, acc 0.625\n",
            "2022-05-05T21:24:50.270979: step 7969, loss 1.12068, acc 0.578125\n",
            "2022-05-05T21:24:50.892549: step 7970, loss 1.38272, acc 0.515625\n",
            "2022-05-05T21:24:51.501177: step 7971, loss 1.41093, acc 0.515625\n",
            "2022-05-05T21:24:52.120297: step 7972, loss 1.52492, acc 0.40625\n",
            "2022-05-05T21:24:52.737172: step 7973, loss 1.39964, acc 0.453125\n",
            "2022-05-05T21:24:53.355796: step 7974, loss 1.38609, acc 0.5625\n",
            "2022-05-05T21:24:53.974450: step 7975, loss 1.61326, acc 0.453125\n",
            "2022-05-05T21:24:54.576212: step 7976, loss 1.44616, acc 0.453125\n",
            "2022-05-05T21:24:55.196166: step 7977, loss 1.52338, acc 0.421875\n",
            "2022-05-05T21:24:55.842131: step 7978, loss 1.32126, acc 0.53125\n",
            "2022-05-05T21:24:56.465210: step 7979, loss 1.43624, acc 0.46875\n",
            "2022-05-05T21:24:57.091806: step 7980, loss 1.38905, acc 0.53125\n",
            "2022-05-05T21:24:57.712802: step 7981, loss 1.24067, acc 0.578125\n",
            "2022-05-05T21:24:58.329570: step 7982, loss 1.36285, acc 0.515625\n",
            "2022-05-05T21:24:58.937265: step 7983, loss 1.41129, acc 0.453125\n",
            "2022-05-05T21:24:59.547699: step 7984, loss 1.56372, acc 0.515625\n",
            "2022-05-05T21:25:00.163206: step 7985, loss 1.24061, acc 0.546875\n",
            "2022-05-05T21:25:00.785506: step 7986, loss 1.44412, acc 0.484375\n",
            "2022-05-05T21:25:01.396573: step 7987, loss 1.68602, acc 0.4375\n",
            "2022-05-05T21:25:02.004152: step 7988, loss 1.44182, acc 0.5\n",
            "2022-05-05T21:25:02.633230: step 7989, loss 1.21755, acc 0.640625\n",
            "2022-05-05T21:25:03.247797: step 7990, loss 1.47463, acc 0.453125\n",
            "2022-05-05T21:25:03.858334: step 7991, loss 1.69711, acc 0.390625\n",
            "2022-05-05T21:25:04.480783: step 7992, loss 1.51463, acc 0.484375\n",
            "2022-05-05T21:25:05.097093: step 7993, loss 1.5382, acc 0.421875\n",
            "2022-05-05T21:25:05.727825: step 7994, loss 1.3995, acc 0.484375\n",
            "2022-05-05T21:25:06.366846: step 7995, loss 1.45188, acc 0.546875\n",
            "2022-05-05T21:25:06.974905: step 7996, loss 1.37723, acc 0.5625\n",
            "2022-05-05T21:25:07.609678: step 7997, loss 1.45302, acc 0.5\n",
            "2022-05-05T21:25:08.220240: step 7998, loss 1.4137, acc 0.53125\n",
            "2022-05-05T21:25:08.842088: step 7999, loss 1.40686, acc 0.546875\n",
            "2022-05-05T21:25:09.461644: step 8000, loss 1.45874, acc 0.5625\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:25:09.603348: step 8000, loss 1.91594, acc 0.32\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-8000\n",
            "\n",
            "2022-05-05T21:25:10.397587: step 8001, loss 1.54144, acc 0.453125\n",
            "2022-05-05T21:25:11.004826: step 8002, loss 1.36526, acc 0.53125\n",
            "2022-05-05T21:25:11.612907: step 8003, loss 1.44039, acc 0.53125\n",
            "2022-05-05T21:25:12.221579: step 8004, loss 1.50637, acc 0.546875\n",
            "2022-05-05T21:25:12.843388: step 8005, loss 1.21874, acc 0.59375\n",
            "2022-05-05T21:25:13.464735: step 8006, loss 1.63168, acc 0.484375\n",
            "2022-05-05T21:25:14.105401: step 8007, loss 1.52612, acc 0.4375\n",
            "2022-05-05T21:25:14.796760: step 8008, loss 1.45781, acc 0.421875\n",
            "2022-05-05T21:25:15.500245: step 8009, loss 1.30948, acc 0.5625\n",
            "2022-05-05T21:25:16.248114: step 8010, loss 1.50249, acc 0.453125\n",
            "2022-05-05T21:25:16.898927: step 8011, loss 1.40163, acc 0.53125\n",
            "2022-05-05T21:25:17.556981: step 8012, loss 1.26991, acc 0.578125\n",
            "2022-05-05T21:25:18.227837: step 8013, loss 1.38067, acc 0.5625\n",
            "2022-05-05T21:25:18.923398: step 8014, loss 1.54544, acc 0.359375\n",
            "2022-05-05T21:25:19.625746: step 8015, loss 1.57525, acc 0.4375\n",
            "2022-05-05T21:25:20.334143: step 8016, loss 1.32481, acc 0.546875\n",
            "2022-05-05T21:25:21.039153: step 8017, loss 1.44329, acc 0.453125\n",
            "2022-05-05T21:25:21.776850: step 8018, loss 1.47239, acc 0.484375\n",
            "2022-05-05T21:25:22.479207: step 8019, loss 1.50333, acc 0.484375\n",
            "2022-05-05T21:25:23.160785: step 8020, loss 1.49452, acc 0.453125\n",
            "2022-05-05T21:25:23.852650: step 8021, loss 1.22515, acc 0.546875\n",
            "2022-05-05T21:25:24.527376: step 8022, loss 1.53374, acc 0.546875\n",
            "2022-05-05T21:25:25.191174: step 8023, loss 1.33558, acc 0.5\n",
            "2022-05-05T21:25:25.816041: step 8024, loss 1.41, acc 0.453125\n",
            "2022-05-05T21:25:26.464558: step 8025, loss 1.30251, acc 0.53125\n",
            "2022-05-05T21:25:27.122237: step 8026, loss 1.54351, acc 0.4375\n",
            "2022-05-05T21:25:27.784725: step 8027, loss 1.42548, acc 0.484375\n",
            "2022-05-05T21:25:28.541576: step 8028, loss 1.30386, acc 0.515625\n",
            "2022-05-05T21:25:29.261664: step 8029, loss 1.37206, acc 0.53125\n",
            "2022-05-05T21:25:29.944853: step 8030, loss 1.46546, acc 0.5\n",
            "2022-05-05T21:25:30.639927: step 8031, loss 1.62995, acc 0.359375\n",
            "2022-05-05T21:25:31.360331: step 8032, loss 1.45302, acc 0.4375\n",
            "2022-05-05T21:25:32.077888: step 8033, loss 1.46933, acc 0.515625\n",
            "2022-05-05T21:25:32.816298: step 8034, loss 1.39842, acc 0.515625\n",
            "2022-05-05T21:25:33.531553: step 8035, loss 1.58759, acc 0.390625\n",
            "2022-05-05T21:25:34.235631: step 8036, loss 1.51081, acc 0.5\n",
            "2022-05-05T21:25:34.962370: step 8037, loss 1.36093, acc 0.46875\n",
            "2022-05-05T21:25:35.696327: step 8038, loss 1.43169, acc 0.453125\n",
            "2022-05-05T21:25:36.414528: step 8039, loss 1.48208, acc 0.484375\n",
            "2022-05-05T21:25:37.152705: step 8040, loss 1.29074, acc 0.515625\n",
            "2022-05-05T21:25:37.877087: step 8041, loss 1.56941, acc 0.5\n",
            "2022-05-05T21:25:38.590167: step 8042, loss 1.33194, acc 0.625\n",
            "2022-05-05T21:25:39.255368: step 8043, loss 1.31317, acc 0.59375\n",
            "2022-05-05T21:25:39.897802: step 8044, loss 1.4305, acc 0.53125\n",
            "2022-05-05T21:25:40.539627: step 8045, loss 1.55649, acc 0.46875\n",
            "2022-05-05T21:25:41.238020: step 8046, loss 1.57646, acc 0.421875\n",
            "2022-05-05T21:25:41.953319: step 8047, loss 1.39453, acc 0.421875\n",
            "2022-05-05T21:25:42.660881: step 8048, loss 1.43001, acc 0.46875\n",
            "2022-05-05T21:25:43.346421: step 8049, loss 1.16022, acc 0.609375\n",
            "2022-05-05T21:25:44.051955: step 8050, loss 1.21409, acc 0.65625\n",
            "2022-05-05T21:25:44.749979: step 8051, loss 1.43168, acc 0.5\n",
            "2022-05-05T21:25:45.429179: step 8052, loss 1.69245, acc 0.34375\n",
            "2022-05-05T21:25:46.141244: step 8053, loss 1.52026, acc 0.5625\n",
            "2022-05-05T21:25:46.841137: step 8054, loss 1.45197, acc 0.59375\n",
            "2022-05-05T21:25:47.566028: step 8055, loss 1.21411, acc 0.609375\n",
            "2022-05-05T21:25:48.304715: step 8056, loss 1.43657, acc 0.5\n",
            "2022-05-05T21:25:49.012623: step 8057, loss 1.57252, acc 0.46875\n",
            "2022-05-05T21:25:49.747654: step 8058, loss 1.57407, acc 0.5\n",
            "2022-05-05T21:25:50.476191: step 8059, loss 1.35064, acc 0.515625\n",
            "2022-05-05T21:25:51.187580: step 8060, loss 1.42574, acc 0.53125\n",
            "2022-05-05T21:25:51.905522: step 8061, loss 1.54583, acc 0.421875\n",
            "2022-05-05T21:25:52.610698: step 8062, loss 1.36723, acc 0.484375\n",
            "2022-05-05T21:25:53.303162: step 8063, loss 1.47086, acc 0.546875\n",
            "2022-05-05T21:25:53.999242: step 8064, loss 1.39346, acc 0.515625\n",
            "2022-05-05T21:25:54.692139: step 8065, loss 1.43229, acc 0.484375\n",
            "2022-05-05T21:25:55.388627: step 8066, loss 1.51378, acc 0.40625\n",
            "2022-05-05T21:25:56.088319: step 8067, loss 1.339, acc 0.515625\n",
            "2022-05-05T21:25:56.775858: step 8068, loss 1.32557, acc 0.515625\n",
            "2022-05-05T21:25:57.483514: step 8069, loss 1.48055, acc 0.515625\n",
            "2022-05-05T21:25:58.143137: step 8070, loss 1.48143, acc 0.484375\n",
            "2022-05-05T21:25:58.823750: step 8071, loss 1.52349, acc 0.453125\n",
            "2022-05-05T21:25:59.463587: step 8072, loss 1.49361, acc 0.4375\n",
            "2022-05-05T21:26:00.088662: step 8073, loss 1.55385, acc 0.390625\n",
            "2022-05-05T21:26:00.724176: step 8074, loss 1.39292, acc 0.453125\n",
            "2022-05-05T21:26:01.345306: step 8075, loss 1.41364, acc 0.5\n",
            "2022-05-05T21:26:01.944706: step 8076, loss 1.50897, acc 0.4375\n",
            "2022-05-05T21:26:02.567816: step 8077, loss 1.28795, acc 0.625\n",
            "2022-05-05T21:26:03.176519: step 8078, loss 1.34715, acc 0.53125\n",
            "2022-05-05T21:26:03.791545: step 8079, loss 1.49597, acc 0.390625\n",
            "2022-05-05T21:26:04.434406: step 8080, loss 1.26748, acc 0.609375\n",
            "2022-05-05T21:26:05.097572: step 8081, loss 1.65268, acc 0.34375\n",
            "2022-05-05T21:26:05.736523: step 8082, loss 1.36643, acc 0.484375\n",
            "2022-05-05T21:26:06.373933: step 8083, loss 1.40367, acc 0.484375\n",
            "2022-05-05T21:26:07.023246: step 8084, loss 1.52561, acc 0.4375\n",
            "2022-05-05T21:26:07.655983: step 8085, loss 1.58389, acc 0.453125\n",
            "2022-05-05T21:26:08.314806: step 8086, loss 1.46838, acc 0.484375\n",
            "2022-05-05T21:26:09.009890: step 8087, loss 1.40902, acc 0.453125\n",
            "2022-05-05T21:26:09.693629: step 8088, loss 1.53334, acc 0.4375\n",
            "2022-05-05T21:26:10.316773: step 8089, loss 1.44916, acc 0.453125\n",
            "2022-05-05T21:26:10.941951: step 8090, loss 1.44839, acc 0.5625\n",
            "2022-05-05T21:26:11.595749: step 8091, loss 1.61199, acc 0.421875\n",
            "2022-05-05T21:26:12.149448: step 8092, loss 1.41222, acc 0.55102\n",
            "2022-05-05T21:26:12.805826: step 8093, loss 1.17234, acc 0.640625\n",
            "2022-05-05T21:26:13.466157: step 8094, loss 1.12932, acc 0.703125\n",
            "2022-05-05T21:26:14.119647: step 8095, loss 1.43319, acc 0.5\n",
            "2022-05-05T21:26:14.793629: step 8096, loss 1.58662, acc 0.5\n",
            "2022-05-05T21:26:15.449669: step 8097, loss 1.34995, acc 0.5625\n",
            "2022-05-05T21:26:16.088812: step 8098, loss 1.42244, acc 0.4375\n",
            "2022-05-05T21:26:16.726919: step 8099, loss 1.23716, acc 0.625\n",
            "2022-05-05T21:26:17.343567: step 8100, loss 1.22732, acc 0.578125\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:26:17.499221: step 8100, loss 1.80004, acc 0.37\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-8100\n",
            "\n",
            "2022-05-05T21:26:18.434724: step 8101, loss 1.40565, acc 0.53125\n",
            "2022-05-05T21:26:19.107852: step 8102, loss 1.26828, acc 0.578125\n",
            "2022-05-05T21:26:19.747705: step 8103, loss 1.33369, acc 0.515625\n",
            "2022-05-05T21:26:20.384464: step 8104, loss 1.20802, acc 0.546875\n",
            "2022-05-05T21:26:21.048347: step 8105, loss 1.31088, acc 0.515625\n",
            "2022-05-05T21:26:21.673623: step 8106, loss 1.38737, acc 0.5\n",
            "2022-05-05T21:26:22.362089: step 8107, loss 1.30282, acc 0.5\n",
            "2022-05-05T21:26:23.007347: step 8108, loss 1.29952, acc 0.546875\n",
            "2022-05-05T21:26:23.628649: step 8109, loss 1.32501, acc 0.53125\n",
            "2022-05-05T21:26:24.283325: step 8110, loss 1.38054, acc 0.515625\n",
            "2022-05-05T21:26:24.911159: step 8111, loss 1.22415, acc 0.609375\n",
            "2022-05-05T21:26:25.556926: step 8112, loss 1.44753, acc 0.5\n",
            "2022-05-05T21:26:26.211164: step 8113, loss 1.39457, acc 0.53125\n",
            "2022-05-05T21:26:26.843285: step 8114, loss 1.19583, acc 0.609375\n",
            "2022-05-05T21:26:27.479847: step 8115, loss 1.25569, acc 0.578125\n",
            "2022-05-05T21:26:28.113876: step 8116, loss 1.45995, acc 0.5\n",
            "2022-05-05T21:26:28.793471: step 8117, loss 1.26245, acc 0.5\n",
            "2022-05-05T21:26:29.467518: step 8118, loss 1.3158, acc 0.5\n",
            "2022-05-05T21:26:30.111553: step 8119, loss 1.18911, acc 0.5625\n",
            "2022-05-05T21:26:30.736069: step 8120, loss 1.36219, acc 0.578125\n",
            "2022-05-05T21:26:31.377210: step 8121, loss 1.31193, acc 0.5625\n",
            "2022-05-05T21:26:32.018972: step 8122, loss 1.33205, acc 0.5625\n",
            "2022-05-05T21:26:32.674669: step 8123, loss 1.3221, acc 0.484375\n",
            "2022-05-05T21:26:33.318105: step 8124, loss 1.2804, acc 0.515625\n",
            "2022-05-05T21:26:33.960944: step 8125, loss 1.34724, acc 0.609375\n",
            "2022-05-05T21:26:34.603108: step 8126, loss 1.37598, acc 0.515625\n",
            "2022-05-05T21:26:35.226791: step 8127, loss 1.21627, acc 0.5625\n",
            "2022-05-05T21:26:35.841050: step 8128, loss 1.38836, acc 0.453125\n",
            "2022-05-05T21:26:36.464448: step 8129, loss 1.41895, acc 0.546875\n",
            "2022-05-05T21:26:37.068421: step 8130, loss 1.24958, acc 0.59375\n",
            "2022-05-05T21:26:37.710177: step 8131, loss 1.22729, acc 0.578125\n",
            "2022-05-05T21:26:38.326192: step 8132, loss 1.2674, acc 0.5625\n",
            "2022-05-05T21:26:38.982967: step 8133, loss 1.37878, acc 0.5\n",
            "2022-05-05T21:26:39.611154: step 8134, loss 1.21862, acc 0.609375\n",
            "2022-05-05T21:26:40.209227: step 8135, loss 1.14549, acc 0.6875\n",
            "2022-05-05T21:26:40.825916: step 8136, loss 1.2624, acc 0.5625\n",
            "2022-05-05T21:26:41.435099: step 8137, loss 1.32189, acc 0.546875\n",
            "2022-05-05T21:26:42.064207: step 8138, loss 1.19869, acc 0.609375\n",
            "2022-05-05T21:26:42.687353: step 8139, loss 1.33133, acc 0.484375\n",
            "2022-05-05T21:26:43.289965: step 8140, loss 1.29663, acc 0.5625\n",
            "2022-05-05T21:26:43.913590: step 8141, loss 1.26551, acc 0.53125\n",
            "2022-05-05T21:26:44.528417: step 8142, loss 1.51001, acc 0.421875\n",
            "2022-05-05T21:26:45.173217: step 8143, loss 1.32724, acc 0.5\n",
            "2022-05-05T21:26:45.805881: step 8144, loss 1.51146, acc 0.5\n",
            "2022-05-05T21:26:46.419371: step 8145, loss 1.46468, acc 0.46875\n",
            "2022-05-05T21:26:47.052330: step 8146, loss 1.46691, acc 0.5625\n",
            "2022-05-05T21:26:47.685775: step 8147, loss 1.30778, acc 0.578125\n",
            "2022-05-05T21:26:48.299942: step 8148, loss 1.37076, acc 0.59375\n",
            "2022-05-05T21:26:48.929892: step 8149, loss 1.21535, acc 0.59375\n",
            "2022-05-05T21:26:49.586121: step 8150, loss 1.08931, acc 0.65625\n",
            "2022-05-05T21:26:50.203111: step 8151, loss 1.27472, acc 0.546875\n",
            "2022-05-05T21:26:50.825590: step 8152, loss 1.11224, acc 0.671875\n",
            "2022-05-05T21:26:51.433451: step 8153, loss 1.1148, acc 0.65625\n",
            "2022-05-05T21:26:52.052567: step 8154, loss 1.32203, acc 0.484375\n",
            "2022-05-05T21:26:52.664147: step 8155, loss 1.47773, acc 0.484375\n",
            "2022-05-05T21:26:53.284259: step 8156, loss 1.04879, acc 0.6875\n",
            "2022-05-05T21:26:53.916244: step 8157, loss 1.31552, acc 0.53125\n",
            "2022-05-05T21:26:54.533015: step 8158, loss 1.41508, acc 0.4375\n",
            "2022-05-05T21:26:55.159231: step 8159, loss 1.05299, acc 0.703125\n",
            "2022-05-05T21:26:55.789877: step 8160, loss 1.4281, acc 0.484375\n",
            "2022-05-05T21:26:56.416189: step 8161, loss 1.34805, acc 0.53125\n",
            "2022-05-05T21:26:57.041823: step 8162, loss 1.51484, acc 0.4375\n",
            "2022-05-05T21:26:57.642877: step 8163, loss 1.34652, acc 0.4375\n",
            "2022-05-05T21:26:58.261636: step 8164, loss 1.2037, acc 0.625\n",
            "2022-05-05T21:26:58.896511: step 8165, loss 1.26445, acc 0.46875\n",
            "2022-05-05T21:26:59.544160: step 8166, loss 1.25677, acc 0.609375\n",
            "2022-05-05T21:27:00.158496: step 8167, loss 1.45065, acc 0.484375\n",
            "2022-05-05T21:27:00.772330: step 8168, loss 1.10062, acc 0.640625\n",
            "2022-05-05T21:27:01.387619: step 8169, loss 1.28996, acc 0.5625\n",
            "2022-05-05T21:27:01.996400: step 8170, loss 1.34791, acc 0.59375\n",
            "2022-05-05T21:27:02.621441: step 8171, loss 1.31586, acc 0.5625\n",
            "2022-05-05T21:27:03.242259: step 8172, loss 1.29043, acc 0.53125\n",
            "2022-05-05T21:27:03.859269: step 8173, loss 1.35373, acc 0.5625\n",
            "2022-05-05T21:27:04.500310: step 8174, loss 1.31232, acc 0.5625\n",
            "2022-05-05T21:27:05.134495: step 8175, loss 1.39759, acc 0.578125\n",
            "2022-05-05T21:27:05.782452: step 8176, loss 1.2109, acc 0.640625\n",
            "2022-05-05T21:27:06.424314: step 8177, loss 1.37201, acc 0.5\n",
            "2022-05-05T21:27:07.053875: step 8178, loss 1.24339, acc 0.609375\n",
            "2022-05-05T21:27:07.708794: step 8179, loss 1.61162, acc 0.453125\n",
            "2022-05-05T21:27:08.332794: step 8180, loss 1.16254, acc 0.578125\n",
            "2022-05-05T21:27:08.939872: step 8181, loss 1.53574, acc 0.453125\n",
            "2022-05-05T21:27:09.574111: step 8182, loss 1.34515, acc 0.515625\n",
            "2022-05-05T21:27:10.214834: step 8183, loss 1.18792, acc 0.546875\n",
            "2022-05-05T21:27:10.839086: step 8184, loss 1.08682, acc 0.671875\n",
            "2022-05-05T21:27:11.457143: step 8185, loss 1.24642, acc 0.5625\n",
            "2022-05-05T21:27:12.069312: step 8186, loss 1.19623, acc 0.65625\n",
            "2022-05-05T21:27:12.708480: step 8187, loss 1.40942, acc 0.484375\n",
            "2022-05-05T21:27:13.315591: step 8188, loss 1.31965, acc 0.609375\n",
            "2022-05-05T21:27:13.945502: step 8189, loss 1.31142, acc 0.515625\n",
            "2022-05-05T21:27:14.572138: step 8190, loss 1.30923, acc 0.484375\n",
            "2022-05-05T21:27:15.187749: step 8191, loss 1.26647, acc 0.53125\n",
            "2022-05-05T21:27:15.818607: step 8192, loss 1.3916, acc 0.5\n",
            "2022-05-05T21:27:16.427864: step 8193, loss 1.44532, acc 0.515625\n",
            "2022-05-05T21:27:17.059977: step 8194, loss 1.0191, acc 0.65625\n",
            "2022-05-05T21:27:17.691903: step 8195, loss 1.46139, acc 0.484375\n",
            "2022-05-05T21:27:18.297814: step 8196, loss 1.43836, acc 0.546875\n",
            "2022-05-05T21:27:18.927602: step 8197, loss 1.30191, acc 0.546875\n",
            "2022-05-05T21:27:19.547896: step 8198, loss 1.26804, acc 0.515625\n",
            "2022-05-05T21:27:20.189790: step 8199, loss 1.30966, acc 0.5625\n",
            "2022-05-05T21:27:20.805401: step 8200, loss 1.29654, acc 0.453125\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:27:20.956806: step 8200, loss 2.21041, acc 0.24\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-8200\n",
            "\n",
            "2022-05-05T21:27:21.767481: step 8201, loss 1.25594, acc 0.609375\n",
            "2022-05-05T21:27:22.372341: step 8202, loss 1.24505, acc 0.640625\n",
            "2022-05-05T21:27:22.992480: step 8203, loss 1.41722, acc 0.53125\n",
            "2022-05-05T21:27:23.599500: step 8204, loss 1.30349, acc 0.578125\n",
            "2022-05-05T21:27:24.222939: step 8205, loss 1.43855, acc 0.53125\n",
            "2022-05-05T21:27:24.845091: step 8206, loss 1.32313, acc 0.5\n",
            "2022-05-05T21:27:25.454898: step 8207, loss 1.11151, acc 0.65625\n",
            "2022-05-05T21:27:26.080883: step 8208, loss 1.2382, acc 0.578125\n",
            "2022-05-05T21:27:26.709951: step 8209, loss 1.3285, acc 0.515625\n",
            "2022-05-05T21:27:27.336895: step 8210, loss 1.23368, acc 0.609375\n",
            "2022-05-05T21:27:27.978740: step 8211, loss 1.26219, acc 0.59375\n",
            "2022-05-05T21:27:28.602444: step 8212, loss 1.26387, acc 0.5625\n",
            "2022-05-05T21:27:29.250472: step 8213, loss 1.31914, acc 0.53125\n",
            "2022-05-05T21:27:29.876853: step 8214, loss 1.3444, acc 0.53125\n",
            "2022-05-05T21:27:30.511440: step 8215, loss 1.26496, acc 0.640625\n",
            "2022-05-05T21:27:31.153224: step 8216, loss 1.43391, acc 0.5\n",
            "2022-05-05T21:27:31.772372: step 8217, loss 1.46321, acc 0.484375\n",
            "2022-05-05T21:27:32.396345: step 8218, loss 1.46856, acc 0.5\n",
            "2022-05-05T21:27:33.023117: step 8219, loss 1.31494, acc 0.484375\n",
            "2022-05-05T21:27:33.674243: step 8220, loss 1.38324, acc 0.5\n",
            "2022-05-05T21:27:34.314289: step 8221, loss 1.40805, acc 0.5\n",
            "2022-05-05T21:27:35.044674: step 8222, loss 1.23395, acc 0.46875\n",
            "2022-05-05T21:27:35.839820: step 8223, loss 1.41006, acc 0.5\n",
            "2022-05-05T21:27:36.532650: step 8224, loss 1.4206, acc 0.515625\n",
            "2022-05-05T21:27:37.234216: step 8225, loss 1.34321, acc 0.5625\n",
            "2022-05-05T21:27:37.919736: step 8226, loss 1.20735, acc 0.59375\n",
            "2022-05-05T21:27:38.633271: step 8227, loss 1.46437, acc 0.484375\n",
            "2022-05-05T21:27:39.340761: step 8228, loss 1.38008, acc 0.5\n",
            "2022-05-05T21:27:40.069214: step 8229, loss 1.08391, acc 0.625\n",
            "2022-05-05T21:27:40.819536: step 8230, loss 1.22975, acc 0.625\n",
            "2022-05-05T21:27:41.574431: step 8231, loss 1.23211, acc 0.59375\n",
            "2022-05-05T21:27:42.280733: step 8232, loss 1.42413, acc 0.53125\n",
            "2022-05-05T21:27:43.003085: step 8233, loss 1.34231, acc 0.515625\n",
            "2022-05-05T21:27:43.726830: step 8234, loss 1.37869, acc 0.53125\n",
            "2022-05-05T21:27:44.424726: step 8235, loss 1.1704, acc 0.609375\n",
            "2022-05-05T21:27:45.131619: step 8236, loss 1.2893, acc 0.515625\n",
            "2022-05-05T21:27:45.812510: step 8237, loss 1.3428, acc 0.5625\n",
            "2022-05-05T21:27:46.430649: step 8238, loss 1.36844, acc 0.53125\n",
            "2022-05-05T21:27:47.067664: step 8239, loss 1.41034, acc 0.4375\n",
            "2022-05-05T21:27:47.701095: step 8240, loss 1.36546, acc 0.515625\n",
            "2022-05-05T21:27:48.321217: step 8241, loss 1.29924, acc 0.546875\n",
            "2022-05-05T21:27:48.971382: step 8242, loss 1.34328, acc 0.46875\n",
            "2022-05-05T21:27:49.666890: step 8243, loss 1.37877, acc 0.5\n",
            "2022-05-05T21:27:50.368120: step 8244, loss 1.39031, acc 0.515625\n",
            "2022-05-05T21:27:51.085635: step 8245, loss 1.32378, acc 0.5625\n",
            "2022-05-05T21:27:51.779413: step 8246, loss 1.49026, acc 0.46875\n",
            "2022-05-05T21:27:52.484673: step 8247, loss 1.25153, acc 0.5625\n",
            "2022-05-05T21:27:53.186058: step 8248, loss 1.28992, acc 0.5\n",
            "2022-05-05T21:27:53.895709: step 8249, loss 1.3745, acc 0.515625\n",
            "2022-05-05T21:27:54.591199: step 8250, loss 1.24369, acc 0.53125\n",
            "2022-05-05T21:27:55.295085: step 8251, loss 1.39494, acc 0.46875\n",
            "2022-05-05T21:27:56.018847: step 8252, loss 1.20959, acc 0.515625\n",
            "2022-05-05T21:27:56.727139: step 8253, loss 1.36763, acc 0.578125\n",
            "2022-05-05T21:27:57.431653: step 8254, loss 1.23976, acc 0.59375\n",
            "2022-05-05T21:27:58.142900: step 8255, loss 1.34956, acc 0.578125\n",
            "2022-05-05T21:27:58.854385: step 8256, loss 1.33958, acc 0.515625\n",
            "2022-05-05T21:27:59.571102: step 8257, loss 1.31564, acc 0.546875\n",
            "2022-05-05T21:28:00.247266: step 8258, loss 1.47811, acc 0.5\n",
            "2022-05-05T21:28:00.890058: step 8259, loss 1.48268, acc 0.4375\n",
            "2022-05-05T21:28:01.548453: step 8260, loss 1.34341, acc 0.53125\n",
            "2022-05-05T21:28:02.207684: step 8261, loss 1.39559, acc 0.5\n",
            "2022-05-05T21:28:02.851741: step 8262, loss 1.4138, acc 0.4375\n",
            "2022-05-05T21:28:03.573096: step 8263, loss 1.46322, acc 0.421875\n",
            "2022-05-05T21:28:04.317874: step 8264, loss 1.50712, acc 0.40625\n",
            "2022-05-05T21:28:05.039171: step 8265, loss 1.30738, acc 0.609375\n",
            "2022-05-05T21:28:05.724774: step 8266, loss 1.06904, acc 0.625\n",
            "2022-05-05T21:28:06.392460: step 8267, loss 1.31028, acc 0.546875\n",
            "2022-05-05T21:28:07.089520: step 8268, loss 1.41968, acc 0.53125\n",
            "2022-05-05T21:28:07.746846: step 8269, loss 1.59131, acc 0.421875\n",
            "2022-05-05T21:28:08.377810: step 8270, loss 1.3257, acc 0.546875\n",
            "2022-05-05T21:28:08.984809: step 8271, loss 1.44762, acc 0.484375\n",
            "2022-05-05T21:28:09.626175: step 8272, loss 1.13827, acc 0.671875\n",
            "2022-05-05T21:28:10.321664: step 8273, loss 1.25752, acc 0.5\n",
            "2022-05-05T21:28:11.033759: step 8274, loss 1.31958, acc 0.546875\n",
            "2022-05-05T21:28:11.742210: step 8275, loss 1.44045, acc 0.421875\n",
            "2022-05-05T21:28:12.461401: step 8276, loss 1.33772, acc 0.4375\n",
            "2022-05-05T21:28:13.151201: step 8277, loss 1.32283, acc 0.515625\n",
            "2022-05-05T21:28:13.822552: step 8278, loss 1.33539, acc 0.5625\n",
            "2022-05-05T21:28:14.479659: step 8279, loss 1.36427, acc 0.5\n",
            "2022-05-05T21:28:15.104976: step 8280, loss 1.22498, acc 0.609375\n",
            "2022-05-05T21:28:15.733874: step 8281, loss 1.52561, acc 0.515625\n",
            "2022-05-05T21:28:16.344584: step 8282, loss 1.41339, acc 0.46875\n",
            "2022-05-05T21:28:16.969603: step 8283, loss 1.58696, acc 0.34375\n",
            "2022-05-05T21:28:17.569023: step 8284, loss 1.3193, acc 0.5625\n",
            "2022-05-05T21:28:18.189641: step 8285, loss 1.33955, acc 0.53125\n",
            "2022-05-05T21:28:18.811800: step 8286, loss 0.893369, acc 0.734375\n",
            "2022-05-05T21:28:19.469816: step 8287, loss 1.42747, acc 0.484375\n",
            "2022-05-05T21:28:20.133365: step 8288, loss 1.35108, acc 0.59375\n",
            "2022-05-05T21:28:20.774456: step 8289, loss 1.30818, acc 0.578125\n",
            "2022-05-05T21:28:21.399812: step 8290, loss 1.12907, acc 0.65625\n",
            "2022-05-05T21:28:22.028729: step 8291, loss 1.3197, acc 0.515625\n",
            "2022-05-05T21:28:22.689824: step 8292, loss 1.42518, acc 0.515625\n",
            "2022-05-05T21:28:23.318969: step 8293, loss 1.20161, acc 0.515625\n",
            "2022-05-05T21:28:23.976580: step 8294, loss 1.45846, acc 0.453125\n",
            "2022-05-05T21:28:24.607174: step 8295, loss 1.42071, acc 0.578125\n",
            "2022-05-05T21:28:25.242590: step 8296, loss 1.36632, acc 0.5625\n",
            "2022-05-05T21:28:25.879530: step 8297, loss 1.23277, acc 0.5\n",
            "2022-05-05T21:28:26.513365: step 8298, loss 1.44946, acc 0.5\n",
            "2022-05-05T21:28:27.159218: step 8299, loss 1.31849, acc 0.515625\n",
            "2022-05-05T21:28:27.769248: step 8300, loss 1.24639, acc 0.5\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:28:27.912639: step 8300, loss 1.83655, acc 0.39\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-8300\n",
            "\n",
            "2022-05-05T21:28:28.748492: step 8301, loss 1.36395, acc 0.453125\n",
            "2022-05-05T21:28:29.380340: step 8302, loss 1.28888, acc 0.5625\n",
            "2022-05-05T21:28:29.990419: step 8303, loss 1.26414, acc 0.5\n",
            "2022-05-05T21:28:30.648962: step 8304, loss 1.1825, acc 0.640625\n",
            "2022-05-05T21:28:31.365396: step 8305, loss 1.33954, acc 0.515625\n",
            "2022-05-05T21:28:32.064726: step 8306, loss 1.19469, acc 0.59375\n",
            "2022-05-05T21:28:32.812589: step 8307, loss 1.25624, acc 0.5625\n",
            "2022-05-05T21:28:33.536899: step 8308, loss 1.25184, acc 0.515625\n",
            "2022-05-05T21:28:34.260085: step 8309, loss 1.28747, acc 0.59375\n",
            "2022-05-05T21:28:34.975675: step 8310, loss 1.41484, acc 0.484375\n",
            "2022-05-05T21:28:35.673014: step 8311, loss 1.38239, acc 0.53125\n",
            "2022-05-05T21:28:36.392565: step 8312, loss 1.52497, acc 0.5\n",
            "2022-05-05T21:28:37.096227: step 8313, loss 1.31922, acc 0.578125\n",
            "2022-05-05T21:28:37.836675: step 8314, loss 1.24006, acc 0.59375\n",
            "2022-05-05T21:28:38.548172: step 8315, loss 1.12124, acc 0.578125\n",
            "2022-05-05T21:28:39.236156: step 8316, loss 1.45811, acc 0.453125\n",
            "2022-05-05T21:28:39.972114: step 8317, loss 1.48165, acc 0.484375\n",
            "2022-05-05T21:28:40.686225: step 8318, loss 1.34906, acc 0.53125\n",
            "2022-05-05T21:28:41.381098: step 8319, loss 1.32222, acc 0.546875\n",
            "2022-05-05T21:28:42.090036: step 8320, loss 1.32203, acc 0.546875\n",
            "2022-05-05T21:28:42.824544: step 8321, loss 1.25332, acc 0.53125\n",
            "2022-05-05T21:28:43.490327: step 8322, loss 1.40496, acc 0.484375\n",
            "2022-05-05T21:28:44.198003: step 8323, loss 1.40635, acc 0.578125\n",
            "2022-05-05T21:28:44.880109: step 8324, loss 1.355, acc 0.53125\n",
            "2022-05-05T21:28:45.532952: step 8325, loss 1.12505, acc 0.625\n",
            "2022-05-05T21:28:46.198257: step 8326, loss 1.48493, acc 0.453125\n",
            "2022-05-05T21:28:46.877842: step 8327, loss 1.35475, acc 0.5\n",
            "2022-05-05T21:28:47.517467: step 8328, loss 1.45461, acc 0.53125\n",
            "2022-05-05T21:28:48.162894: step 8329, loss 1.55041, acc 0.390625\n",
            "2022-05-05T21:28:48.803157: step 8330, loss 1.24969, acc 0.546875\n",
            "2022-05-05T21:28:49.433389: step 8331, loss 1.2292, acc 0.578125\n",
            "2022-05-05T21:28:50.054881: step 8332, loss 1.37106, acc 0.59375\n",
            "2022-05-05T21:28:50.667102: step 8333, loss 1.45595, acc 0.5625\n",
            "2022-05-05T21:28:51.297115: step 8334, loss 1.27064, acc 0.609375\n",
            "2022-05-05T21:28:51.939736: step 8335, loss 1.47443, acc 0.515625\n",
            "2022-05-05T21:28:52.575250: step 8336, loss 1.40276, acc 0.640625\n",
            "2022-05-05T21:28:53.233487: step 8337, loss 1.50728, acc 0.5625\n",
            "2022-05-05T21:28:53.866390: step 8338, loss 1.40951, acc 0.515625\n",
            "2022-05-05T21:28:54.508465: step 8339, loss 1.56995, acc 0.390625\n",
            "2022-05-05T21:28:55.140906: step 8340, loss 1.45056, acc 0.515625\n",
            "2022-05-05T21:28:55.760099: step 8341, loss 1.17755, acc 0.625\n",
            "2022-05-05T21:28:56.400274: step 8342, loss 1.35008, acc 0.53125\n",
            "2022-05-05T21:28:57.032837: step 8343, loss 1.30392, acc 0.484375\n",
            "2022-05-05T21:28:57.644736: step 8344, loss 1.32966, acc 0.5625\n",
            "2022-05-05T21:28:58.273090: step 8345, loss 1.42389, acc 0.453125\n",
            "2022-05-05T21:28:58.893891: step 8346, loss 1.42997, acc 0.515625\n",
            "2022-05-05T21:28:59.522600: step 8347, loss 1.27965, acc 0.5625\n",
            "2022-05-05T21:29:00.135192: step 8348, loss 1.22822, acc 0.578125\n",
            "2022-05-05T21:29:00.752172: step 8349, loss 1.24799, acc 0.515625\n",
            "2022-05-05T21:29:01.376581: step 8350, loss 1.26653, acc 0.53125\n",
            "2022-05-05T21:29:01.987802: step 8351, loss 1.37558, acc 0.5625\n",
            "2022-05-05T21:29:02.611139: step 8352, loss 1.34299, acc 0.5\n",
            "2022-05-05T21:29:03.226396: step 8353, loss 1.48422, acc 0.46875\n",
            "2022-05-05T21:29:03.860386: step 8354, loss 1.43369, acc 0.546875\n",
            "2022-05-05T21:29:04.488929: step 8355, loss 1.37241, acc 0.625\n",
            "2022-05-05T21:29:05.100336: step 8356, loss 1.5046, acc 0.40625\n",
            "2022-05-05T21:29:05.725798: step 8357, loss 1.45212, acc 0.421875\n",
            "2022-05-05T21:29:06.348810: step 8358, loss 1.47378, acc 0.5\n",
            "2022-05-05T21:29:06.956685: step 8359, loss 1.10556, acc 0.625\n",
            "2022-05-05T21:29:07.577090: step 8360, loss 1.43156, acc 0.453125\n",
            "2022-05-05T21:29:08.177727: step 8361, loss 1.49752, acc 0.484375\n",
            "2022-05-05T21:29:08.806598: step 8362, loss 1.31792, acc 0.546875\n",
            "2022-05-05T21:29:09.425890: step 8363, loss 1.09323, acc 0.625\n",
            "2022-05-05T21:29:10.032808: step 8364, loss 1.26929, acc 0.546875\n",
            "2022-05-05T21:29:10.656359: step 8365, loss 1.41864, acc 0.484375\n",
            "2022-05-05T21:29:11.263671: step 8366, loss 1.28365, acc 0.578125\n",
            "2022-05-05T21:29:11.885537: step 8367, loss 1.40568, acc 0.546875\n",
            "2022-05-05T21:29:12.507820: step 8368, loss 1.36439, acc 0.546875\n",
            "2022-05-05T21:29:13.119703: step 8369, loss 1.21063, acc 0.640625\n",
            "2022-05-05T21:29:13.743473: step 8370, loss 1.07527, acc 0.671875\n",
            "2022-05-05T21:29:14.384214: step 8371, loss 1.46826, acc 0.53125\n",
            "2022-05-05T21:29:15.002061: step 8372, loss 1.26562, acc 0.65625\n",
            "2022-05-05T21:29:15.612768: step 8373, loss 1.47074, acc 0.484375\n",
            "2022-05-05T21:29:16.215326: step 8374, loss 1.44805, acc 0.40625\n",
            "2022-05-05T21:29:16.836849: step 8375, loss 1.35253, acc 0.46875\n",
            "2022-05-05T21:29:17.447664: step 8376, loss 1.34748, acc 0.578125\n",
            "2022-05-05T21:29:18.063600: step 8377, loss 1.58776, acc 0.40625\n",
            "2022-05-05T21:29:18.688956: step 8378, loss 1.29256, acc 0.578125\n",
            "2022-05-05T21:29:19.296975: step 8379, loss 1.36067, acc 0.5\n",
            "2022-05-05T21:29:19.928023: step 8380, loss 1.50436, acc 0.5\n",
            "2022-05-05T21:29:20.534649: step 8381, loss 1.35469, acc 0.59375\n",
            "2022-05-05T21:29:21.163538: step 8382, loss 1.53411, acc 0.40625\n",
            "2022-05-05T21:29:21.802419: step 8383, loss 1.33959, acc 0.546875\n",
            "2022-05-05T21:29:22.419913: step 8384, loss 1.4398, acc 0.53125\n",
            "2022-05-05T21:29:23.046597: step 8385, loss 1.27127, acc 0.515625\n",
            "2022-05-05T21:29:23.662813: step 8386, loss 1.19344, acc 0.609375\n",
            "2022-05-05T21:29:24.321513: step 8387, loss 1.1863, acc 0.59375\n",
            "2022-05-05T21:29:24.940673: step 8388, loss 1.49899, acc 0.5\n",
            "2022-05-05T21:29:25.554904: step 8389, loss 1.27434, acc 0.546875\n",
            "2022-05-05T21:29:26.177123: step 8390, loss 1.35314, acc 0.46875\n",
            "2022-05-05T21:29:26.792238: step 8391, loss 1.2969, acc 0.515625\n",
            "2022-05-05T21:29:27.404337: step 8392, loss 1.49121, acc 0.453125\n",
            "2022-05-05T21:29:28.021909: step 8393, loss 1.33707, acc 0.484375\n",
            "2022-05-05T21:29:28.628912: step 8394, loss 1.30132, acc 0.59375\n",
            "2022-05-05T21:29:29.258278: step 8395, loss 1.30189, acc 0.515625\n",
            "2022-05-05T21:29:29.872522: step 8396, loss 1.29331, acc 0.609375\n",
            "2022-05-05T21:29:30.499440: step 8397, loss 1.36304, acc 0.546875\n",
            "2022-05-05T21:29:31.113180: step 8398, loss 1.3277, acc 0.546875\n",
            "2022-05-05T21:29:31.729412: step 8399, loss 1.40948, acc 0.515625\n",
            "2022-05-05T21:29:32.349533: step 8400, loss 1.09875, acc 0.640625\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:29:32.494858: step 8400, loss 2.04139, acc 0.3\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-8400\n",
            "\n",
            "2022-05-05T21:29:33.286576: step 8401, loss 1.04704, acc 0.640625\n",
            "2022-05-05T21:29:33.887030: step 8402, loss 1.64908, acc 0.46875\n",
            "2022-05-05T21:29:34.506414: step 8403, loss 1.48388, acc 0.46875\n",
            "2022-05-05T21:29:35.127348: step 8404, loss 1.33059, acc 0.53125\n",
            "2022-05-05T21:29:35.739382: step 8405, loss 1.51357, acc 0.4375\n",
            "2022-05-05T21:29:36.357165: step 8406, loss 1.24555, acc 0.609375\n",
            "2022-05-05T21:29:36.957357: step 8407, loss 1.30212, acc 0.53125\n",
            "2022-05-05T21:29:37.576609: step 8408, loss 1.28027, acc 0.578125\n",
            "2022-05-05T21:29:38.188744: step 8409, loss 1.16828, acc 0.578125\n",
            "2022-05-05T21:29:38.832832: step 8410, loss 1.3671, acc 0.5\n",
            "2022-05-05T21:29:39.458067: step 8411, loss 1.42783, acc 0.484375\n",
            "2022-05-05T21:29:40.070067: step 8412, loss 1.45854, acc 0.53125\n",
            "2022-05-05T21:29:40.698621: step 8413, loss 1.40644, acc 0.5\n",
            "2022-05-05T21:29:41.325175: step 8414, loss 1.27962, acc 0.515625\n",
            "2022-05-05T21:29:42.014802: step 8415, loss 1.44505, acc 0.546875\n",
            "2022-05-05T21:29:42.668397: step 8416, loss 1.68083, acc 0.4375\n",
            "2022-05-05T21:29:43.329374: step 8417, loss 1.31422, acc 0.53125\n",
            "2022-05-05T21:29:44.015026: step 8418, loss 1.45489, acc 0.484375\n",
            "2022-05-05T21:29:44.692493: step 8419, loss 1.31017, acc 0.546875\n",
            "2022-05-05T21:29:45.343817: step 8420, loss 1.30372, acc 0.578125\n",
            "2022-05-05T21:29:45.975527: step 8421, loss 1.26963, acc 0.546875\n",
            "2022-05-05T21:29:46.618712: step 8422, loss 1.49633, acc 0.421875\n",
            "2022-05-05T21:29:47.249696: step 8423, loss 1.33369, acc 0.546875\n",
            "2022-05-05T21:29:47.914185: step 8424, loss 1.48238, acc 0.4375\n",
            "2022-05-05T21:29:48.572936: step 8425, loss 1.39025, acc 0.5625\n",
            "2022-05-05T21:29:49.209251: step 8426, loss 1.22344, acc 0.59375\n",
            "2022-05-05T21:29:49.882737: step 8427, loss 1.48034, acc 0.515625\n",
            "2022-05-05T21:29:50.547437: step 8428, loss 1.54355, acc 0.359375\n",
            "2022-05-05T21:29:51.210167: step 8429, loss 1.36225, acc 0.609375\n",
            "2022-05-05T21:29:51.866573: step 8430, loss 1.39683, acc 0.5\n",
            "2022-05-05T21:29:52.507275: step 8431, loss 1.29343, acc 0.53125\n",
            "2022-05-05T21:29:53.142155: step 8432, loss 1.39314, acc 0.40625\n",
            "2022-05-05T21:29:53.774150: step 8433, loss 1.37141, acc 0.5625\n",
            "2022-05-05T21:29:54.384873: step 8434, loss 1.39215, acc 0.546875\n",
            "2022-05-05T21:29:55.017113: step 8435, loss 1.49997, acc 0.421875\n",
            "2022-05-05T21:29:55.641405: step 8436, loss 1.33731, acc 0.5\n",
            "2022-05-05T21:29:56.281299: step 8437, loss 1.49394, acc 0.453125\n",
            "2022-05-05T21:29:56.897641: step 8438, loss 1.41167, acc 0.5625\n",
            "2022-05-05T21:29:57.501611: step 8439, loss 1.57806, acc 0.4375\n",
            "2022-05-05T21:29:58.121822: step 8440, loss 1.4851, acc 0.484375\n",
            "2022-05-05T21:29:58.733519: step 8441, loss 1.38798, acc 0.546875\n",
            "2022-05-05T21:29:59.373046: step 8442, loss 1.36088, acc 0.53125\n",
            "2022-05-05T21:30:00.011421: step 8443, loss 1.43836, acc 0.5625\n",
            "2022-05-05T21:30:00.627473: step 8444, loss 1.55791, acc 0.375\n",
            "2022-05-05T21:30:01.242885: step 8445, loss 1.6278, acc 0.34375\n",
            "2022-05-05T21:30:01.867646: step 8446, loss 1.26868, acc 0.5625\n",
            "2022-05-05T21:30:02.490952: step 8447, loss 1.31049, acc 0.46875\n",
            "2022-05-05T21:30:03.121763: step 8448, loss 1.47775, acc 0.421875\n",
            "2022-05-05T21:30:03.728598: step 8449, loss 1.26983, acc 0.5625\n",
            "2022-05-05T21:30:04.361118: step 8450, loss 1.59362, acc 0.421875\n",
            "2022-05-05T21:30:04.994699: step 8451, loss 1.62167, acc 0.4375\n",
            "2022-05-05T21:30:05.649082: step 8452, loss 1.4959, acc 0.4375\n",
            "2022-05-05T21:30:06.269721: step 8453, loss 1.10381, acc 0.65625\n",
            "2022-05-05T21:30:06.882108: step 8454, loss 1.4586, acc 0.40625\n",
            "2022-05-05T21:30:07.507223: step 8455, loss 1.57747, acc 0.453125\n",
            "2022-05-05T21:30:08.121518: step 8456, loss 1.41576, acc 0.484375\n",
            "2022-05-05T21:30:08.744120: step 8457, loss 1.38576, acc 0.578125\n",
            "2022-05-05T21:30:09.358183: step 8458, loss 1.43961, acc 0.484375\n",
            "2022-05-05T21:30:09.981360: step 8459, loss 1.46696, acc 0.546875\n",
            "2022-05-05T21:30:10.600662: step 8460, loss 1.40996, acc 0.4375\n",
            "2022-05-05T21:30:11.193793: step 8461, loss 1.34679, acc 0.5\n",
            "2022-05-05T21:30:11.806675: step 8462, loss 1.45133, acc 0.515625\n",
            "2022-05-05T21:30:12.425853: step 8463, loss 1.3545, acc 0.453125\n",
            "2022-05-05T21:30:13.031265: step 8464, loss 1.41439, acc 0.546875\n",
            "2022-05-05T21:30:13.646254: step 8465, loss 1.53337, acc 0.40625\n",
            "2022-05-05T21:30:14.243060: step 8466, loss 1.30364, acc 0.5625\n",
            "2022-05-05T21:30:14.858058: step 8467, loss 1.45646, acc 0.453125\n",
            "2022-05-05T21:30:15.467483: step 8468, loss 1.45629, acc 0.453125\n",
            "2022-05-05T21:30:16.096709: step 8469, loss 1.3171, acc 0.59375\n",
            "2022-05-05T21:30:16.711412: step 8470, loss 1.45349, acc 0.46875\n",
            "2022-05-05T21:30:17.323147: step 8471, loss 1.28004, acc 0.53125\n",
            "2022-05-05T21:30:17.941682: step 8472, loss 1.58375, acc 0.40625\n",
            "2022-05-05T21:30:18.560333: step 8473, loss 1.20649, acc 0.625\n",
            "2022-05-05T21:30:19.165397: step 8474, loss 1.44035, acc 0.484375\n",
            "2022-05-05T21:30:19.782112: step 8475, loss 1.2146, acc 0.53125\n",
            "2022-05-05T21:30:20.387809: step 8476, loss 1.42145, acc 0.546875\n",
            "2022-05-05T21:30:21.009255: step 8477, loss 1.34412, acc 0.5\n",
            "2022-05-05T21:30:21.624544: step 8478, loss 1.36269, acc 0.453125\n",
            "2022-05-05T21:30:22.229074: step 8479, loss 1.40071, acc 0.5\n",
            "2022-05-05T21:30:22.846590: step 8480, loss 1.30656, acc 0.5\n",
            "2022-05-05T21:30:23.452114: step 8481, loss 1.37518, acc 0.53125\n",
            "2022-05-05T21:30:24.073617: step 8482, loss 1.36394, acc 0.578125\n",
            "2022-05-05T21:30:24.687891: step 8483, loss 1.36719, acc 0.515625\n",
            "2022-05-05T21:30:25.292886: step 8484, loss 1.44562, acc 0.4375\n",
            "2022-05-05T21:30:25.914244: step 8485, loss 1.3576, acc 0.515625\n",
            "2022-05-05T21:30:26.552753: step 8486, loss 1.49244, acc 0.390625\n",
            "2022-05-05T21:30:27.165630: step 8487, loss 1.43521, acc 0.484375\n",
            "2022-05-05T21:30:27.788373: step 8488, loss 1.34826, acc 0.546875\n",
            "2022-05-05T21:30:28.392803: step 8489, loss 1.36412, acc 0.515625\n",
            "2022-05-05T21:30:29.013683: step 8490, loss 1.26003, acc 0.59375\n",
            "2022-05-05T21:30:29.633740: step 8491, loss 1.46179, acc 0.453125\n",
            "2022-05-05T21:30:30.249573: step 8492, loss 1.30044, acc 0.546875\n",
            "2022-05-05T21:30:30.878672: step 8493, loss 1.29895, acc 0.53125\n",
            "2022-05-05T21:30:31.492816: step 8494, loss 1.30595, acc 0.5\n",
            "2022-05-05T21:30:32.112236: step 8495, loss 1.25694, acc 0.546875\n",
            "2022-05-05T21:30:32.731814: step 8496, loss 1.71765, acc 0.453125\n",
            "2022-05-05T21:30:33.362287: step 8497, loss 1.56106, acc 0.40625\n",
            "2022-05-05T21:30:33.995581: step 8498, loss 1.28083, acc 0.5625\n",
            "2022-05-05T21:30:34.617710: step 8499, loss 1.56247, acc 0.4375\n",
            "2022-05-05T21:30:35.245285: step 8500, loss 1.29673, acc 0.578125\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:30:35.402586: step 8500, loss 1.96623, acc 0.34\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-8500\n",
            "\n",
            "2022-05-05T21:30:36.201311: step 8501, loss 1.34715, acc 0.515625\n",
            "2022-05-05T21:30:36.844596: step 8502, loss 1.43639, acc 0.515625\n",
            "2022-05-05T21:30:37.465784: step 8503, loss 1.36858, acc 0.453125\n",
            "2022-05-05T21:30:38.084111: step 8504, loss 1.4314, acc 0.5\n",
            "2022-05-05T21:30:38.687491: step 8505, loss 1.50594, acc 0.453125\n",
            "2022-05-05T21:30:39.310415: step 8506, loss 1.37031, acc 0.5625\n",
            "2022-05-05T21:30:39.910728: step 8507, loss 1.05776, acc 0.71875\n",
            "2022-05-05T21:30:40.528788: step 8508, loss 1.29388, acc 0.546875\n",
            "2022-05-05T21:30:41.141736: step 8509, loss 1.33013, acc 0.4375\n",
            "2022-05-05T21:30:41.758361: step 8510, loss 1.35515, acc 0.578125\n",
            "2022-05-05T21:30:42.370624: step 8511, loss 1.19472, acc 0.609375\n",
            "2022-05-05T21:30:42.976243: step 8512, loss 1.152, acc 0.59375\n",
            "2022-05-05T21:30:43.586867: step 8513, loss 1.27849, acc 0.484375\n",
            "2022-05-05T21:30:44.197570: step 8514, loss 1.25216, acc 0.46875\n",
            "2022-05-05T21:30:44.797020: step 8515, loss 1.4221, acc 0.46875\n",
            "2022-05-05T21:30:45.422862: step 8516, loss 1.40906, acc 0.4375\n",
            "2022-05-05T21:30:46.027745: step 8517, loss 1.38233, acc 0.484375\n",
            "2022-05-05T21:30:46.651653: step 8518, loss 1.38866, acc 0.4375\n",
            "2022-05-05T21:30:47.280634: step 8519, loss 1.48054, acc 0.46875\n",
            "2022-05-05T21:30:47.887215: step 8520, loss 1.3148, acc 0.53125\n",
            "2022-05-05T21:30:48.508918: step 8521, loss 1.44572, acc 0.484375\n",
            "2022-05-05T21:30:49.119729: step 8522, loss 1.27841, acc 0.5\n",
            "2022-05-05T21:30:49.750713: step 8523, loss 1.1763, acc 0.578125\n",
            "2022-05-05T21:30:50.375763: step 8524, loss 1.33926, acc 0.578125\n",
            "2022-05-05T21:30:51.003502: step 8525, loss 1.36045, acc 0.484375\n",
            "2022-05-05T21:30:51.658545: step 8526, loss 1.04004, acc 0.65625\n",
            "2022-05-05T21:30:52.280010: step 8527, loss 1.50382, acc 0.46875\n",
            "2022-05-05T21:30:52.917547: step 8528, loss 1.46506, acc 0.421875\n",
            "2022-05-05T21:30:53.541484: step 8529, loss 1.57909, acc 0.40625\n",
            "2022-05-05T21:30:54.156253: step 8530, loss 1.54647, acc 0.515625\n",
            "2022-05-05T21:30:54.795771: step 8531, loss 1.21394, acc 0.609375\n",
            "2022-05-05T21:30:55.409292: step 8532, loss 1.43178, acc 0.5\n",
            "2022-05-05T21:30:56.049680: step 8533, loss 1.37158, acc 0.5\n",
            "2022-05-05T21:30:56.674973: step 8534, loss 1.23857, acc 0.53125\n",
            "2022-05-05T21:30:57.303319: step 8535, loss 1.30197, acc 0.453125\n",
            "2022-05-05T21:30:57.920342: step 8536, loss 1.3749, acc 0.5625\n",
            "2022-05-05T21:30:58.526640: step 8537, loss 1.45154, acc 0.40625\n",
            "2022-05-05T21:30:59.162515: step 8538, loss 1.37298, acc 0.53125\n",
            "2022-05-05T21:30:59.806912: step 8539, loss 1.42371, acc 0.484375\n",
            "2022-05-05T21:31:00.414898: step 8540, loss 1.43742, acc 0.546875\n",
            "2022-05-05T21:31:01.038223: step 8541, loss 1.18848, acc 0.546875\n",
            "2022-05-05T21:31:01.642479: step 8542, loss 1.36112, acc 0.515625\n",
            "2022-05-05T21:31:02.256567: step 8543, loss 1.29561, acc 0.5625\n",
            "2022-05-05T21:31:02.873636: step 8544, loss 1.24571, acc 0.609375\n",
            "2022-05-05T21:31:03.479179: step 8545, loss 1.37276, acc 0.546875\n",
            "2022-05-05T21:31:04.086754: step 8546, loss 1.55312, acc 0.453125\n",
            "2022-05-05T21:31:04.684913: step 8547, loss 1.43727, acc 0.515625\n",
            "2022-05-05T21:31:05.288887: step 8548, loss 1.26631, acc 0.5625\n",
            "2022-05-05T21:31:05.901904: step 8549, loss 1.3977, acc 0.5\n",
            "2022-05-05T21:31:06.500605: step 8550, loss 1.18494, acc 0.625\n",
            "2022-05-05T21:31:07.121324: step 8551, loss 1.35502, acc 0.546875\n",
            "2022-05-05T21:31:07.740115: step 8552, loss 1.39753, acc 0.53125\n",
            "2022-05-05T21:31:08.357916: step 8553, loss 1.15303, acc 0.625\n",
            "2022-05-05T21:31:08.965832: step 8554, loss 1.46554, acc 0.484375\n",
            "2022-05-05T21:31:09.573147: step 8555, loss 1.54683, acc 0.4375\n",
            "2022-05-05T21:31:10.179865: step 8556, loss 1.63259, acc 0.390625\n",
            "2022-05-05T21:31:10.784161: step 8557, loss 1.37218, acc 0.546875\n",
            "2022-05-05T21:31:11.399779: step 8558, loss 1.36151, acc 0.484375\n",
            "2022-05-05T21:31:11.996003: step 8559, loss 1.31349, acc 0.609375\n",
            "2022-05-05T21:31:12.609128: step 8560, loss 1.33866, acc 0.46875\n",
            "2022-05-05T21:31:13.208303: step 8561, loss 1.44505, acc 0.53125\n",
            "2022-05-05T21:31:13.813375: step 8562, loss 1.32959, acc 0.484375\n",
            "2022-05-05T21:31:14.421341: step 8563, loss 1.49029, acc 0.484375\n",
            "2022-05-05T21:31:15.015107: step 8564, loss 1.47375, acc 0.53125\n",
            "2022-05-05T21:31:15.628701: step 8565, loss 1.38043, acc 0.515625\n",
            "2022-05-05T21:31:16.248835: step 8566, loss 1.6966, acc 0.421875\n",
            "2022-05-05T21:31:16.851516: step 8567, loss 1.36823, acc 0.515625\n",
            "2022-05-05T21:31:17.462668: step 8568, loss 1.26705, acc 0.53125\n",
            "2022-05-05T21:31:18.087702: step 8569, loss 1.0596, acc 0.546875\n",
            "2022-05-05T21:31:18.699472: step 8570, loss 1.37252, acc 0.5\n",
            "2022-05-05T21:31:19.307367: step 8571, loss 1.22852, acc 0.609375\n",
            "2022-05-05T21:31:19.913983: step 8572, loss 1.34047, acc 0.546875\n",
            "2022-05-05T21:31:20.525383: step 8573, loss 1.2711, acc 0.53125\n",
            "2022-05-05T21:31:21.122803: step 8574, loss 1.24331, acc 0.4375\n",
            "2022-05-05T21:31:21.733334: step 8575, loss 1.47149, acc 0.46875\n",
            "2022-05-05T21:31:22.351807: step 8576, loss 1.30741, acc 0.5\n",
            "2022-05-05T21:31:22.952150: step 8577, loss 1.8804, acc 0.359375\n",
            "2022-05-05T21:31:23.572759: step 8578, loss 1.52953, acc 0.53125\n",
            "2022-05-05T21:31:24.180715: step 8579, loss 1.32132, acc 0.5625\n",
            "2022-05-05T21:31:24.802506: step 8580, loss 1.40544, acc 0.484375\n",
            "2022-05-05T21:31:25.418124: step 8581, loss 1.50758, acc 0.453125\n",
            "2022-05-05T21:31:26.027932: step 8582, loss 1.4008, acc 0.546875\n",
            "2022-05-05T21:31:26.656087: step 8583, loss 1.33013, acc 0.5\n",
            "2022-05-05T21:31:27.255329: step 8584, loss 1.25519, acc 0.578125\n",
            "2022-05-05T21:31:27.870689: step 8585, loss 1.37592, acc 0.453125\n",
            "2022-05-05T21:31:28.486197: step 8586, loss 1.5228, acc 0.484375\n",
            "2022-05-05T21:31:29.098245: step 8587, loss 1.44983, acc 0.5\n",
            "2022-05-05T21:31:29.724449: step 8588, loss 1.24359, acc 0.53125\n",
            "2022-05-05T21:31:30.327160: step 8589, loss 1.51207, acc 0.4375\n",
            "2022-05-05T21:31:30.947816: step 8590, loss 1.18092, acc 0.546875\n",
            "2022-05-05T21:31:31.553645: step 8591, loss 1.28187, acc 0.5\n",
            "2022-05-05T21:31:32.163035: step 8592, loss 1.3033, acc 0.546875\n",
            "2022-05-05T21:31:32.781714: step 8593, loss 1.27107, acc 0.5625\n",
            "2022-05-05T21:31:33.384100: step 8594, loss 1.29177, acc 0.640625\n",
            "2022-05-05T21:31:33.986806: step 8595, loss 1.32637, acc 0.5\n",
            "2022-05-05T21:31:34.584579: step 8596, loss 1.37559, acc 0.515625\n",
            "2022-05-05T21:31:35.200686: step 8597, loss 1.37735, acc 0.5\n",
            "2022-05-05T21:31:35.811402: step 8598, loss 1.47395, acc 0.5\n",
            "2022-05-05T21:31:36.415518: step 8599, loss 1.32726, acc 0.515625\n",
            "2022-05-05T21:31:37.030016: step 8600, loss 1.28185, acc 0.578125\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:31:37.171812: step 8600, loss 1.8489, acc 0.37\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-8600\n",
            "\n",
            "2022-05-05T21:31:37.920550: step 8601, loss 1.42112, acc 0.546875\n",
            "2022-05-05T21:31:38.541525: step 8602, loss 1.32563, acc 0.484375\n",
            "2022-05-05T21:31:39.157480: step 8603, loss 1.3707, acc 0.53125\n",
            "2022-05-05T21:31:39.764453: step 8604, loss 1.3487, acc 0.5625\n",
            "2022-05-05T21:31:40.381208: step 8605, loss 1.59327, acc 0.46875\n",
            "2022-05-05T21:31:40.991615: step 8606, loss 1.56249, acc 0.4375\n",
            "2022-05-05T21:31:41.608248: step 8607, loss 1.31142, acc 0.484375\n",
            "2022-05-05T21:31:42.223832: step 8608, loss 1.35567, acc 0.546875\n",
            "2022-05-05T21:31:42.822909: step 8609, loss 1.30207, acc 0.46875\n",
            "2022-05-05T21:31:43.453127: step 8610, loss 1.32862, acc 0.515625\n",
            "2022-05-05T21:31:44.060335: step 8611, loss 1.42197, acc 0.5\n",
            "2022-05-05T21:31:44.673664: step 8612, loss 1.26669, acc 0.609375\n",
            "2022-05-05T21:31:45.288849: step 8613, loss 1.45233, acc 0.46875\n",
            "2022-05-05T21:31:45.886962: step 8614, loss 1.38436, acc 0.453125\n",
            "2022-05-05T21:31:46.498609: step 8615, loss 1.28292, acc 0.53125\n",
            "2022-05-05T21:31:47.100171: step 8616, loss 1.63684, acc 0.4375\n",
            "2022-05-05T21:31:47.688822: step 8617, loss 1.63832, acc 0.453125\n",
            "2022-05-05T21:31:48.305437: step 8618, loss 1.3455, acc 0.515625\n",
            "2022-05-05T21:31:48.933603: step 8619, loss 1.45431, acc 0.484375\n",
            "2022-05-05T21:31:49.555525: step 8620, loss 1.34872, acc 0.53125\n",
            "2022-05-05T21:31:50.153084: step 8621, loss 1.44669, acc 0.5625\n",
            "2022-05-05T21:31:50.776380: step 8622, loss 1.4696, acc 0.546875\n",
            "2022-05-05T21:31:51.387765: step 8623, loss 1.26254, acc 0.625\n",
            "2022-05-05T21:31:51.985419: step 8624, loss 1.43435, acc 0.484375\n",
            "2022-05-05T21:31:52.598366: step 8625, loss 1.17648, acc 0.609375\n",
            "2022-05-05T21:31:53.198706: step 8626, loss 1.30038, acc 0.53125\n",
            "2022-05-05T21:31:53.814747: step 8627, loss 1.25056, acc 0.578125\n",
            "2022-05-05T21:31:54.426693: step 8628, loss 1.4741, acc 0.46875\n",
            "2022-05-05T21:31:55.029329: step 8629, loss 1.25721, acc 0.546875\n",
            "2022-05-05T21:31:55.642862: step 8630, loss 1.47795, acc 0.40625\n",
            "2022-05-05T21:31:56.249145: step 8631, loss 1.30095, acc 0.59375\n",
            "2022-05-05T21:31:56.865356: step 8632, loss 1.48583, acc 0.453125\n",
            "2022-05-05T21:31:57.478039: step 8633, loss 1.63197, acc 0.4375\n",
            "2022-05-05T21:31:58.080461: step 8634, loss 1.61782, acc 0.46875\n",
            "2022-05-05T21:31:58.695276: step 8635, loss 1.42372, acc 0.5\n",
            "2022-05-05T21:31:59.318652: step 8636, loss 1.32917, acc 0.59375\n",
            "2022-05-05T21:31:59.945477: step 8637, loss 1.34158, acc 0.546875\n",
            "2022-05-05T21:32:00.549184: step 8638, loss 1.42236, acc 0.515625\n",
            "2022-05-05T21:32:01.145625: step 8639, loss 1.4152, acc 0.453125\n",
            "2022-05-05T21:32:01.752958: step 8640, loss 1.22095, acc 0.578125\n",
            "2022-05-05T21:32:02.359383: step 8641, loss 1.38454, acc 0.484375\n",
            "2022-05-05T21:32:02.966786: step 8642, loss 1.41552, acc 0.4375\n",
            "2022-05-05T21:32:03.577547: step 8643, loss 1.16362, acc 0.578125\n",
            "2022-05-05T21:32:04.182190: step 8644, loss 1.37635, acc 0.484375\n",
            "2022-05-05T21:32:04.797495: step 8645, loss 1.37997, acc 0.515625\n",
            "2022-05-05T21:32:05.398589: step 8646, loss 1.45202, acc 0.515625\n",
            "2022-05-05T21:32:06.014245: step 8647, loss 1.38416, acc 0.46875\n",
            "2022-05-05T21:32:06.613505: step 8648, loss 1.51456, acc 0.453125\n",
            "2022-05-05T21:32:07.221552: step 8649, loss 1.5573, acc 0.421875\n",
            "2022-05-05T21:32:07.825459: step 8650, loss 1.36702, acc 0.5625\n",
            "2022-05-05T21:32:08.431977: step 8651, loss 1.23735, acc 0.53125\n",
            "2022-05-05T21:32:09.041199: step 8652, loss 1.51861, acc 0.484375\n",
            "2022-05-05T21:32:09.672432: step 8653, loss 1.42325, acc 0.5\n",
            "2022-05-05T21:32:10.291790: step 8654, loss 1.28788, acc 0.5\n",
            "2022-05-05T21:32:10.917324: step 8655, loss 1.45359, acc 0.546875\n",
            "2022-05-05T21:32:11.516405: step 8656, loss 1.5246, acc 0.46875\n",
            "2022-05-05T21:32:12.130094: step 8657, loss 1.51845, acc 0.4375\n",
            "2022-05-05T21:32:12.736502: step 8658, loss 1.3021, acc 0.625\n",
            "2022-05-05T21:32:13.353013: step 8659, loss 1.394, acc 0.5\n",
            "2022-05-05T21:32:13.969023: step 8660, loss 1.41198, acc 0.515625\n",
            "2022-05-05T21:32:14.574678: step 8661, loss 1.44305, acc 0.53125\n",
            "2022-05-05T21:32:15.194369: step 8662, loss 1.26674, acc 0.53125\n",
            "2022-05-05T21:32:15.807714: step 8663, loss 1.49176, acc 0.4375\n",
            "2022-05-05T21:32:16.425294: step 8664, loss 1.54314, acc 0.546875\n",
            "2022-05-05T21:32:17.039631: step 8665, loss 1.46755, acc 0.5\n",
            "2022-05-05T21:32:17.645631: step 8666, loss 1.46911, acc 0.515625\n",
            "2022-05-05T21:32:18.258760: step 8667, loss 1.58565, acc 0.390625\n",
            "2022-05-05T21:32:18.859461: step 8668, loss 1.38617, acc 0.484375\n",
            "2022-05-05T21:32:19.489453: step 8669, loss 1.34862, acc 0.484375\n",
            "2022-05-05T21:32:19.984775: step 8670, loss 1.49615, acc 0.510204\n",
            "2022-05-05T21:32:20.612238: step 8671, loss 1.12577, acc 0.640625\n",
            "2022-05-05T21:32:21.218958: step 8672, loss 1.21111, acc 0.65625\n",
            "2022-05-05T21:32:21.834228: step 8673, loss 1.28987, acc 0.578125\n",
            "2022-05-05T21:32:22.463257: step 8674, loss 1.25764, acc 0.59375\n",
            "2022-05-05T21:32:23.071291: step 8675, loss 1.33666, acc 0.515625\n",
            "2022-05-05T21:32:23.683695: step 8676, loss 1.21515, acc 0.609375\n",
            "2022-05-05T21:32:24.300767: step 8677, loss 1.50989, acc 0.46875\n",
            "2022-05-05T21:32:24.912781: step 8678, loss 1.35579, acc 0.484375\n",
            "2022-05-05T21:32:25.523792: step 8679, loss 1.36328, acc 0.53125\n",
            "2022-05-05T21:32:26.121836: step 8680, loss 1.0585, acc 0.640625\n",
            "2022-05-05T21:32:26.751692: step 8681, loss 1.17412, acc 0.5625\n",
            "2022-05-05T21:32:27.367482: step 8682, loss 1.19446, acc 0.578125\n",
            "2022-05-05T21:32:27.975898: step 8683, loss 1.27246, acc 0.5625\n",
            "2022-05-05T21:32:28.583212: step 8684, loss 1.28667, acc 0.53125\n",
            "2022-05-05T21:32:29.199148: step 8685, loss 1.37382, acc 0.546875\n",
            "2022-05-05T21:32:29.830817: step 8686, loss 1.41633, acc 0.515625\n",
            "2022-05-05T21:32:30.465698: step 8687, loss 1.13052, acc 0.625\n",
            "2022-05-05T21:32:31.078814: step 8688, loss 1.07995, acc 0.71875\n",
            "2022-05-05T21:32:31.692170: step 8689, loss 1.25467, acc 0.625\n",
            "2022-05-05T21:32:32.304839: step 8690, loss 1.19443, acc 0.578125\n",
            "2022-05-05T21:32:32.921872: step 8691, loss 1.15955, acc 0.671875\n",
            "2022-05-05T21:32:33.551248: step 8692, loss 1.31506, acc 0.578125\n",
            "2022-05-05T21:32:34.157147: step 8693, loss 1.23728, acc 0.59375\n",
            "2022-05-05T21:32:34.770947: step 8694, loss 1.37672, acc 0.484375\n",
            "2022-05-05T21:32:35.377618: step 8695, loss 1.23536, acc 0.640625\n",
            "2022-05-05T21:32:35.995242: step 8696, loss 1.14617, acc 0.609375\n",
            "2022-05-05T21:32:36.611132: step 8697, loss 1.20117, acc 0.640625\n",
            "2022-05-05T21:32:37.219632: step 8698, loss 1.22538, acc 0.546875\n",
            "2022-05-05T21:32:37.844513: step 8699, loss 1.05952, acc 0.640625\n",
            "2022-05-05T21:32:38.452340: step 8700, loss 1.23004, acc 0.5625\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:32:38.608350: step 8700, loss 2.21335, acc 0.31\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-8700\n",
            "\n",
            "2022-05-05T21:32:39.351804: step 8701, loss 1.16719, acc 0.578125\n",
            "2022-05-05T21:32:39.981402: step 8702, loss 1.30919, acc 0.515625\n",
            "2022-05-05T21:32:40.621644: step 8703, loss 1.42448, acc 0.515625\n",
            "2022-05-05T21:32:41.243351: step 8704, loss 1.11991, acc 0.546875\n",
            "2022-05-05T21:32:41.852546: step 8705, loss 1.21659, acc 0.59375\n",
            "2022-05-05T21:32:42.456113: step 8706, loss 1.30159, acc 0.515625\n",
            "2022-05-05T21:32:43.063273: step 8707, loss 1.30323, acc 0.59375\n",
            "2022-05-05T21:32:43.661448: step 8708, loss 1.12458, acc 0.609375\n",
            "2022-05-05T21:32:44.277152: step 8709, loss 0.952507, acc 0.671875\n",
            "2022-05-05T21:32:44.892100: step 8710, loss 1.35897, acc 0.578125\n",
            "2022-05-05T21:32:45.517040: step 8711, loss 1.17735, acc 0.625\n",
            "2022-05-05T21:32:46.132111: step 8712, loss 1.20055, acc 0.625\n",
            "2022-05-05T21:32:46.724036: step 8713, loss 1.14449, acc 0.65625\n",
            "2022-05-05T21:32:47.343507: step 8714, loss 1.35784, acc 0.484375\n",
            "2022-05-05T21:32:47.963207: step 8715, loss 1.33512, acc 0.546875\n",
            "2022-05-05T21:32:48.563604: step 8716, loss 1.37984, acc 0.53125\n",
            "2022-05-05T21:32:49.185946: step 8717, loss 1.40278, acc 0.53125\n",
            "2022-05-05T21:32:49.783144: step 8718, loss 1.28326, acc 0.578125\n",
            "2022-05-05T21:32:50.396055: step 8719, loss 1.47556, acc 0.515625\n",
            "2022-05-05T21:32:51.024178: step 8720, loss 1.21972, acc 0.671875\n",
            "2022-05-05T21:32:51.619780: step 8721, loss 1.13609, acc 0.6875\n",
            "2022-05-05T21:32:52.233500: step 8722, loss 1.32672, acc 0.578125\n",
            "2022-05-05T21:32:52.835074: step 8723, loss 1.2641, acc 0.578125\n",
            "2022-05-05T21:32:53.448408: step 8724, loss 1.24121, acc 0.578125\n",
            "2022-05-05T21:32:54.052427: step 8725, loss 1.26969, acc 0.53125\n",
            "2022-05-05T21:32:54.670615: step 8726, loss 1.44477, acc 0.46875\n",
            "2022-05-05T21:32:55.278832: step 8727, loss 1.11619, acc 0.59375\n",
            "2022-05-05T21:32:55.884116: step 8728, loss 1.45985, acc 0.421875\n",
            "2022-05-05T21:32:56.506147: step 8729, loss 1.16664, acc 0.5625\n",
            "2022-05-05T21:32:57.106401: step 8730, loss 1.18177, acc 0.546875\n",
            "2022-05-05T21:32:57.727343: step 8731, loss 1.11804, acc 0.5625\n",
            "2022-05-05T21:32:58.338043: step 8732, loss 1.16649, acc 0.546875\n",
            "2022-05-05T21:32:58.945676: step 8733, loss 1.2157, acc 0.578125\n",
            "2022-05-05T21:32:59.570961: step 8734, loss 1.41855, acc 0.453125\n",
            "2022-05-05T21:33:00.172297: step 8735, loss 1.27518, acc 0.515625\n",
            "2022-05-05T21:33:00.797562: step 8736, loss 1.06624, acc 0.703125\n",
            "2022-05-05T21:33:01.412016: step 8737, loss 1.33085, acc 0.515625\n",
            "2022-05-05T21:33:02.010096: step 8738, loss 1.29642, acc 0.515625\n",
            "2022-05-05T21:33:02.627456: step 8739, loss 1.12158, acc 0.625\n",
            "2022-05-05T21:33:03.231245: step 8740, loss 1.67715, acc 0.375\n",
            "2022-05-05T21:33:03.845641: step 8741, loss 1.46974, acc 0.390625\n",
            "2022-05-05T21:33:04.453540: step 8742, loss 1.25631, acc 0.640625\n",
            "2022-05-05T21:33:05.059238: step 8743, loss 1.17576, acc 0.5625\n",
            "2022-05-05T21:33:05.679802: step 8744, loss 1.35573, acc 0.609375\n",
            "2022-05-05T21:33:06.300815: step 8745, loss 1.1422, acc 0.65625\n",
            "2022-05-05T21:33:06.912215: step 8746, loss 1.37155, acc 0.53125\n",
            "2022-05-05T21:33:07.528701: step 8747, loss 1.21951, acc 0.53125\n",
            "2022-05-05T21:33:08.130331: step 8748, loss 1.1269, acc 0.53125\n",
            "2022-05-05T21:33:08.748188: step 8749, loss 1.23658, acc 0.609375\n",
            "2022-05-05T21:33:09.349407: step 8750, loss 1.48255, acc 0.453125\n",
            "2022-05-05T21:33:09.965802: step 8751, loss 1.31994, acc 0.5\n",
            "2022-05-05T21:33:10.578069: step 8752, loss 1.37706, acc 0.484375\n",
            "2022-05-05T21:33:11.170729: step 8753, loss 1.40014, acc 0.484375\n",
            "2022-05-05T21:33:11.805274: step 8754, loss 1.16881, acc 0.609375\n",
            "2022-05-05T21:33:12.412113: step 8755, loss 1.21833, acc 0.625\n",
            "2022-05-05T21:33:13.017132: step 8756, loss 1.23774, acc 0.546875\n",
            "2022-05-05T21:33:13.630652: step 8757, loss 1.3384, acc 0.453125\n",
            "2022-05-05T21:33:14.230897: step 8758, loss 1.18535, acc 0.5625\n",
            "2022-05-05T21:33:14.853509: step 8759, loss 1.07416, acc 0.6875\n",
            "2022-05-05T21:33:15.453860: step 8760, loss 1.20833, acc 0.546875\n",
            "2022-05-05T21:33:16.066820: step 8761, loss 1.67223, acc 0.5\n",
            "2022-05-05T21:33:16.666611: step 8762, loss 1.16646, acc 0.53125\n",
            "2022-05-05T21:33:17.275460: step 8763, loss 1.38273, acc 0.53125\n",
            "2022-05-05T21:33:17.884088: step 8764, loss 1.4189, acc 0.5\n",
            "2022-05-05T21:33:18.484540: step 8765, loss 1.57214, acc 0.4375\n",
            "2022-05-05T21:33:19.101978: step 8766, loss 1.34077, acc 0.578125\n",
            "2022-05-05T21:33:19.702385: step 8767, loss 1.33081, acc 0.5625\n",
            "2022-05-05T21:33:20.310707: step 8768, loss 1.35221, acc 0.546875\n",
            "2022-05-05T21:33:20.917781: step 8769, loss 1.55902, acc 0.515625\n",
            "2022-05-05T21:33:21.526754: step 8770, loss 1.28474, acc 0.546875\n",
            "2022-05-05T21:33:22.157939: step 8771, loss 1.30497, acc 0.5\n",
            "2022-05-05T21:33:22.760827: step 8772, loss 1.15651, acc 0.5625\n",
            "2022-05-05T21:33:23.368751: step 8773, loss 1.45409, acc 0.515625\n",
            "2022-05-05T21:33:23.978562: step 8774, loss 1.28606, acc 0.515625\n",
            "2022-05-05T21:33:24.573841: step 8775, loss 1.08043, acc 0.671875\n",
            "2022-05-05T21:33:25.202044: step 8776, loss 1.59821, acc 0.4375\n",
            "2022-05-05T21:33:25.805190: step 8777, loss 1.41182, acc 0.5\n",
            "2022-05-05T21:33:26.430765: step 8778, loss 1.39205, acc 0.53125\n",
            "2022-05-05T21:33:27.046497: step 8779, loss 1.29298, acc 0.53125\n",
            "2022-05-05T21:33:27.652175: step 8780, loss 1.20535, acc 0.578125\n",
            "2022-05-05T21:33:28.264791: step 8781, loss 1.45671, acc 0.484375\n",
            "2022-05-05T21:33:28.866849: step 8782, loss 1.28093, acc 0.578125\n",
            "2022-05-05T21:33:29.492100: step 8783, loss 1.03203, acc 0.703125\n",
            "2022-05-05T21:33:30.104928: step 8784, loss 1.20722, acc 0.625\n",
            "2022-05-05T21:33:30.716905: step 8785, loss 1.19765, acc 0.640625\n",
            "2022-05-05T21:33:31.333273: step 8786, loss 1.16324, acc 0.609375\n",
            "2022-05-05T21:33:31.942266: step 8787, loss 1.09403, acc 0.65625\n",
            "2022-05-05T21:33:32.575408: step 8788, loss 1.46437, acc 0.5\n",
            "2022-05-05T21:33:33.183313: step 8789, loss 1.3444, acc 0.609375\n",
            "2022-05-05T21:33:33.799671: step 8790, loss 1.10428, acc 0.625\n",
            "2022-05-05T21:33:34.413255: step 8791, loss 1.29485, acc 0.625\n",
            "2022-05-05T21:33:35.022218: step 8792, loss 1.35213, acc 0.5625\n",
            "2022-05-05T21:33:35.632648: step 8793, loss 1.37044, acc 0.484375\n",
            "2022-05-05T21:33:36.235092: step 8794, loss 1.19843, acc 0.609375\n",
            "2022-05-05T21:33:36.852297: step 8795, loss 1.28858, acc 0.5\n",
            "2022-05-05T21:33:37.463150: step 8796, loss 1.33087, acc 0.578125\n",
            "2022-05-05T21:33:38.068775: step 8797, loss 1.31972, acc 0.53125\n",
            "2022-05-05T21:33:38.686487: step 8798, loss 1.39785, acc 0.546875\n",
            "2022-05-05T21:33:39.286624: step 8799, loss 1.27845, acc 0.53125\n",
            "2022-05-05T21:33:39.910602: step 8800, loss 1.24069, acc 0.5625\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:33:40.061969: step 8800, loss 1.91492, acc 0.37\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-8800\n",
            "\n",
            "2022-05-05T21:33:40.817583: step 8801, loss 1.42187, acc 0.546875\n",
            "2022-05-05T21:33:41.415541: step 8802, loss 1.36522, acc 0.53125\n",
            "2022-05-05T21:33:42.031704: step 8803, loss 1.39243, acc 0.46875\n",
            "2022-05-05T21:33:42.659542: step 8804, loss 1.41339, acc 0.4375\n",
            "2022-05-05T21:33:43.260381: step 8805, loss 1.4513, acc 0.5\n",
            "2022-05-05T21:33:43.864577: step 8806, loss 1.45819, acc 0.4375\n",
            "2022-05-05T21:33:44.469596: step 8807, loss 1.36916, acc 0.5\n",
            "2022-05-05T21:33:45.078764: step 8808, loss 1.31944, acc 0.53125\n",
            "2022-05-05T21:33:45.682748: step 8809, loss 1.41159, acc 0.46875\n",
            "2022-05-05T21:33:46.278556: step 8810, loss 1.23109, acc 0.53125\n",
            "2022-05-05T21:33:46.890541: step 8811, loss 1.31675, acc 0.5625\n",
            "2022-05-05T21:33:47.520160: step 8812, loss 1.33242, acc 0.5625\n",
            "2022-05-05T21:33:48.148709: step 8813, loss 1.24237, acc 0.578125\n",
            "2022-05-05T21:33:48.761357: step 8814, loss 1.33374, acc 0.515625\n",
            "2022-05-05T21:33:49.366712: step 8815, loss 1.2661, acc 0.53125\n",
            "2022-05-05T21:33:49.979354: step 8816, loss 1.32169, acc 0.578125\n",
            "2022-05-05T21:33:50.589012: step 8817, loss 1.07725, acc 0.5625\n",
            "2022-05-05T21:33:51.210768: step 8818, loss 1.32301, acc 0.5\n",
            "2022-05-05T21:33:51.825722: step 8819, loss 1.39158, acc 0.46875\n",
            "2022-05-05T21:33:52.436901: step 8820, loss 1.25032, acc 0.546875\n",
            "2022-05-05T21:33:53.072555: step 8821, loss 1.45581, acc 0.5625\n",
            "2022-05-05T21:33:53.680458: step 8822, loss 1.14758, acc 0.625\n",
            "2022-05-05T21:33:54.306839: step 8823, loss 1.40082, acc 0.53125\n",
            "2022-05-05T21:33:54.923036: step 8824, loss 1.19327, acc 0.546875\n",
            "2022-05-05T21:33:55.520400: step 8825, loss 1.18241, acc 0.609375\n",
            "2022-05-05T21:33:56.143919: step 8826, loss 1.30403, acc 0.484375\n",
            "2022-05-05T21:33:56.757092: step 8827, loss 1.34658, acc 0.546875\n",
            "2022-05-05T21:33:57.380421: step 8828, loss 1.45292, acc 0.5\n",
            "2022-05-05T21:33:57.986561: step 8829, loss 1.30299, acc 0.578125\n",
            "2022-05-05T21:33:58.591755: step 8830, loss 1.21573, acc 0.5625\n",
            "2022-05-05T21:33:59.207924: step 8831, loss 1.29043, acc 0.5\n",
            "2022-05-05T21:33:59.818214: step 8832, loss 1.25114, acc 0.5625\n",
            "2022-05-05T21:34:00.438600: step 8833, loss 1.40634, acc 0.484375\n",
            "2022-05-05T21:34:01.041702: step 8834, loss 1.16635, acc 0.59375\n",
            "2022-05-05T21:34:01.648112: step 8835, loss 1.33488, acc 0.5\n",
            "2022-05-05T21:34:02.271767: step 8836, loss 1.32363, acc 0.515625\n",
            "2022-05-05T21:34:02.891644: step 8837, loss 1.2466, acc 0.578125\n",
            "2022-05-05T21:34:03.506297: step 8838, loss 1.18539, acc 0.640625\n",
            "2022-05-05T21:34:04.111945: step 8839, loss 1.33355, acc 0.484375\n",
            "2022-05-05T21:34:04.705210: step 8840, loss 1.35197, acc 0.53125\n",
            "2022-05-05T21:34:05.324145: step 8841, loss 1.39574, acc 0.46875\n",
            "2022-05-05T21:34:05.921842: step 8842, loss 1.46972, acc 0.4375\n",
            "2022-05-05T21:34:06.544115: step 8843, loss 1.45919, acc 0.4375\n",
            "2022-05-05T21:34:07.147166: step 8844, loss 1.37577, acc 0.53125\n",
            "2022-05-05T21:34:07.760611: step 8845, loss 1.43478, acc 0.53125\n",
            "2022-05-05T21:34:08.374169: step 8846, loss 1.37601, acc 0.5\n",
            "2022-05-05T21:34:08.979679: step 8847, loss 1.17629, acc 0.5625\n",
            "2022-05-05T21:34:09.598753: step 8848, loss 1.25516, acc 0.5625\n",
            "2022-05-05T21:34:10.196564: step 8849, loss 1.38327, acc 0.5625\n",
            "2022-05-05T21:34:10.807710: step 8850, loss 1.24181, acc 0.53125\n",
            "2022-05-05T21:34:11.428165: step 8851, loss 1.56528, acc 0.40625\n",
            "2022-05-05T21:34:12.044431: step 8852, loss 1.2536, acc 0.546875\n",
            "2022-05-05T21:34:12.655962: step 8853, loss 1.33248, acc 0.578125\n",
            "2022-05-05T21:34:13.260200: step 8854, loss 1.13925, acc 0.609375\n",
            "2022-05-05T21:34:13.875397: step 8855, loss 1.34022, acc 0.5\n",
            "2022-05-05T21:34:14.488458: step 8856, loss 1.23572, acc 0.546875\n",
            "2022-05-05T21:34:15.099951: step 8857, loss 1.36988, acc 0.53125\n",
            "2022-05-05T21:34:15.723303: step 8858, loss 1.31644, acc 0.484375\n",
            "2022-05-05T21:34:16.322694: step 8859, loss 1.23098, acc 0.5625\n",
            "2022-05-05T21:34:16.929356: step 8860, loss 1.40353, acc 0.5625\n",
            "2022-05-05T21:34:17.541600: step 8861, loss 1.45949, acc 0.53125\n",
            "2022-05-05T21:34:18.145577: step 8862, loss 1.42592, acc 0.5625\n",
            "2022-05-05T21:34:18.763417: step 8863, loss 1.41364, acc 0.515625\n",
            "2022-05-05T21:34:19.376304: step 8864, loss 1.24959, acc 0.625\n",
            "2022-05-05T21:34:19.988945: step 8865, loss 1.09359, acc 0.65625\n",
            "2022-05-05T21:34:20.601914: step 8866, loss 1.25187, acc 0.546875\n",
            "2022-05-05T21:34:21.201892: step 8867, loss 1.57692, acc 0.421875\n",
            "2022-05-05T21:34:21.821946: step 8868, loss 1.34384, acc 0.46875\n",
            "2022-05-05T21:34:22.430381: step 8869, loss 1.32215, acc 0.546875\n",
            "2022-05-05T21:34:23.050059: step 8870, loss 1.11961, acc 0.625\n",
            "2022-05-05T21:34:23.659777: step 8871, loss 1.42408, acc 0.515625\n",
            "2022-05-05T21:34:24.278379: step 8872, loss 1.33754, acc 0.53125\n",
            "2022-05-05T21:34:24.891104: step 8873, loss 1.23788, acc 0.625\n",
            "2022-05-05T21:34:25.493480: step 8874, loss 1.34746, acc 0.515625\n",
            "2022-05-05T21:34:26.114876: step 8875, loss 1.27135, acc 0.53125\n",
            "2022-05-05T21:34:26.729699: step 8876, loss 1.34175, acc 0.515625\n",
            "2022-05-05T21:34:27.340062: step 8877, loss 1.19145, acc 0.578125\n",
            "2022-05-05T21:34:27.953440: step 8878, loss 1.25619, acc 0.671875\n",
            "2022-05-05T21:34:28.553204: step 8879, loss 1.41791, acc 0.484375\n",
            "2022-05-05T21:34:29.165729: step 8880, loss 1.47104, acc 0.546875\n",
            "2022-05-05T21:34:29.772297: step 8881, loss 1.59611, acc 0.421875\n",
            "2022-05-05T21:34:30.382796: step 8882, loss 1.274, acc 0.515625\n",
            "2022-05-05T21:34:30.991449: step 8883, loss 1.462, acc 0.53125\n",
            "2022-05-05T21:34:31.585643: step 8884, loss 1.20416, acc 0.578125\n",
            "2022-05-05T21:34:32.198209: step 8885, loss 1.34295, acc 0.484375\n",
            "2022-05-05T21:34:32.796437: step 8886, loss 1.33103, acc 0.515625\n",
            "2022-05-05T21:34:33.413865: step 8887, loss 1.45133, acc 0.46875\n",
            "2022-05-05T21:34:34.038129: step 8888, loss 1.00206, acc 0.6875\n",
            "2022-05-05T21:34:34.646741: step 8889, loss 1.39712, acc 0.546875\n",
            "2022-05-05T21:34:35.257948: step 8890, loss 1.36329, acc 0.546875\n",
            "2022-05-05T21:34:35.861451: step 8891, loss 1.39028, acc 0.484375\n",
            "2022-05-05T21:34:36.469641: step 8892, loss 1.31136, acc 0.515625\n",
            "2022-05-05T21:34:37.082233: step 8893, loss 1.63931, acc 0.3125\n",
            "2022-05-05T21:34:37.695694: step 8894, loss 1.3033, acc 0.5625\n",
            "2022-05-05T21:34:38.321536: step 8895, loss 1.49355, acc 0.484375\n",
            "2022-05-05T21:34:38.930336: step 8896, loss 1.37253, acc 0.515625\n",
            "2022-05-05T21:34:39.549831: step 8897, loss 1.06911, acc 0.625\n",
            "2022-05-05T21:34:40.165694: step 8898, loss 1.32452, acc 0.515625\n",
            "2022-05-05T21:34:40.784891: step 8899, loss 1.30735, acc 0.5\n",
            "2022-05-05T21:34:41.405846: step 8900, loss 1.49963, acc 0.484375\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:34:41.548676: step 8900, loss 1.83374, acc 0.4\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-8900\n",
            "\n",
            "2022-05-05T21:34:42.318709: step 8901, loss 1.35954, acc 0.53125\n",
            "2022-05-05T21:34:42.928602: step 8902, loss 1.35295, acc 0.4375\n",
            "2022-05-05T21:34:43.542815: step 8903, loss 1.21425, acc 0.59375\n",
            "2022-05-05T21:34:44.156549: step 8904, loss 1.30875, acc 0.59375\n",
            "2022-05-05T21:34:44.826724: step 8905, loss 1.36986, acc 0.453125\n",
            "2022-05-05T21:34:45.438034: step 8906, loss 1.30665, acc 0.546875\n",
            "2022-05-05T21:34:46.048645: step 8907, loss 1.31054, acc 0.5\n",
            "2022-05-05T21:34:46.664252: step 8908, loss 1.28738, acc 0.53125\n",
            "2022-05-05T21:34:47.267456: step 8909, loss 1.38165, acc 0.546875\n",
            "2022-05-05T21:34:47.887483: step 8910, loss 1.37882, acc 0.53125\n",
            "2022-05-05T21:34:48.509845: step 8911, loss 1.39128, acc 0.515625\n",
            "2022-05-05T21:34:49.114356: step 8912, loss 1.14726, acc 0.578125\n",
            "2022-05-05T21:34:49.734966: step 8913, loss 1.34915, acc 0.5\n",
            "2022-05-05T21:34:50.329623: step 8914, loss 1.2911, acc 0.546875\n",
            "2022-05-05T21:34:50.946170: step 8915, loss 1.31505, acc 0.515625\n",
            "2022-05-05T21:34:51.565246: step 8916, loss 1.17584, acc 0.6875\n",
            "2022-05-05T21:34:52.175634: step 8917, loss 1.5563, acc 0.421875\n",
            "2022-05-05T21:34:52.794236: step 8918, loss 1.28071, acc 0.546875\n",
            "2022-05-05T21:34:53.391280: step 8919, loss 1.28142, acc 0.578125\n",
            "2022-05-05T21:34:54.006122: step 8920, loss 1.59573, acc 0.375\n",
            "2022-05-05T21:34:54.631272: step 8921, loss 1.30211, acc 0.5625\n",
            "2022-05-05T21:34:55.244680: step 8922, loss 1.1979, acc 0.59375\n",
            "2022-05-05T21:34:55.868713: step 8923, loss 1.28676, acc 0.5625\n",
            "2022-05-05T21:34:56.475473: step 8924, loss 1.12731, acc 0.640625\n",
            "2022-05-05T21:34:57.095762: step 8925, loss 1.09365, acc 0.671875\n",
            "2022-05-05T21:34:57.712051: step 8926, loss 1.38081, acc 0.578125\n",
            "2022-05-05T21:34:58.316184: step 8927, loss 1.25274, acc 0.546875\n",
            "2022-05-05T21:34:58.926538: step 8928, loss 1.31017, acc 0.5625\n",
            "2022-05-05T21:34:59.543557: step 8929, loss 1.39671, acc 0.546875\n",
            "2022-05-05T21:35:00.161488: step 8930, loss 1.4505, acc 0.5\n",
            "2022-05-05T21:35:00.773833: step 8931, loss 1.44162, acc 0.484375\n",
            "2022-05-05T21:35:01.385207: step 8932, loss 1.28062, acc 0.5\n",
            "2022-05-05T21:35:01.994904: step 8933, loss 1.10084, acc 0.625\n",
            "2022-05-05T21:35:02.596048: step 8934, loss 1.32863, acc 0.46875\n",
            "2022-05-05T21:35:03.216333: step 8935, loss 1.27636, acc 0.453125\n",
            "2022-05-05T21:35:03.812031: step 8936, loss 1.28818, acc 0.515625\n",
            "2022-05-05T21:35:04.420479: step 8937, loss 1.26608, acc 0.546875\n",
            "2022-05-05T21:35:05.049478: step 8938, loss 1.4152, acc 0.546875\n",
            "2022-05-05T21:35:05.655426: step 8939, loss 1.33869, acc 0.5\n",
            "2022-05-05T21:35:06.273284: step 8940, loss 1.28215, acc 0.53125\n",
            "2022-05-05T21:35:06.880346: step 8941, loss 1.28801, acc 0.53125\n",
            "2022-05-05T21:35:07.500160: step 8942, loss 1.48607, acc 0.484375\n",
            "2022-05-05T21:35:08.109039: step 8943, loss 1.48029, acc 0.421875\n",
            "2022-05-05T21:35:08.716168: step 8944, loss 1.29485, acc 0.546875\n",
            "2022-05-05T21:35:09.328845: step 8945, loss 1.36666, acc 0.453125\n",
            "2022-05-05T21:35:09.934617: step 8946, loss 1.30054, acc 0.5\n",
            "2022-05-05T21:35:10.549269: step 8947, loss 1.3021, acc 0.546875\n",
            "2022-05-05T21:35:11.164700: step 8948, loss 1.51376, acc 0.5\n",
            "2022-05-05T21:35:11.773814: step 8949, loss 1.40178, acc 0.515625\n",
            "2022-05-05T21:35:12.403939: step 8950, loss 1.31459, acc 0.515625\n",
            "2022-05-05T21:35:13.021130: step 8951, loss 1.04594, acc 0.671875\n",
            "2022-05-05T21:35:13.630369: step 8952, loss 1.32967, acc 0.5\n",
            "2022-05-05T21:35:14.244587: step 8953, loss 1.42218, acc 0.53125\n",
            "2022-05-05T21:35:14.848519: step 8954, loss 1.53474, acc 0.453125\n",
            "2022-05-05T21:35:15.487622: step 8955, loss 1.06454, acc 0.71875\n",
            "2022-05-05T21:35:16.090956: step 8956, loss 1.30882, acc 0.46875\n",
            "2022-05-05T21:35:16.704393: step 8957, loss 1.41902, acc 0.46875\n",
            "2022-05-05T21:35:17.313752: step 8958, loss 1.33745, acc 0.59375\n",
            "2022-05-05T21:35:17.914916: step 8959, loss 1.07035, acc 0.671875\n",
            "2022-05-05T21:35:18.544574: step 8960, loss 1.23094, acc 0.609375\n",
            "2022-05-05T21:35:19.150240: step 8961, loss 1.4204, acc 0.484375\n",
            "2022-05-05T21:35:19.768505: step 8962, loss 1.3337, acc 0.546875\n",
            "2022-05-05T21:35:20.382628: step 8963, loss 1.43257, acc 0.484375\n",
            "2022-05-05T21:35:20.974743: step 8964, loss 1.03726, acc 0.59375\n",
            "2022-05-05T21:35:21.583019: step 8965, loss 1.22368, acc 0.53125\n",
            "2022-05-05T21:35:22.183239: step 8966, loss 1.18232, acc 0.546875\n",
            "2022-05-05T21:35:22.801282: step 8967, loss 1.31164, acc 0.578125\n",
            "2022-05-05T21:35:23.414460: step 8968, loss 1.473, acc 0.453125\n",
            "2022-05-05T21:35:24.017745: step 8969, loss 1.36968, acc 0.53125\n",
            "2022-05-05T21:35:24.640050: step 8970, loss 1.41434, acc 0.578125\n",
            "2022-05-05T21:35:25.251735: step 8971, loss 1.41103, acc 0.515625\n",
            "2022-05-05T21:35:25.908049: step 8972, loss 1.14286, acc 0.5625\n",
            "2022-05-05T21:35:26.534209: step 8973, loss 1.41544, acc 0.453125\n",
            "2022-05-05T21:35:27.155193: step 8974, loss 1.36914, acc 0.453125\n",
            "2022-05-05T21:35:27.789368: step 8975, loss 1.39677, acc 0.515625\n",
            "2022-05-05T21:35:28.396701: step 8976, loss 1.74902, acc 0.375\n",
            "2022-05-05T21:35:29.015793: step 8977, loss 1.39175, acc 0.5625\n",
            "2022-05-05T21:35:29.644127: step 8978, loss 1.45653, acc 0.5\n",
            "2022-05-05T21:35:30.262332: step 8979, loss 1.40148, acc 0.453125\n",
            "2022-05-05T21:35:30.893907: step 8980, loss 1.21529, acc 0.53125\n",
            "2022-05-05T21:35:31.511250: step 8981, loss 1.49821, acc 0.421875\n",
            "2022-05-05T21:35:32.134575: step 8982, loss 1.39147, acc 0.515625\n",
            "2022-05-05T21:35:32.774233: step 8983, loss 1.36539, acc 0.5\n",
            "2022-05-05T21:35:33.389110: step 8984, loss 1.33099, acc 0.53125\n",
            "2022-05-05T21:35:34.013135: step 8985, loss 1.35756, acc 0.546875\n",
            "2022-05-05T21:35:34.631898: step 8986, loss 1.39973, acc 0.46875\n",
            "2022-05-05T21:35:35.254515: step 8987, loss 1.28209, acc 0.609375\n",
            "2022-05-05T21:35:35.870492: step 8988, loss 1.52804, acc 0.453125\n",
            "2022-05-05T21:35:36.502224: step 8989, loss 1.48361, acc 0.5\n",
            "2022-05-05T21:35:37.127877: step 8990, loss 1.41988, acc 0.5\n",
            "2022-05-05T21:35:37.739254: step 8991, loss 1.33834, acc 0.546875\n",
            "2022-05-05T21:35:38.367555: step 8992, loss 1.46931, acc 0.375\n",
            "2022-05-05T21:35:38.983233: step 8993, loss 1.38387, acc 0.421875\n",
            "2022-05-05T21:35:39.589339: step 8994, loss 1.21323, acc 0.53125\n",
            "2022-05-05T21:35:40.207869: step 8995, loss 1.28739, acc 0.625\n",
            "2022-05-05T21:35:40.822605: step 8996, loss 1.41356, acc 0.453125\n",
            "2022-05-05T21:35:41.440003: step 8997, loss 1.13336, acc 0.59375\n",
            "2022-05-05T21:35:42.048611: step 8998, loss 1.33719, acc 0.5\n",
            "2022-05-05T21:35:42.651363: step 8999, loss 1.39587, acc 0.453125\n",
            "2022-05-05T21:35:43.259211: step 9000, loss 1.34752, acc 0.640625\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:35:43.408194: step 9000, loss 1.84507, acc 0.38\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-9000\n",
            "\n",
            "2022-05-05T21:35:44.203029: step 9001, loss 1.30007, acc 0.5\n",
            "2022-05-05T21:35:44.813902: step 9002, loss 1.68012, acc 0.4375\n",
            "2022-05-05T21:35:45.430184: step 9003, loss 1.39492, acc 0.515625\n",
            "2022-05-05T21:35:46.039052: step 9004, loss 1.20318, acc 0.609375\n",
            "2022-05-05T21:35:46.709454: step 9005, loss 1.64391, acc 0.453125\n",
            "2022-05-05T21:35:47.330138: step 9006, loss 1.39079, acc 0.578125\n",
            "2022-05-05T21:35:47.940843: step 9007, loss 1.32186, acc 0.484375\n",
            "2022-05-05T21:35:48.567437: step 9008, loss 1.22004, acc 0.578125\n",
            "2022-05-05T21:35:49.172282: step 9009, loss 1.31664, acc 0.578125\n",
            "2022-05-05T21:35:49.787761: step 9010, loss 1.21196, acc 0.578125\n",
            "2022-05-05T21:35:50.413965: step 9011, loss 1.35757, acc 0.5\n",
            "2022-05-05T21:35:51.018485: step 9012, loss 1.4395, acc 0.484375\n",
            "2022-05-05T21:35:51.660945: step 9013, loss 1.37383, acc 0.5\n",
            "2022-05-05T21:35:52.263158: step 9014, loss 1.35439, acc 0.515625\n",
            "2022-05-05T21:35:52.874078: step 9015, loss 1.39542, acc 0.5\n",
            "2022-05-05T21:35:53.498795: step 9016, loss 1.3245, acc 0.546875\n",
            "2022-05-05T21:35:54.117981: step 9017, loss 1.37188, acc 0.515625\n",
            "2022-05-05T21:35:54.737926: step 9018, loss 1.2844, acc 0.5\n",
            "2022-05-05T21:35:55.343850: step 9019, loss 1.29679, acc 0.5\n",
            "2022-05-05T21:35:55.963389: step 9020, loss 1.18887, acc 0.640625\n",
            "2022-05-05T21:35:56.598377: step 9021, loss 1.49986, acc 0.5\n",
            "2022-05-05T21:35:57.211826: step 9022, loss 1.42658, acc 0.484375\n",
            "2022-05-05T21:35:57.831350: step 9023, loss 1.52834, acc 0.484375\n",
            "2022-05-05T21:35:58.445305: step 9024, loss 1.62572, acc 0.390625\n",
            "2022-05-05T21:35:59.065914: step 9025, loss 1.17329, acc 0.671875\n",
            "2022-05-05T21:35:59.683462: step 9026, loss 1.31841, acc 0.609375\n",
            "2022-05-05T21:36:00.296960: step 9027, loss 1.29838, acc 0.59375\n",
            "2022-05-05T21:36:00.914262: step 9028, loss 1.16783, acc 0.625\n",
            "2022-05-05T21:36:01.511692: step 9029, loss 1.36412, acc 0.484375\n",
            "2022-05-05T21:36:02.117978: step 9030, loss 1.43242, acc 0.46875\n",
            "2022-05-05T21:36:02.743686: step 9031, loss 1.37747, acc 0.5\n",
            "2022-05-05T21:36:03.362152: step 9032, loss 1.31256, acc 0.5625\n",
            "2022-05-05T21:36:03.974467: step 9033, loss 1.34396, acc 0.5625\n",
            "2022-05-05T21:36:04.581247: step 9034, loss 1.11252, acc 0.671875\n",
            "2022-05-05T21:36:05.200890: step 9035, loss 1.47607, acc 0.46875\n",
            "2022-05-05T21:36:05.823112: step 9036, loss 1.31709, acc 0.546875\n",
            "2022-05-05T21:36:06.434452: step 9037, loss 1.23695, acc 0.515625\n",
            "2022-05-05T21:36:07.077496: step 9038, loss 1.72466, acc 0.34375\n",
            "2022-05-05T21:36:07.682862: step 9039, loss 1.21923, acc 0.578125\n",
            "2022-05-05T21:36:08.303144: step 9040, loss 1.58545, acc 0.453125\n",
            "2022-05-05T21:36:08.923643: step 9041, loss 1.37162, acc 0.453125\n",
            "2022-05-05T21:36:09.541052: step 9042, loss 1.36022, acc 0.515625\n",
            "2022-05-05T21:36:10.158642: step 9043, loss 1.26397, acc 0.515625\n",
            "2022-05-05T21:36:10.768273: step 9044, loss 1.42233, acc 0.484375\n",
            "2022-05-05T21:36:11.377544: step 9045, loss 1.29668, acc 0.578125\n",
            "2022-05-05T21:36:11.982796: step 9046, loss 1.37192, acc 0.484375\n",
            "2022-05-05T21:36:12.591581: step 9047, loss 1.39215, acc 0.515625\n",
            "2022-05-05T21:36:13.213143: step 9048, loss 1.38761, acc 0.484375\n",
            "2022-05-05T21:36:13.811569: step 9049, loss 1.62961, acc 0.40625\n",
            "2022-05-05T21:36:14.441318: step 9050, loss 1.14376, acc 0.59375\n",
            "2022-05-05T21:36:15.052537: step 9051, loss 1.3445, acc 0.515625\n",
            "2022-05-05T21:36:15.661806: step 9052, loss 1.38263, acc 0.546875\n",
            "2022-05-05T21:36:16.271484: step 9053, loss 1.3519, acc 0.53125\n",
            "2022-05-05T21:36:16.879138: step 9054, loss 1.39469, acc 0.53125\n",
            "2022-05-05T21:36:17.533028: step 9055, loss 1.32021, acc 0.609375\n",
            "2022-05-05T21:36:18.140361: step 9056, loss 1.32586, acc 0.59375\n",
            "2022-05-05T21:36:18.735611: step 9057, loss 1.46166, acc 0.484375\n",
            "2022-05-05T21:36:19.361210: step 9058, loss 1.32272, acc 0.515625\n",
            "2022-05-05T21:36:19.966116: step 9059, loss 1.28591, acc 0.5625\n",
            "2022-05-05T21:36:20.575125: step 9060, loss 1.60378, acc 0.421875\n",
            "2022-05-05T21:36:21.183680: step 9061, loss 1.36711, acc 0.5\n",
            "2022-05-05T21:36:21.782191: step 9062, loss 1.23162, acc 0.5625\n",
            "2022-05-05T21:36:22.406303: step 9063, loss 1.29441, acc 0.59375\n",
            "2022-05-05T21:36:23.009592: step 9064, loss 1.34008, acc 0.578125\n",
            "2022-05-05T21:36:23.626370: step 9065, loss 1.278, acc 0.5625\n",
            "2022-05-05T21:36:24.227802: step 9066, loss 1.43103, acc 0.46875\n",
            "2022-05-05T21:36:24.850278: step 9067, loss 1.21058, acc 0.5\n",
            "2022-05-05T21:36:25.467028: step 9068, loss 1.15216, acc 0.6875\n",
            "2022-05-05T21:36:26.068210: step 9069, loss 1.36876, acc 0.53125\n",
            "2022-05-05T21:36:26.688831: step 9070, loss 1.44408, acc 0.46875\n",
            "2022-05-05T21:36:27.300280: step 9071, loss 1.43154, acc 0.46875\n",
            "2022-05-05T21:36:27.941334: step 9072, loss 1.27739, acc 0.578125\n",
            "2022-05-05T21:36:28.562820: step 9073, loss 1.33527, acc 0.515625\n",
            "2022-05-05T21:36:29.171395: step 9074, loss 1.5245, acc 0.4375\n",
            "2022-05-05T21:36:29.794161: step 9075, loss 1.41564, acc 0.53125\n",
            "2022-05-05T21:36:30.404692: step 9076, loss 1.25297, acc 0.59375\n",
            "2022-05-05T21:36:31.022931: step 9077, loss 1.43974, acc 0.546875\n",
            "2022-05-05T21:36:31.645868: step 9078, loss 1.42356, acc 0.46875\n",
            "2022-05-05T21:36:32.252792: step 9079, loss 1.49714, acc 0.390625\n",
            "2022-05-05T21:36:32.863244: step 9080, loss 1.36715, acc 0.5\n",
            "2022-05-05T21:36:33.458704: step 9081, loss 1.39808, acc 0.515625\n",
            "2022-05-05T21:36:34.069230: step 9082, loss 1.72545, acc 0.375\n",
            "2022-05-05T21:36:34.681097: step 9083, loss 1.33019, acc 0.578125\n",
            "2022-05-05T21:36:35.284037: step 9084, loss 1.11415, acc 0.609375\n",
            "2022-05-05T21:36:35.891691: step 9085, loss 1.45428, acc 0.578125\n",
            "2022-05-05T21:36:36.496526: step 9086, loss 1.24997, acc 0.546875\n",
            "2022-05-05T21:36:37.103284: step 9087, loss 1.44764, acc 0.484375\n",
            "2022-05-05T21:36:37.724891: step 9088, loss 1.33852, acc 0.5\n",
            "2022-05-05T21:36:38.346740: step 9089, loss 1.46486, acc 0.5\n",
            "2022-05-05T21:36:38.957301: step 9090, loss 1.34707, acc 0.5\n",
            "2022-05-05T21:36:39.549755: step 9091, loss 1.22648, acc 0.609375\n",
            "2022-05-05T21:36:40.177297: step 9092, loss 1.4242, acc 0.4375\n",
            "2022-05-05T21:36:40.789403: step 9093, loss 1.13673, acc 0.609375\n",
            "2022-05-05T21:36:41.400257: step 9094, loss 1.35139, acc 0.546875\n",
            "2022-05-05T21:36:42.008250: step 9095, loss 1.34267, acc 0.53125\n",
            "2022-05-05T21:36:42.609841: step 9096, loss 1.50828, acc 0.484375\n",
            "2022-05-05T21:36:43.216492: step 9097, loss 1.42632, acc 0.5\n",
            "2022-05-05T21:36:43.812642: step 9098, loss 1.56745, acc 0.453125\n",
            "2022-05-05T21:36:44.427444: step 9099, loss 1.40874, acc 0.5\n",
            "2022-05-05T21:36:45.037303: step 9100, loss 1.42495, acc 0.546875\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:36:45.178348: step 9100, loss 2.10687, acc 0.26\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-9100\n",
            "\n",
            "2022-05-05T21:36:45.959812: step 9101, loss 1.34022, acc 0.515625\n",
            "2022-05-05T21:36:46.561362: step 9102, loss 1.50407, acc 0.484375\n",
            "2022-05-05T21:36:47.176014: step 9103, loss 1.38502, acc 0.5\n",
            "2022-05-05T21:36:47.773424: step 9104, loss 1.34769, acc 0.5\n",
            "2022-05-05T21:36:48.414223: step 9105, loss 1.38178, acc 0.4375\n",
            "2022-05-05T21:36:49.013608: step 9106, loss 1.37601, acc 0.4375\n",
            "2022-05-05T21:36:49.624105: step 9107, loss 1.33613, acc 0.515625\n",
            "2022-05-05T21:36:50.237661: step 9108, loss 1.06751, acc 0.625\n",
            "2022-05-05T21:36:50.828099: step 9109, loss 1.34118, acc 0.515625\n",
            "2022-05-05T21:36:51.441137: step 9110, loss 1.34618, acc 0.59375\n",
            "2022-05-05T21:36:52.038533: step 9111, loss 1.15537, acc 0.578125\n",
            "2022-05-05T21:36:52.649155: step 9112, loss 1.28085, acc 0.53125\n",
            "2022-05-05T21:36:53.255405: step 9113, loss 1.41212, acc 0.46875\n",
            "2022-05-05T21:36:53.896852: step 9114, loss 1.36495, acc 0.5\n",
            "2022-05-05T21:36:54.513984: step 9115, loss 1.49038, acc 0.453125\n",
            "2022-05-05T21:36:55.109735: step 9116, loss 1.41861, acc 0.578125\n",
            "2022-05-05T21:36:55.724668: step 9117, loss 1.47074, acc 0.453125\n",
            "2022-05-05T21:36:56.337595: step 9118, loss 1.36577, acc 0.515625\n",
            "2022-05-05T21:36:56.928831: step 9119, loss 1.25525, acc 0.578125\n",
            "2022-05-05T21:36:57.550519: step 9120, loss 1.38201, acc 0.46875\n",
            "2022-05-05T21:36:58.139609: step 9121, loss 1.32194, acc 0.53125\n",
            "2022-05-05T21:36:58.792688: step 9122, loss 1.52413, acc 0.375\n",
            "2022-05-05T21:36:59.400674: step 9123, loss 1.3591, acc 0.53125\n",
            "2022-05-05T21:36:59.999400: step 9124, loss 1.39633, acc 0.53125\n",
            "2022-05-05T21:37:00.633429: step 9125, loss 1.4924, acc 0.453125\n",
            "2022-05-05T21:37:01.225329: step 9126, loss 1.39968, acc 0.46875\n",
            "2022-05-05T21:37:01.839217: step 9127, loss 1.26124, acc 0.609375\n",
            "2022-05-05T21:37:02.438816: step 9128, loss 1.41625, acc 0.515625\n",
            "2022-05-05T21:37:03.035617: step 9129, loss 1.44922, acc 0.4375\n",
            "2022-05-05T21:37:03.644678: step 9130, loss 1.73083, acc 0.375\n",
            "2022-05-05T21:37:04.242649: step 9131, loss 1.28359, acc 0.59375\n",
            "2022-05-05T21:37:04.863367: step 9132, loss 1.12206, acc 0.53125\n",
            "2022-05-05T21:37:05.467294: step 9133, loss 1.36475, acc 0.5\n",
            "2022-05-05T21:37:06.088585: step 9134, loss 1.46499, acc 0.484375\n",
            "2022-05-05T21:37:06.697696: step 9135, loss 1.40594, acc 0.453125\n",
            "2022-05-05T21:37:07.293881: step 9136, loss 1.48347, acc 0.46875\n",
            "2022-05-05T21:37:07.901264: step 9137, loss 1.23971, acc 0.578125\n",
            "2022-05-05T21:37:08.500246: step 9138, loss 1.23242, acc 0.546875\n",
            "2022-05-05T21:37:09.136947: step 9139, loss 1.26324, acc 0.5625\n",
            "2022-05-05T21:37:09.744162: step 9140, loss 1.34261, acc 0.53125\n",
            "2022-05-05T21:37:10.335766: step 9141, loss 1.46664, acc 0.4375\n",
            "2022-05-05T21:37:10.951622: step 9142, loss 1.2766, acc 0.5\n",
            "2022-05-05T21:37:11.554728: step 9143, loss 1.22577, acc 0.640625\n",
            "2022-05-05T21:37:12.172783: step 9144, loss 1.39351, acc 0.46875\n",
            "2022-05-05T21:37:12.775423: step 9145, loss 1.31379, acc 0.546875\n",
            "2022-05-05T21:37:13.377603: step 9146, loss 1.58632, acc 0.421875\n",
            "2022-05-05T21:37:14.009683: step 9147, loss 1.40904, acc 0.4375\n",
            "2022-05-05T21:37:14.613079: step 9148, loss 1.40922, acc 0.515625\n",
            "2022-05-05T21:37:15.230156: step 9149, loss 1.42089, acc 0.453125\n",
            "2022-05-05T21:37:15.837547: step 9150, loss 1.41851, acc 0.53125\n",
            "2022-05-05T21:37:16.432158: step 9151, loss 1.27116, acc 0.546875\n",
            "2022-05-05T21:37:17.054488: step 9152, loss 1.38159, acc 0.5\n",
            "2022-05-05T21:37:17.659299: step 9153, loss 1.28726, acc 0.53125\n",
            "2022-05-05T21:37:18.273124: step 9154, loss 1.19109, acc 0.65625\n",
            "2022-05-05T21:37:18.880041: step 9155, loss 1.45092, acc 0.53125\n",
            "2022-05-05T21:37:19.526099: step 9156, loss 1.43223, acc 0.53125\n",
            "2022-05-05T21:37:20.149448: step 9157, loss 1.50464, acc 0.484375\n",
            "2022-05-05T21:37:20.760632: step 9158, loss 1.41095, acc 0.53125\n",
            "2022-05-05T21:37:21.379683: step 9159, loss 1.386, acc 0.4375\n",
            "2022-05-05T21:37:21.979532: step 9160, loss 1.38582, acc 0.53125\n",
            "2022-05-05T21:37:22.596420: step 9161, loss 1.48976, acc 0.46875\n",
            "2022-05-05T21:37:23.217616: step 9162, loss 1.4961, acc 0.5\n",
            "2022-05-05T21:37:23.816929: step 9163, loss 1.2833, acc 0.578125\n",
            "2022-05-05T21:37:24.441848: step 9164, loss 1.18125, acc 0.59375\n",
            "2022-05-05T21:37:25.035742: step 9165, loss 1.12381, acc 0.59375\n",
            "2022-05-05T21:37:25.656192: step 9166, loss 1.43426, acc 0.421875\n",
            "2022-05-05T21:37:26.269706: step 9167, loss 1.26584, acc 0.53125\n",
            "2022-05-05T21:37:26.882505: step 9168, loss 1.2738, acc 0.515625\n",
            "2022-05-05T21:37:27.499450: step 9169, loss 1.30164, acc 0.609375\n",
            "2022-05-05T21:37:28.098097: step 9170, loss 1.39901, acc 0.484375\n",
            "2022-05-05T21:37:28.724549: step 9171, loss 1.54665, acc 0.390625\n",
            "2022-05-05T21:37:29.342581: step 9172, loss 1.30329, acc 0.515625\n",
            "2022-05-05T21:37:29.974870: step 9173, loss 1.20468, acc 0.5625\n",
            "2022-05-05T21:37:30.590748: step 9174, loss 1.48864, acc 0.46875\n",
            "2022-05-05T21:37:31.198341: step 9175, loss 1.27766, acc 0.5625\n",
            "2022-05-05T21:37:31.821814: step 9176, loss 1.27843, acc 0.46875\n",
            "2022-05-05T21:37:32.436843: step 9177, loss 1.32527, acc 0.546875\n",
            "2022-05-05T21:37:33.042243: step 9178, loss 1.34116, acc 0.5\n",
            "2022-05-05T21:37:33.652803: step 9179, loss 1.44186, acc 0.5\n",
            "2022-05-05T21:37:34.254929: step 9180, loss 1.33063, acc 0.515625\n",
            "2022-05-05T21:37:34.865867: step 9181, loss 1.25449, acc 0.546875\n",
            "2022-05-05T21:37:35.479507: step 9182, loss 1.23405, acc 0.578125\n",
            "2022-05-05T21:37:36.077841: step 9183, loss 1.56634, acc 0.453125\n",
            "2022-05-05T21:37:36.698527: step 9184, loss 1.39841, acc 0.578125\n",
            "2022-05-05T21:37:37.304023: step 9185, loss 1.55955, acc 0.4375\n",
            "2022-05-05T21:37:37.913438: step 9186, loss 1.35116, acc 0.53125\n",
            "2022-05-05T21:37:38.523283: step 9187, loss 1.48758, acc 0.515625\n",
            "2022-05-05T21:37:39.135523: step 9188, loss 1.1697, acc 0.625\n",
            "2022-05-05T21:37:39.740938: step 9189, loss 1.12998, acc 0.578125\n",
            "2022-05-05T21:37:40.372036: step 9190, loss 1.59769, acc 0.515625\n",
            "2022-05-05T21:37:40.993604: step 9191, loss 1.31385, acc 0.53125\n",
            "2022-05-05T21:37:41.614917: step 9192, loss 1.24697, acc 0.609375\n",
            "2022-05-05T21:37:42.228370: step 9193, loss 1.27714, acc 0.59375\n",
            "2022-05-05T21:37:42.833690: step 9194, loss 1.24374, acc 0.59375\n",
            "2022-05-05T21:37:43.446818: step 9195, loss 1.31234, acc 0.5625\n",
            "2022-05-05T21:37:44.057884: step 9196, loss 1.52383, acc 0.5\n",
            "2022-05-05T21:37:44.696963: step 9197, loss 1.3621, acc 0.484375\n",
            "2022-05-05T21:37:45.318018: step 9198, loss 1.24346, acc 0.609375\n",
            "2022-05-05T21:37:45.931804: step 9199, loss 1.53752, acc 0.5\n",
            "2022-05-05T21:37:46.532678: step 9200, loss 1.26, acc 0.5625\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:37:46.679522: step 9200, loss 2.06072, acc 0.28\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-9200\n",
            "\n",
            "2022-05-05T21:37:47.475456: step 9201, loss 1.34731, acc 0.578125\n",
            "2022-05-05T21:37:48.089425: step 9202, loss 1.3328, acc 0.390625\n",
            "2022-05-05T21:37:48.690171: step 9203, loss 1.2726, acc 0.546875\n",
            "2022-05-05T21:37:49.300157: step 9204, loss 1.46583, acc 0.515625\n",
            "2022-05-05T21:37:49.902533: step 9205, loss 1.41542, acc 0.453125\n",
            "2022-05-05T21:37:50.532256: step 9206, loss 1.65105, acc 0.46875\n",
            "2022-05-05T21:37:51.139342: step 9207, loss 1.25857, acc 0.546875\n",
            "2022-05-05T21:37:51.739896: step 9208, loss 1.44433, acc 0.4375\n",
            "2022-05-05T21:37:52.367782: step 9209, loss 1.3561, acc 0.453125\n",
            "2022-05-05T21:37:52.959132: step 9210, loss 1.16698, acc 0.59375\n",
            "2022-05-05T21:37:53.570585: step 9211, loss 1.34225, acc 0.515625\n",
            "2022-05-05T21:37:54.188931: step 9212, loss 1.49467, acc 0.5\n",
            "2022-05-05T21:37:54.801615: step 9213, loss 1.3074, acc 0.5\n",
            "2022-05-05T21:37:55.423187: step 9214, loss 1.17613, acc 0.578125\n",
            "2022-05-05T21:37:56.027422: step 9215, loss 1.50736, acc 0.484375\n",
            "2022-05-05T21:37:56.652283: step 9216, loss 1.35086, acc 0.5\n",
            "2022-05-05T21:37:57.267884: step 9217, loss 1.4057, acc 0.421875\n",
            "2022-05-05T21:37:57.869918: step 9218, loss 1.30342, acc 0.484375\n",
            "2022-05-05T21:37:58.486708: step 9219, loss 1.1701, acc 0.59375\n",
            "2022-05-05T21:37:59.090331: step 9220, loss 1.31085, acc 0.5625\n",
            "2022-05-05T21:37:59.705293: step 9221, loss 1.25641, acc 0.5625\n",
            "2022-05-05T21:38:00.329885: step 9222, loss 1.2272, acc 0.609375\n",
            "2022-05-05T21:38:00.990019: step 9223, loss 1.19154, acc 0.59375\n",
            "2022-05-05T21:38:01.611675: step 9224, loss 1.41898, acc 0.4375\n",
            "2022-05-05T21:38:02.219396: step 9225, loss 1.4186, acc 0.34375\n",
            "2022-05-05T21:38:02.834669: step 9226, loss 1.4532, acc 0.453125\n",
            "2022-05-05T21:38:03.456652: step 9227, loss 1.35782, acc 0.484375\n",
            "2022-05-05T21:38:04.060789: step 9228, loss 1.29763, acc 0.53125\n",
            "2022-05-05T21:38:04.677562: step 9229, loss 1.37217, acc 0.484375\n",
            "2022-05-05T21:38:05.276935: step 9230, loss 1.27827, acc 0.53125\n",
            "2022-05-05T21:38:05.911898: step 9231, loss 1.45582, acc 0.5\n",
            "2022-05-05T21:38:06.529930: step 9232, loss 1.27495, acc 0.546875\n",
            "2022-05-05T21:38:07.135487: step 9233, loss 1.45682, acc 0.453125\n",
            "2022-05-05T21:38:07.747852: step 9234, loss 1.22558, acc 0.59375\n",
            "2022-05-05T21:38:08.353485: step 9235, loss 1.4828, acc 0.5\n",
            "2022-05-05T21:38:08.970849: step 9236, loss 1.26207, acc 0.59375\n",
            "2022-05-05T21:38:09.588724: step 9237, loss 1.33189, acc 0.5\n",
            "2022-05-05T21:38:10.203810: step 9238, loss 1.61115, acc 0.34375\n",
            "2022-05-05T21:38:10.842813: step 9239, loss 1.10128, acc 0.6875\n",
            "2022-05-05T21:38:11.452123: step 9240, loss 1.52526, acc 0.4375\n",
            "2022-05-05T21:38:12.064865: step 9241, loss 1.45228, acc 0.46875\n",
            "2022-05-05T21:38:12.680212: step 9242, loss 1.51885, acc 0.46875\n",
            "2022-05-05T21:38:13.279925: step 9243, loss 1.27299, acc 0.546875\n",
            "2022-05-05T21:38:13.900639: step 9244, loss 1.27039, acc 0.515625\n",
            "2022-05-05T21:38:14.507862: step 9245, loss 1.40236, acc 0.453125\n",
            "2022-05-05T21:38:15.121469: step 9246, loss 1.30747, acc 0.515625\n",
            "2022-05-05T21:38:15.732274: step 9247, loss 1.44332, acc 0.578125\n",
            "2022-05-05T21:38:16.220880: step 9248, loss 1.74637, acc 0.428571\n",
            "2022-05-05T21:38:16.837248: step 9249, loss 1.30249, acc 0.59375\n",
            "2022-05-05T21:38:17.434577: step 9250, loss 1.36047, acc 0.546875\n",
            "2022-05-05T21:38:18.036913: step 9251, loss 1.33394, acc 0.5\n",
            "2022-05-05T21:38:18.636089: step 9252, loss 1.24079, acc 0.59375\n",
            "2022-05-05T21:38:19.245122: step 9253, loss 1.16712, acc 0.5625\n",
            "2022-05-05T21:38:19.859628: step 9254, loss 1.12682, acc 0.65625\n",
            "2022-05-05T21:38:20.456462: step 9255, loss 1.00249, acc 0.671875\n",
            "2022-05-05T21:38:21.073276: step 9256, loss 1.22559, acc 0.5625\n",
            "2022-05-05T21:38:21.689827: step 9257, loss 1.06528, acc 0.578125\n",
            "2022-05-05T21:38:22.292048: step 9258, loss 1.36179, acc 0.53125\n",
            "2022-05-05T21:38:22.893965: step 9259, loss 1.21048, acc 0.53125\n",
            "2022-05-05T21:38:23.505882: step 9260, loss 1.04783, acc 0.640625\n",
            "2022-05-05T21:38:24.118168: step 9261, loss 0.994227, acc 0.703125\n",
            "2022-05-05T21:38:24.733214: step 9262, loss 1.16531, acc 0.65625\n",
            "2022-05-05T21:38:25.338123: step 9263, loss 1.28836, acc 0.640625\n",
            "2022-05-05T21:38:25.941368: step 9264, loss 1.35248, acc 0.515625\n",
            "2022-05-05T21:38:26.552458: step 9265, loss 1.32279, acc 0.515625\n",
            "2022-05-05T21:38:27.168689: step 9266, loss 1.34533, acc 0.5625\n",
            "2022-05-05T21:38:27.765899: step 9267, loss 1.22005, acc 0.625\n",
            "2022-05-05T21:38:28.378030: step 9268, loss 1.24069, acc 0.609375\n",
            "2022-05-05T21:38:28.975204: step 9269, loss 1.22151, acc 0.671875\n",
            "2022-05-05T21:38:29.584171: step 9270, loss 1.33283, acc 0.546875\n",
            "2022-05-05T21:38:30.194052: step 9271, loss 1.22113, acc 0.59375\n",
            "2022-05-05T21:38:30.798473: step 9272, loss 1.21729, acc 0.578125\n",
            "2022-05-05T21:38:31.424866: step 9273, loss 1.32745, acc 0.53125\n",
            "2022-05-05T21:38:32.029458: step 9274, loss 1.15628, acc 0.609375\n",
            "2022-05-05T21:38:32.650713: step 9275, loss 1.07855, acc 0.640625\n",
            "2022-05-05T21:38:33.262655: step 9276, loss 1.54417, acc 0.421875\n",
            "2022-05-05T21:38:33.866129: step 9277, loss 1.20788, acc 0.609375\n",
            "2022-05-05T21:38:34.509716: step 9278, loss 1.19112, acc 0.65625\n",
            "2022-05-05T21:38:35.127114: step 9279, loss 1.12825, acc 0.578125\n",
            "2022-05-05T21:38:35.753862: step 9280, loss 1.16611, acc 0.625\n",
            "2022-05-05T21:38:36.364287: step 9281, loss 1.38701, acc 0.53125\n",
            "2022-05-05T21:38:36.967113: step 9282, loss 1.41396, acc 0.53125\n",
            "2022-05-05T21:38:37.580909: step 9283, loss 1.13699, acc 0.6875\n",
            "2022-05-05T21:38:38.180465: step 9284, loss 1.24477, acc 0.578125\n",
            "2022-05-05T21:38:38.790861: step 9285, loss 1.15766, acc 0.578125\n",
            "2022-05-05T21:38:39.401219: step 9286, loss 1.19864, acc 0.625\n",
            "2022-05-05T21:38:39.989521: step 9287, loss 1.15406, acc 0.609375\n",
            "2022-05-05T21:38:40.600410: step 9288, loss 1.32339, acc 0.5\n",
            "2022-05-05T21:38:41.189061: step 9289, loss 1.28197, acc 0.5\n",
            "2022-05-05T21:38:41.802305: step 9290, loss 1.05237, acc 0.609375\n",
            "2022-05-05T21:38:42.427926: step 9291, loss 1.47823, acc 0.4375\n",
            "2022-05-05T21:38:43.041861: step 9292, loss 1.21343, acc 0.578125\n",
            "2022-05-05T21:38:43.653216: step 9293, loss 1.43059, acc 0.5\n",
            "2022-05-05T21:38:44.253159: step 9294, loss 1.10037, acc 0.6875\n",
            "2022-05-05T21:38:44.867332: step 9295, loss 1.39803, acc 0.546875\n",
            "2022-05-05T21:38:45.466466: step 9296, loss 1.19981, acc 0.5625\n",
            "2022-05-05T21:38:46.083129: step 9297, loss 1.23171, acc 0.578125\n",
            "2022-05-05T21:38:46.699607: step 9298, loss 1.10716, acc 0.609375\n",
            "2022-05-05T21:38:47.294433: step 9299, loss 1.55555, acc 0.4375\n",
            "2022-05-05T21:38:47.911329: step 9300, loss 1.30375, acc 0.515625\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:38:48.055030: step 9300, loss 1.62789, acc 0.5\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-9300\n",
            "\n",
            "2022-05-05T21:38:48.861586: step 9301, loss 1.38968, acc 0.484375\n",
            "2022-05-05T21:38:49.463508: step 9302, loss 1.06673, acc 0.625\n",
            "2022-05-05T21:38:50.077537: step 9303, loss 1.32749, acc 0.515625\n",
            "2022-05-05T21:38:50.682320: step 9304, loss 1.07866, acc 0.71875\n",
            "2022-05-05T21:38:51.305248: step 9305, loss 1.16318, acc 0.609375\n",
            "2022-05-05T21:38:51.920082: step 9306, loss 1.23226, acc 0.5625\n",
            "2022-05-05T21:38:52.561159: step 9307, loss 1.3212, acc 0.53125\n",
            "2022-05-05T21:38:53.168615: step 9308, loss 1.47182, acc 0.421875\n",
            "2022-05-05T21:38:53.768222: step 9309, loss 1.15468, acc 0.609375\n",
            "2022-05-05T21:38:54.377185: step 9310, loss 1.16908, acc 0.59375\n",
            "2022-05-05T21:38:54.991840: step 9311, loss 1.32234, acc 0.5625\n",
            "2022-05-05T21:38:55.588807: step 9312, loss 1.44167, acc 0.53125\n",
            "2022-05-05T21:38:56.195460: step 9313, loss 1.28762, acc 0.609375\n",
            "2022-05-05T21:38:56.795763: step 9314, loss 1.35202, acc 0.515625\n",
            "2022-05-05T21:38:57.415754: step 9315, loss 1.38583, acc 0.515625\n",
            "2022-05-05T21:38:58.032714: step 9316, loss 1.51479, acc 0.484375\n",
            "2022-05-05T21:38:58.638437: step 9317, loss 1.25961, acc 0.546875\n",
            "2022-05-05T21:38:59.248817: step 9318, loss 1.14025, acc 0.578125\n",
            "2022-05-05T21:38:59.855008: step 9319, loss 1.27873, acc 0.53125\n",
            "2022-05-05T21:39:00.492215: step 9320, loss 1.239, acc 0.59375\n",
            "2022-05-05T21:39:01.101439: step 9321, loss 1.2667, acc 0.53125\n",
            "2022-05-05T21:39:01.702230: step 9322, loss 1.42673, acc 0.53125\n",
            "2022-05-05T21:39:02.312946: step 9323, loss 1.13005, acc 0.609375\n",
            "2022-05-05T21:39:02.946482: step 9324, loss 1.282, acc 0.5\n",
            "2022-05-05T21:39:03.555171: step 9325, loss 1.33057, acc 0.5625\n",
            "2022-05-05T21:39:04.162273: step 9326, loss 1.23085, acc 0.578125\n",
            "2022-05-05T21:39:04.762247: step 9327, loss 1.17417, acc 0.546875\n",
            "2022-05-05T21:39:05.377238: step 9328, loss 1.39957, acc 0.546875\n",
            "2022-05-05T21:39:05.976136: step 9329, loss 1.29967, acc 0.515625\n",
            "2022-05-05T21:39:06.579194: step 9330, loss 1.3743, acc 0.59375\n",
            "2022-05-05T21:39:07.175586: step 9331, loss 1.01344, acc 0.6875\n",
            "2022-05-05T21:39:07.792829: step 9332, loss 1.31124, acc 0.53125\n",
            "2022-05-05T21:39:08.404622: step 9333, loss 1.38471, acc 0.484375\n",
            "2022-05-05T21:39:09.004375: step 9334, loss 1.13503, acc 0.578125\n",
            "2022-05-05T21:39:09.611302: step 9335, loss 1.30554, acc 0.484375\n",
            "2022-05-05T21:39:10.219089: step 9336, loss 1.09374, acc 0.609375\n",
            "2022-05-05T21:39:10.838834: step 9337, loss 1.3977, acc 0.546875\n",
            "2022-05-05T21:39:11.453105: step 9338, loss 1.13414, acc 0.59375\n",
            "2022-05-05T21:39:12.050359: step 9339, loss 1.24151, acc 0.546875\n",
            "2022-05-05T21:39:12.665432: step 9340, loss 1.11574, acc 0.640625\n",
            "2022-05-05T21:39:13.288866: step 9341, loss 1.23084, acc 0.578125\n",
            "2022-05-05T21:39:13.902467: step 9342, loss 1.29816, acc 0.53125\n",
            "2022-05-05T21:39:14.510855: step 9343, loss 1.25752, acc 0.5625\n",
            "2022-05-05T21:39:15.115553: step 9344, loss 1.17271, acc 0.625\n",
            "2022-05-05T21:39:15.736675: step 9345, loss 1.29431, acc 0.5625\n",
            "2022-05-05T21:39:16.330423: step 9346, loss 1.11762, acc 0.65625\n",
            "2022-05-05T21:39:16.943245: step 9347, loss 1.18183, acc 0.609375\n",
            "2022-05-05T21:39:17.544687: step 9348, loss 1.23309, acc 0.59375\n",
            "2022-05-05T21:39:18.140109: step 9349, loss 1.45652, acc 0.515625\n",
            "2022-05-05T21:39:18.750611: step 9350, loss 1.41514, acc 0.546875\n",
            "2022-05-05T21:39:19.352634: step 9351, loss 1.27777, acc 0.5625\n",
            "2022-05-05T21:39:19.965399: step 9352, loss 1.45405, acc 0.4375\n",
            "2022-05-05T21:39:20.574214: step 9353, loss 1.27448, acc 0.5\n",
            "2022-05-05T21:39:21.183129: step 9354, loss 1.27276, acc 0.53125\n",
            "2022-05-05T21:39:21.790078: step 9355, loss 1.46442, acc 0.5\n",
            "2022-05-05T21:39:22.388378: step 9356, loss 1.54348, acc 0.46875\n",
            "2022-05-05T21:39:22.989342: step 9357, loss 1.26725, acc 0.484375\n",
            "2022-05-05T21:39:23.611899: step 9358, loss 1.47823, acc 0.453125\n",
            "2022-05-05T21:39:24.226479: step 9359, loss 1.35244, acc 0.59375\n",
            "2022-05-05T21:39:24.838575: step 9360, loss 1.43803, acc 0.484375\n",
            "2022-05-05T21:39:25.443794: step 9361, loss 1.15807, acc 0.59375\n",
            "2022-05-05T21:39:26.069347: step 9362, loss 1.30043, acc 0.5\n",
            "2022-05-05T21:39:26.676322: step 9363, loss 1.23209, acc 0.546875\n",
            "2022-05-05T21:39:27.283115: step 9364, loss 1.31597, acc 0.546875\n",
            "2022-05-05T21:39:27.900536: step 9365, loss 1.29933, acc 0.546875\n",
            "2022-05-05T21:39:28.506383: step 9366, loss 1.17, acc 0.5625\n",
            "2022-05-05T21:39:29.111920: step 9367, loss 1.18472, acc 0.5\n",
            "2022-05-05T21:39:29.720585: step 9368, loss 1.27473, acc 0.515625\n",
            "2022-05-05T21:39:30.327726: step 9369, loss 1.41115, acc 0.53125\n",
            "2022-05-05T21:39:30.947463: step 9370, loss 1.29177, acc 0.578125\n",
            "2022-05-05T21:39:31.550025: step 9371, loss 1.29297, acc 0.5625\n",
            "2022-05-05T21:39:32.157718: step 9372, loss 1.29204, acc 0.5625\n",
            "2022-05-05T21:39:32.756639: step 9373, loss 1.21927, acc 0.5\n",
            "2022-05-05T21:39:33.364172: step 9374, loss 1.36225, acc 0.515625\n",
            "2022-05-05T21:39:33.989831: step 9375, loss 1.10303, acc 0.65625\n",
            "2022-05-05T21:39:34.597190: step 9376, loss 1.09066, acc 0.65625\n",
            "2022-05-05T21:39:35.204117: step 9377, loss 1.18449, acc 0.53125\n",
            "2022-05-05T21:39:35.798226: step 9378, loss 1.16102, acc 0.5625\n",
            "2022-05-05T21:39:36.408556: step 9379, loss 1.42524, acc 0.515625\n",
            "2022-05-05T21:39:37.000578: step 9380, loss 1.38266, acc 0.515625\n",
            "2022-05-05T21:39:37.614894: step 9381, loss 1.26722, acc 0.578125\n",
            "2022-05-05T21:39:38.221622: step 9382, loss 1.1447, acc 0.625\n",
            "2022-05-05T21:39:38.811565: step 9383, loss 1.25778, acc 0.5625\n",
            "2022-05-05T21:39:39.414852: step 9384, loss 1.08742, acc 0.59375\n",
            "2022-05-05T21:39:40.014237: step 9385, loss 1.30794, acc 0.484375\n",
            "2022-05-05T21:39:40.623328: step 9386, loss 1.23788, acc 0.546875\n",
            "2022-05-05T21:39:41.218849: step 9387, loss 1.33203, acc 0.5625\n",
            "2022-05-05T21:39:41.827296: step 9388, loss 1.37645, acc 0.53125\n",
            "2022-05-05T21:39:42.439251: step 9389, loss 1.24886, acc 0.59375\n",
            "2022-05-05T21:39:43.039331: step 9390, loss 1.21463, acc 0.5625\n",
            "2022-05-05T21:39:43.658854: step 9391, loss 1.30425, acc 0.5625\n",
            "2022-05-05T21:39:44.270983: step 9392, loss 1.33814, acc 0.453125\n",
            "2022-05-05T21:39:44.886043: step 9393, loss 1.1161, acc 0.625\n",
            "2022-05-05T21:39:45.494520: step 9394, loss 1.64204, acc 0.40625\n",
            "2022-05-05T21:39:46.105233: step 9395, loss 1.2627, acc 0.5625\n",
            "2022-05-05T21:39:46.715761: step 9396, loss 1.38062, acc 0.546875\n",
            "2022-05-05T21:39:47.319886: step 9397, loss 1.29782, acc 0.59375\n",
            "2022-05-05T21:39:47.927056: step 9398, loss 1.30599, acc 0.578125\n",
            "2022-05-05T21:39:48.538948: step 9399, loss 1.25201, acc 0.578125\n",
            "2022-05-05T21:39:49.137692: step 9400, loss 1.11455, acc 0.65625\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:39:49.277946: step 9400, loss 1.91879, acc 0.36\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-9400\n",
            "\n",
            "2022-05-05T21:39:50.060564: step 9401, loss 1.23592, acc 0.578125\n",
            "2022-05-05T21:39:50.670957: step 9402, loss 1.18595, acc 0.609375\n",
            "2022-05-05T21:39:51.282620: step 9403, loss 1.37921, acc 0.5\n",
            "2022-05-05T21:39:51.900397: step 9404, loss 1.1807, acc 0.609375\n",
            "2022-05-05T21:39:52.499593: step 9405, loss 1.26937, acc 0.5625\n",
            "2022-05-05T21:39:53.112636: step 9406, loss 1.56259, acc 0.40625\n",
            "2022-05-05T21:39:53.723633: step 9407, loss 1.2396, acc 0.53125\n",
            "2022-05-05T21:39:54.344532: step 9408, loss 1.16492, acc 0.578125\n",
            "2022-05-05T21:39:54.955698: step 9409, loss 1.1148, acc 0.625\n",
            "2022-05-05T21:39:55.553436: step 9410, loss 1.15846, acc 0.59375\n",
            "2022-05-05T21:39:56.167800: step 9411, loss 1.34448, acc 0.5625\n",
            "2022-05-05T21:39:56.781870: step 9412, loss 1.47834, acc 0.484375\n",
            "2022-05-05T21:39:57.377395: step 9413, loss 1.20383, acc 0.515625\n",
            "2022-05-05T21:39:57.984896: step 9414, loss 1.2305, acc 0.5625\n",
            "2022-05-05T21:39:58.596359: step 9415, loss 1.39544, acc 0.5625\n",
            "2022-05-05T21:39:59.222655: step 9416, loss 1.39753, acc 0.578125\n",
            "2022-05-05T21:39:59.835286: step 9417, loss 1.20023, acc 0.640625\n",
            "2022-05-05T21:40:00.434605: step 9418, loss 1.28424, acc 0.546875\n",
            "2022-05-05T21:40:01.058038: step 9419, loss 1.27754, acc 0.53125\n",
            "2022-05-05T21:40:01.663160: step 9420, loss 1.2628, acc 0.53125\n",
            "2022-05-05T21:40:02.271689: step 9421, loss 1.42223, acc 0.546875\n",
            "2022-05-05T21:40:02.869590: step 9422, loss 1.22884, acc 0.578125\n",
            "2022-05-05T21:40:03.472704: step 9423, loss 1.39879, acc 0.5\n",
            "2022-05-05T21:40:04.098642: step 9424, loss 1.19361, acc 0.671875\n",
            "2022-05-05T21:40:04.728447: step 9425, loss 1.25899, acc 0.59375\n",
            "2022-05-05T21:40:05.342285: step 9426, loss 1.17729, acc 0.59375\n",
            "2022-05-05T21:40:05.933916: step 9427, loss 1.25168, acc 0.53125\n",
            "2022-05-05T21:40:06.542350: step 9428, loss 1.42098, acc 0.515625\n",
            "2022-05-05T21:40:07.145170: step 9429, loss 1.45901, acc 0.46875\n",
            "2022-05-05T21:40:07.734273: step 9430, loss 1.40546, acc 0.5\n",
            "2022-05-05T21:40:08.341766: step 9431, loss 1.36279, acc 0.4375\n",
            "2022-05-05T21:40:08.936942: step 9432, loss 1.43065, acc 0.546875\n",
            "2022-05-05T21:40:09.546815: step 9433, loss 1.29192, acc 0.5625\n",
            "2022-05-05T21:40:10.148400: step 9434, loss 1.26905, acc 0.5625\n",
            "2022-05-05T21:40:10.752146: step 9435, loss 1.3208, acc 0.5\n",
            "2022-05-05T21:40:11.360472: step 9436, loss 1.24994, acc 0.546875\n",
            "2022-05-05T21:40:11.955857: step 9437, loss 1.43222, acc 0.4375\n",
            "2022-05-05T21:40:12.569166: step 9438, loss 1.45348, acc 0.5\n",
            "2022-05-05T21:40:13.168294: step 9439, loss 1.39744, acc 0.46875\n",
            "2022-05-05T21:40:13.774097: step 9440, loss 1.19092, acc 0.625\n",
            "2022-05-05T21:40:14.377621: step 9441, loss 1.26119, acc 0.5625\n",
            "2022-05-05T21:40:14.998905: step 9442, loss 1.3917, acc 0.546875\n",
            "2022-05-05T21:40:15.596623: step 9443, loss 1.26384, acc 0.546875\n",
            "2022-05-05T21:40:16.195406: step 9444, loss 1.31731, acc 0.421875\n",
            "2022-05-05T21:40:16.811946: step 9445, loss 1.30002, acc 0.515625\n",
            "2022-05-05T21:40:17.415201: step 9446, loss 1.07825, acc 0.59375\n",
            "2022-05-05T21:40:18.007927: step 9447, loss 1.22816, acc 0.65625\n",
            "2022-05-05T21:40:18.603005: step 9448, loss 1.36006, acc 0.59375\n",
            "2022-05-05T21:40:19.197185: step 9449, loss 1.14469, acc 0.625\n",
            "2022-05-05T21:40:19.800317: step 9450, loss 1.1769, acc 0.546875\n",
            "2022-05-05T21:40:20.402118: step 9451, loss 1.37478, acc 0.453125\n",
            "2022-05-05T21:40:20.998758: step 9452, loss 1.50551, acc 0.4375\n",
            "2022-05-05T21:40:21.602801: step 9453, loss 1.51505, acc 0.46875\n",
            "2022-05-05T21:40:22.194870: step 9454, loss 1.40117, acc 0.515625\n",
            "2022-05-05T21:40:22.794424: step 9455, loss 1.30754, acc 0.53125\n",
            "2022-05-05T21:40:23.381475: step 9456, loss 1.35691, acc 0.453125\n",
            "2022-05-05T21:40:23.981584: step 9457, loss 1.35419, acc 0.546875\n",
            "2022-05-05T21:40:24.580277: step 9458, loss 1.34803, acc 0.53125\n",
            "2022-05-05T21:40:25.202977: step 9459, loss 1.38987, acc 0.421875\n",
            "2022-05-05T21:40:25.806143: step 9460, loss 1.45393, acc 0.484375\n",
            "2022-05-05T21:40:26.406149: step 9461, loss 1.31782, acc 0.609375\n",
            "2022-05-05T21:40:27.012444: step 9462, loss 1.3517, acc 0.53125\n",
            "2022-05-05T21:40:27.613373: step 9463, loss 1.15121, acc 0.5625\n",
            "2022-05-05T21:40:28.199600: step 9464, loss 1.3319, acc 0.53125\n",
            "2022-05-05T21:40:28.803775: step 9465, loss 1.28424, acc 0.578125\n",
            "2022-05-05T21:40:29.399899: step 9466, loss 1.34089, acc 0.515625\n",
            "2022-05-05T21:40:30.014616: step 9467, loss 1.21976, acc 0.5625\n",
            "2022-05-05T21:40:30.621929: step 9468, loss 1.3429, acc 0.59375\n",
            "2022-05-05T21:40:31.241862: step 9469, loss 1.1281, acc 0.59375\n",
            "2022-05-05T21:40:31.845137: step 9470, loss 1.13149, acc 0.625\n",
            "2022-05-05T21:40:32.449940: step 9471, loss 1.34694, acc 0.5\n",
            "2022-05-05T21:40:33.057254: step 9472, loss 1.42922, acc 0.421875\n",
            "2022-05-05T21:40:33.665319: step 9473, loss 1.08204, acc 0.640625\n",
            "2022-05-05T21:40:34.282474: step 9474, loss 1.24485, acc 0.5625\n",
            "2022-05-05T21:40:34.886314: step 9475, loss 1.27214, acc 0.625\n",
            "2022-05-05T21:40:35.507280: step 9476, loss 1.35212, acc 0.484375\n",
            "2022-05-05T21:40:36.116221: step 9477, loss 1.43427, acc 0.515625\n",
            "2022-05-05T21:40:36.709628: step 9478, loss 1.2125, acc 0.546875\n",
            "2022-05-05T21:40:37.311607: step 9479, loss 1.33158, acc 0.53125\n",
            "2022-05-05T21:40:37.917815: step 9480, loss 1.20164, acc 0.5625\n",
            "2022-05-05T21:40:38.503794: step 9481, loss 1.35558, acc 0.578125\n",
            "2022-05-05T21:40:39.107371: step 9482, loss 1.40009, acc 0.484375\n",
            "2022-05-05T21:40:39.707082: step 9483, loss 1.21969, acc 0.5625\n",
            "2022-05-05T21:40:40.315602: step 9484, loss 1.37709, acc 0.484375\n",
            "2022-05-05T21:40:40.917413: step 9485, loss 1.39625, acc 0.53125\n",
            "2022-05-05T21:40:41.527892: step 9486, loss 1.19221, acc 0.609375\n",
            "2022-05-05T21:40:42.144613: step 9487, loss 1.20691, acc 0.609375\n",
            "2022-05-05T21:40:42.741617: step 9488, loss 1.39906, acc 0.4375\n",
            "2022-05-05T21:40:43.345434: step 9489, loss 1.17486, acc 0.578125\n",
            "2022-05-05T21:40:43.946336: step 9490, loss 1.19142, acc 0.578125\n",
            "2022-05-05T21:40:44.560692: step 9491, loss 1.14706, acc 0.5625\n",
            "2022-05-05T21:40:45.165811: step 9492, loss 1.36198, acc 0.5\n",
            "2022-05-05T21:40:45.787658: step 9493, loss 1.33822, acc 0.484375\n",
            "2022-05-05T21:40:46.402597: step 9494, loss 1.39943, acc 0.46875\n",
            "2022-05-05T21:40:47.002047: step 9495, loss 1.40607, acc 0.53125\n",
            "2022-05-05T21:40:47.612296: step 9496, loss 1.5244, acc 0.40625\n",
            "2022-05-05T21:40:48.223681: step 9497, loss 1.30524, acc 0.546875\n",
            "2022-05-05T21:40:48.824169: step 9498, loss 1.29388, acc 0.546875\n",
            "2022-05-05T21:40:49.429928: step 9499, loss 1.34317, acc 0.59375\n",
            "2022-05-05T21:40:50.027535: step 9500, loss 1.2228, acc 0.625\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:40:50.168827: step 9500, loss 1.98858, acc 0.36\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-9500\n",
            "\n",
            "2022-05-05T21:40:50.995619: step 9501, loss 1.32531, acc 0.546875\n",
            "2022-05-05T21:40:51.599874: step 9502, loss 1.35093, acc 0.59375\n",
            "2022-05-05T21:40:52.204485: step 9503, loss 1.07313, acc 0.65625\n",
            "2022-05-05T21:40:52.817523: step 9504, loss 1.22668, acc 0.546875\n",
            "2022-05-05T21:40:53.439390: step 9505, loss 1.21181, acc 0.625\n",
            "2022-05-05T21:40:54.040369: step 9506, loss 1.20847, acc 0.578125\n",
            "2022-05-05T21:40:54.653165: step 9507, loss 1.22054, acc 0.59375\n",
            "2022-05-05T21:40:55.248224: step 9508, loss 1.59308, acc 0.40625\n",
            "2022-05-05T21:40:55.889885: step 9509, loss 1.36793, acc 0.5\n",
            "2022-05-05T21:40:56.493308: step 9510, loss 1.28773, acc 0.5625\n",
            "2022-05-05T21:40:57.098056: step 9511, loss 1.48526, acc 0.453125\n",
            "2022-05-05T21:40:57.722531: step 9512, loss 1.1588, acc 0.65625\n",
            "2022-05-05T21:40:58.330732: step 9513, loss 1.31936, acc 0.546875\n",
            "2022-05-05T21:40:58.939293: step 9514, loss 1.36298, acc 0.53125\n",
            "2022-05-05T21:40:59.549798: step 9515, loss 1.2351, acc 0.5\n",
            "2022-05-05T21:41:00.152233: step 9516, loss 1.49634, acc 0.484375\n",
            "2022-05-05T21:41:00.784251: step 9517, loss 1.20343, acc 0.578125\n",
            "2022-05-05T21:41:01.374897: step 9518, loss 1.33386, acc 0.53125\n",
            "2022-05-05T21:41:01.988154: step 9519, loss 1.32294, acc 0.59375\n",
            "2022-05-05T21:41:02.595249: step 9520, loss 1.2775, acc 0.515625\n",
            "2022-05-05T21:41:03.187966: step 9521, loss 1.14829, acc 0.640625\n",
            "2022-05-05T21:41:03.792674: step 9522, loss 1.49429, acc 0.484375\n",
            "2022-05-05T21:41:04.385621: step 9523, loss 1.33908, acc 0.5625\n",
            "2022-05-05T21:41:04.996538: step 9524, loss 1.39759, acc 0.484375\n",
            "2022-05-05T21:41:05.595057: step 9525, loss 1.15301, acc 0.609375\n",
            "2022-05-05T21:41:06.222327: step 9526, loss 1.24886, acc 0.578125\n",
            "2022-05-05T21:41:06.844273: step 9527, loss 1.42534, acc 0.5625\n",
            "2022-05-05T21:41:07.451130: step 9528, loss 1.45938, acc 0.5\n",
            "2022-05-05T21:41:08.064418: step 9529, loss 1.57613, acc 0.46875\n",
            "2022-05-05T21:41:08.669789: step 9530, loss 1.51309, acc 0.40625\n",
            "2022-05-05T21:41:09.281130: step 9531, loss 1.42594, acc 0.46875\n",
            "2022-05-05T21:41:09.890102: step 9532, loss 1.14487, acc 0.578125\n",
            "2022-05-05T21:41:10.490445: step 9533, loss 1.35586, acc 0.46875\n",
            "2022-05-05T21:41:11.109924: step 9534, loss 1.34498, acc 0.46875\n",
            "2022-05-05T21:41:11.703559: step 9535, loss 1.20799, acc 0.578125\n",
            "2022-05-05T21:41:12.320846: step 9536, loss 1.22446, acc 0.5625\n",
            "2022-05-05T21:41:12.927029: step 9537, loss 1.19491, acc 0.578125\n",
            "2022-05-05T21:41:13.521498: step 9538, loss 1.28945, acc 0.53125\n",
            "2022-05-05T21:41:14.126556: step 9539, loss 1.39427, acc 0.5625\n",
            "2022-05-05T21:41:14.722724: step 9540, loss 1.27015, acc 0.53125\n",
            "2022-05-05T21:41:15.334268: step 9541, loss 1.44833, acc 0.421875\n",
            "2022-05-05T21:41:15.933398: step 9542, loss 1.25159, acc 0.546875\n",
            "2022-05-05T21:41:16.558602: step 9543, loss 1.15087, acc 0.59375\n",
            "2022-05-05T21:41:17.161557: step 9544, loss 1.03701, acc 0.65625\n",
            "2022-05-05T21:41:17.764773: step 9545, loss 1.31733, acc 0.515625\n",
            "2022-05-05T21:41:18.373932: step 9546, loss 1.19017, acc 0.625\n",
            "2022-05-05T21:41:18.972878: step 9547, loss 1.41766, acc 0.453125\n",
            "2022-05-05T21:41:19.580851: step 9548, loss 1.36312, acc 0.515625\n",
            "2022-05-05T21:41:20.188500: step 9549, loss 1.39972, acc 0.484375\n",
            "2022-05-05T21:41:20.787681: step 9550, loss 1.1821, acc 0.59375\n",
            "2022-05-05T21:41:21.394751: step 9551, loss 1.33628, acc 0.53125\n",
            "2022-05-05T21:41:21.987091: step 9552, loss 1.08164, acc 0.609375\n",
            "2022-05-05T21:41:22.593632: step 9553, loss 1.24145, acc 0.53125\n",
            "2022-05-05T21:41:23.198921: step 9554, loss 1.48254, acc 0.515625\n",
            "2022-05-05T21:41:23.798727: step 9555, loss 1.27988, acc 0.625\n",
            "2022-05-05T21:41:24.402030: step 9556, loss 1.33866, acc 0.453125\n",
            "2022-05-05T21:41:24.996882: step 9557, loss 1.21243, acc 0.59375\n",
            "2022-05-05T21:41:25.603498: step 9558, loss 1.22623, acc 0.578125\n",
            "2022-05-05T21:41:26.208520: step 9559, loss 1.27746, acc 0.453125\n",
            "2022-05-05T21:41:26.837596: step 9560, loss 1.27839, acc 0.53125\n",
            "2022-05-05T21:41:27.455838: step 9561, loss 1.29887, acc 0.515625\n",
            "2022-05-05T21:41:28.046379: step 9562, loss 1.33574, acc 0.453125\n",
            "2022-05-05T21:41:28.658435: step 9563, loss 1.44508, acc 0.5\n",
            "2022-05-05T21:41:29.252631: step 9564, loss 1.60942, acc 0.390625\n",
            "2022-05-05T21:41:29.877205: step 9565, loss 1.16099, acc 0.578125\n",
            "2022-05-05T21:41:30.485114: step 9566, loss 1.18374, acc 0.6875\n",
            "2022-05-05T21:41:31.090243: step 9567, loss 1.40283, acc 0.484375\n",
            "2022-05-05T21:41:31.703972: step 9568, loss 1.21639, acc 0.625\n",
            "2022-05-05T21:41:32.317094: step 9569, loss 1.32389, acc 0.53125\n",
            "2022-05-05T21:41:32.938064: step 9570, loss 1.30767, acc 0.546875\n",
            "2022-05-05T21:41:33.552218: step 9571, loss 1.42205, acc 0.5\n",
            "2022-05-05T21:41:34.150784: step 9572, loss 1.34665, acc 0.53125\n",
            "2022-05-05T21:41:34.757624: step 9573, loss 1.23426, acc 0.625\n",
            "2022-05-05T21:41:35.352077: step 9574, loss 1.20634, acc 0.546875\n",
            "2022-05-05T21:41:35.970687: step 9575, loss 1.4743, acc 0.46875\n",
            "2022-05-05T21:41:36.577110: step 9576, loss 1.23538, acc 0.578125\n",
            "2022-05-05T21:41:37.179273: step 9577, loss 1.25843, acc 0.515625\n",
            "2022-05-05T21:41:37.801412: step 9578, loss 1.51268, acc 0.4375\n",
            "2022-05-05T21:41:38.400673: step 9579, loss 1.46086, acc 0.40625\n",
            "2022-05-05T21:41:39.012351: step 9580, loss 1.35264, acc 0.546875\n",
            "2022-05-05T21:41:39.614136: step 9581, loss 1.63026, acc 0.40625\n",
            "2022-05-05T21:41:40.221452: step 9582, loss 1.36981, acc 0.53125\n",
            "2022-05-05T21:41:40.829335: step 9583, loss 1.35789, acc 0.46875\n",
            "2022-05-05T21:41:41.424733: step 9584, loss 1.23258, acc 0.609375\n",
            "2022-05-05T21:41:42.025499: step 9585, loss 1.23326, acc 0.609375\n",
            "2022-05-05T21:41:42.628524: step 9586, loss 1.44649, acc 0.578125\n",
            "2022-05-05T21:41:43.235094: step 9587, loss 1.39139, acc 0.46875\n",
            "2022-05-05T21:41:43.839743: step 9588, loss 1.19869, acc 0.703125\n",
            "2022-05-05T21:41:44.430417: step 9589, loss 1.3789, acc 0.5\n",
            "2022-05-05T21:41:45.044607: step 9590, loss 1.45524, acc 0.46875\n",
            "2022-05-05T21:41:45.638499: step 9591, loss 1.23957, acc 0.546875\n",
            "2022-05-05T21:41:46.246296: step 9592, loss 1.23102, acc 0.625\n",
            "2022-05-05T21:41:46.887569: step 9593, loss 1.32509, acc 0.46875\n",
            "2022-05-05T21:41:47.477105: step 9594, loss 1.20172, acc 0.5\n",
            "2022-05-05T21:41:48.111501: step 9595, loss 1.20139, acc 0.578125\n",
            "2022-05-05T21:41:48.708724: step 9596, loss 1.41714, acc 0.5625\n",
            "2022-05-05T21:41:49.316832: step 9597, loss 1.3599, acc 0.53125\n",
            "2022-05-05T21:41:49.915920: step 9598, loss 1.3223, acc 0.53125\n",
            "2022-05-05T21:41:50.518711: step 9599, loss 1.28831, acc 0.484375\n",
            "2022-05-05T21:41:51.129382: step 9600, loss 1.14173, acc 0.671875\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:41:51.276327: step 9600, loss 2.07165, acc 0.32\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-9600\n",
            "\n",
            "2022-05-05T21:41:52.066396: step 9601, loss 1.46918, acc 0.453125\n",
            "2022-05-05T21:41:52.670893: step 9602, loss 1.32449, acc 0.515625\n",
            "2022-05-05T21:41:53.279213: step 9603, loss 1.2779, acc 0.59375\n",
            "2022-05-05T21:41:53.876716: step 9604, loss 1.06908, acc 0.625\n",
            "2022-05-05T21:41:54.493453: step 9605, loss 1.38298, acc 0.578125\n",
            "2022-05-05T21:41:55.091569: step 9606, loss 1.28486, acc 0.53125\n",
            "2022-05-05T21:41:55.704204: step 9607, loss 1.32774, acc 0.546875\n",
            "2022-05-05T21:41:56.317729: step 9608, loss 1.31149, acc 0.578125\n",
            "2022-05-05T21:41:56.928316: step 9609, loss 1.32898, acc 0.5\n",
            "2022-05-05T21:41:57.569153: step 9610, loss 1.34287, acc 0.53125\n",
            "2022-05-05T21:41:58.190705: step 9611, loss 1.14575, acc 0.578125\n",
            "2022-05-05T21:41:58.809572: step 9612, loss 1.21855, acc 0.5625\n",
            "2022-05-05T21:41:59.423912: step 9613, loss 1.42276, acc 0.5\n",
            "2022-05-05T21:42:00.021635: step 9614, loss 1.49408, acc 0.484375\n",
            "2022-05-05T21:42:00.634169: step 9615, loss 1.45323, acc 0.421875\n",
            "2022-05-05T21:42:01.246104: step 9616, loss 1.30971, acc 0.578125\n",
            "2022-05-05T21:42:01.843116: step 9617, loss 1.3434, acc 0.546875\n",
            "2022-05-05T21:42:02.455692: step 9618, loss 1.34213, acc 0.453125\n",
            "2022-05-05T21:42:03.075368: step 9619, loss 1.36613, acc 0.4375\n",
            "2022-05-05T21:42:03.681262: step 9620, loss 1.09632, acc 0.625\n",
            "2022-05-05T21:42:04.277887: step 9621, loss 1.1142, acc 0.53125\n",
            "2022-05-05T21:42:04.888365: step 9622, loss 1.35855, acc 0.53125\n",
            "2022-05-05T21:42:05.488105: step 9623, loss 1.42512, acc 0.578125\n",
            "2022-05-05T21:42:06.086316: step 9624, loss 1.31145, acc 0.53125\n",
            "2022-05-05T21:42:06.696075: step 9625, loss 1.26482, acc 0.546875\n",
            "2022-05-05T21:42:07.293111: step 9626, loss 1.3701, acc 0.578125\n",
            "2022-05-05T21:42:07.904656: step 9627, loss 1.37637, acc 0.46875\n",
            "2022-05-05T21:42:08.533967: step 9628, loss 1.1393, acc 0.671875\n",
            "2022-05-05T21:42:09.140737: step 9629, loss 1.23607, acc 0.546875\n",
            "2022-05-05T21:42:09.753700: step 9630, loss 1.24939, acc 0.59375\n",
            "2022-05-05T21:42:10.361341: step 9631, loss 1.40001, acc 0.53125\n",
            "2022-05-05T21:42:10.969463: step 9632, loss 1.39698, acc 0.484375\n",
            "2022-05-05T21:42:11.568283: step 9633, loss 1.35523, acc 0.53125\n",
            "2022-05-05T21:42:12.177339: step 9634, loss 1.42814, acc 0.484375\n",
            "2022-05-05T21:42:12.775494: step 9635, loss 1.56259, acc 0.390625\n",
            "2022-05-05T21:42:13.362626: step 9636, loss 1.39245, acc 0.515625\n",
            "2022-05-05T21:42:13.972097: step 9637, loss 1.16516, acc 0.53125\n",
            "2022-05-05T21:42:14.573930: step 9638, loss 1.24488, acc 0.671875\n",
            "2022-05-05T21:42:15.183742: step 9639, loss 1.30016, acc 0.5625\n",
            "2022-05-05T21:42:15.796914: step 9640, loss 1.2142, acc 0.53125\n",
            "2022-05-05T21:42:16.404766: step 9641, loss 1.20652, acc 0.625\n",
            "2022-05-05T21:42:17.028497: step 9642, loss 1.44544, acc 0.421875\n",
            "2022-05-05T21:42:17.633960: step 9643, loss 1.2354, acc 0.5625\n",
            "2022-05-05T21:42:18.242275: step 9644, loss 1.40099, acc 0.53125\n",
            "2022-05-05T21:42:18.866383: step 9645, loss 1.57221, acc 0.40625\n",
            "2022-05-05T21:42:19.462964: step 9646, loss 1.18831, acc 0.5625\n",
            "2022-05-05T21:42:20.075746: step 9647, loss 1.33027, acc 0.515625\n",
            "2022-05-05T21:42:20.671412: step 9648, loss 1.32557, acc 0.515625\n",
            "2022-05-05T21:42:21.283091: step 9649, loss 1.35879, acc 0.421875\n",
            "2022-05-05T21:42:21.886242: step 9650, loss 1.25067, acc 0.5\n",
            "2022-05-05T21:42:22.502370: step 9651, loss 1.4073, acc 0.5\n",
            "2022-05-05T21:42:23.111022: step 9652, loss 1.52625, acc 0.46875\n",
            "2022-05-05T21:42:23.712957: step 9653, loss 1.46884, acc 0.453125\n",
            "2022-05-05T21:42:24.319216: step 9654, loss 1.35646, acc 0.578125\n",
            "2022-05-05T21:42:24.916383: step 9655, loss 1.3763, acc 0.421875\n",
            "2022-05-05T21:42:25.520281: step 9656, loss 1.47049, acc 0.5\n",
            "2022-05-05T21:42:26.120532: step 9657, loss 1.31244, acc 0.546875\n",
            "2022-05-05T21:42:26.731095: step 9658, loss 1.52654, acc 0.5\n",
            "2022-05-05T21:42:27.342182: step 9659, loss 1.39867, acc 0.46875\n",
            "2022-05-05T21:42:27.939871: step 9660, loss 1.28794, acc 0.515625\n",
            "2022-05-05T21:42:28.547808: step 9661, loss 1.10092, acc 0.59375\n",
            "2022-05-05T21:42:29.169585: step 9662, loss 1.30735, acc 0.609375\n",
            "2022-05-05T21:42:29.770745: step 9663, loss 1.09474, acc 0.609375\n",
            "2022-05-05T21:42:30.377601: step 9664, loss 1.2791, acc 0.53125\n",
            "2022-05-05T21:42:30.976778: step 9665, loss 1.31099, acc 0.5\n",
            "2022-05-05T21:42:31.582464: step 9666, loss 1.41674, acc 0.484375\n",
            "2022-05-05T21:42:32.181675: step 9667, loss 1.18895, acc 0.546875\n",
            "2022-05-05T21:42:32.794699: step 9668, loss 1.20182, acc 0.515625\n",
            "2022-05-05T21:42:33.396202: step 9669, loss 1.28036, acc 0.515625\n",
            "2022-05-05T21:42:33.983879: step 9670, loss 1.28295, acc 0.625\n",
            "2022-05-05T21:42:34.632153: step 9671, loss 1.21368, acc 0.578125\n",
            "2022-05-05T21:42:35.305640: step 9672, loss 1.35972, acc 0.546875\n",
            "2022-05-05T21:42:35.903487: step 9673, loss 1.5414, acc 0.421875\n",
            "2022-05-05T21:42:36.508443: step 9674, loss 1.24164, acc 0.546875\n",
            "2022-05-05T21:42:37.089135: step 9675, loss 1.3748, acc 0.46875\n",
            "2022-05-05T21:42:37.694267: step 9676, loss 1.39097, acc 0.546875\n",
            "2022-05-05T21:42:38.285641: step 9677, loss 1.30737, acc 0.546875\n",
            "2022-05-05T21:42:38.891109: step 9678, loss 1.43225, acc 0.515625\n",
            "2022-05-05T21:42:39.513315: step 9679, loss 1.22913, acc 0.609375\n",
            "2022-05-05T21:42:40.107126: step 9680, loss 1.31921, acc 0.53125\n",
            "2022-05-05T21:42:40.718490: step 9681, loss 1.38199, acc 0.46875\n",
            "2022-05-05T21:42:41.320706: step 9682, loss 1.40279, acc 0.421875\n",
            "2022-05-05T21:42:41.934337: step 9683, loss 1.60248, acc 0.4375\n",
            "2022-05-05T21:42:42.532755: step 9684, loss 1.29698, acc 0.546875\n",
            "2022-05-05T21:42:43.144157: step 9685, loss 1.43936, acc 0.4375\n",
            "2022-05-05T21:42:43.748443: step 9686, loss 1.45799, acc 0.4375\n",
            "2022-05-05T21:42:44.351241: step 9687, loss 1.52958, acc 0.421875\n",
            "2022-05-05T21:42:44.949829: step 9688, loss 1.53299, acc 0.421875\n",
            "2022-05-05T21:42:45.540274: step 9689, loss 1.33949, acc 0.484375\n",
            "2022-05-05T21:42:46.151367: step 9690, loss 1.34441, acc 0.53125\n",
            "2022-05-05T21:42:46.751893: step 9691, loss 1.26582, acc 0.5625\n",
            "2022-05-05T21:42:47.341755: step 9692, loss 1.38954, acc 0.515625\n",
            "2022-05-05T21:42:47.952041: step 9693, loss 1.44048, acc 0.453125\n",
            "2022-05-05T21:42:48.538309: step 9694, loss 1.20081, acc 0.59375\n",
            "2022-05-05T21:42:49.133704: step 9695, loss 1.37617, acc 0.53125\n",
            "2022-05-05T21:42:49.753400: step 9696, loss 1.26782, acc 0.5625\n",
            "2022-05-05T21:42:50.341711: step 9697, loss 1.45161, acc 0.453125\n",
            "2022-05-05T21:42:50.936974: step 9698, loss 1.269, acc 0.546875\n",
            "2022-05-05T21:42:51.520938: step 9699, loss 1.22375, acc 0.59375\n",
            "2022-05-05T21:42:52.125466: step 9700, loss 1.16583, acc 0.578125\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:42:52.264158: step 9700, loss 2.19484, acc 0.25\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-9700\n",
            "\n",
            "2022-05-05T21:42:53.056277: step 9701, loss 1.36583, acc 0.453125\n",
            "2022-05-05T21:42:53.667591: step 9702, loss 1.41375, acc 0.53125\n",
            "2022-05-05T21:42:54.268002: step 9703, loss 1.55499, acc 0.40625\n",
            "2022-05-05T21:42:54.859085: step 9704, loss 1.30866, acc 0.5\n",
            "2022-05-05T21:42:55.457773: step 9705, loss 1.36086, acc 0.546875\n",
            "2022-05-05T21:42:56.063949: step 9706, loss 1.17443, acc 0.609375\n",
            "2022-05-05T21:42:56.651240: step 9707, loss 1.33915, acc 0.515625\n",
            "2022-05-05T21:42:57.253083: step 9708, loss 1.34603, acc 0.5\n",
            "2022-05-05T21:42:57.842404: step 9709, loss 1.44359, acc 0.5\n",
            "2022-05-05T21:42:58.490322: step 9710, loss 1.33321, acc 0.515625\n",
            "2022-05-05T21:42:59.322790: step 9711, loss 1.19908, acc 0.65625\n",
            "2022-05-05T21:43:00.155776: step 9712, loss 1.61233, acc 0.5\n",
            "2022-05-05T21:43:00.843293: step 9713, loss 1.35505, acc 0.4375\n",
            "2022-05-05T21:43:01.544421: step 9714, loss 1.30099, acc 0.53125\n",
            "2022-05-05T21:43:02.228300: step 9715, loss 1.56351, acc 0.5\n",
            "2022-05-05T21:43:02.919084: step 9716, loss 1.55211, acc 0.515625\n",
            "2022-05-05T21:43:03.783262: step 9717, loss 1.26424, acc 0.59375\n",
            "2022-05-05T21:43:04.772871: step 9718, loss 1.37795, acc 0.5\n",
            "2022-05-05T21:43:06.056925: step 9719, loss 1.4033, acc 0.484375\n",
            "2022-05-05T21:43:07.010372: step 9720, loss 1.26643, acc 0.578125\n",
            "2022-05-05T21:43:07.650974: step 9721, loss 1.48498, acc 0.390625\n",
            "2022-05-05T21:43:08.244844: step 9722, loss 1.18287, acc 0.59375\n",
            "2022-05-05T21:43:08.847545: step 9723, loss 1.198, acc 0.546875\n",
            "2022-05-05T21:43:09.442711: step 9724, loss 1.34782, acc 0.546875\n",
            "2022-05-05T21:43:10.050783: step 9725, loss 1.31706, acc 0.53125\n",
            "2022-05-05T21:43:10.685440: step 9726, loss 1.3233, acc 0.515625\n",
            "2022-05-05T21:43:11.285836: step 9727, loss 1.33488, acc 0.53125\n",
            "2022-05-05T21:43:11.891713: step 9728, loss 1.47289, acc 0.5\n",
            "2022-05-05T21:43:12.482718: step 9729, loss 1.48039, acc 0.546875\n",
            "2022-05-05T21:43:13.107982: step 9730, loss 1.30941, acc 0.484375\n",
            "2022-05-05T21:43:13.707094: step 9731, loss 1.3896, acc 0.53125\n",
            "2022-05-05T21:43:14.315965: step 9732, loss 1.38333, acc 0.484375\n",
            "2022-05-05T21:43:14.931376: step 9733, loss 1.3564, acc 0.484375\n",
            "2022-05-05T21:43:15.532620: step 9734, loss 1.30976, acc 0.515625\n",
            "2022-05-05T21:43:16.142765: step 9735, loss 1.36945, acc 0.5\n",
            "2022-05-05T21:43:16.733090: step 9736, loss 1.41975, acc 0.4375\n",
            "2022-05-05T21:43:17.334687: step 9737, loss 1.09407, acc 0.65625\n",
            "2022-05-05T21:43:17.935825: step 9738, loss 1.25869, acc 0.546875\n",
            "2022-05-05T21:43:18.528396: step 9739, loss 1.32744, acc 0.515625\n",
            "2022-05-05T21:43:19.140666: step 9740, loss 1.31843, acc 0.5\n",
            "2022-05-05T21:43:19.738358: step 9741, loss 1.20696, acc 0.5625\n",
            "2022-05-05T21:43:20.343606: step 9742, loss 1.32789, acc 0.546875\n",
            "2022-05-05T21:43:20.982410: step 9743, loss 1.58026, acc 0.34375\n",
            "2022-05-05T21:43:21.585597: step 9744, loss 1.32386, acc 0.546875\n",
            "2022-05-05T21:43:22.197961: step 9745, loss 1.29113, acc 0.46875\n",
            "2022-05-05T21:43:22.802470: step 9746, loss 1.47079, acc 0.484375\n",
            "2022-05-05T21:43:23.407257: step 9747, loss 1.37044, acc 0.484375\n",
            "2022-05-05T21:43:24.003556: step 9748, loss 1.46047, acc 0.390625\n",
            "2022-05-05T21:43:24.619115: step 9749, loss 1.08914, acc 0.625\n",
            "2022-05-05T21:43:25.228462: step 9750, loss 1.20149, acc 0.5625\n",
            "2022-05-05T21:43:25.822832: step 9751, loss 1.31194, acc 0.515625\n",
            "2022-05-05T21:43:26.435506: step 9752, loss 1.56866, acc 0.453125\n",
            "2022-05-05T21:43:27.041003: step 9753, loss 1.49346, acc 0.46875\n",
            "2022-05-05T21:43:27.642794: step 9754, loss 1.34634, acc 0.46875\n",
            "2022-05-05T21:43:28.241473: step 9755, loss 1.17402, acc 0.546875\n",
            "2022-05-05T21:43:28.845634: step 9756, loss 1.45047, acc 0.453125\n",
            "2022-05-05T21:43:29.444919: step 9757, loss 1.28985, acc 0.53125\n",
            "2022-05-05T21:43:30.049321: step 9758, loss 1.20388, acc 0.59375\n",
            "2022-05-05T21:43:30.653763: step 9759, loss 1.34519, acc 0.578125\n",
            "2022-05-05T21:43:31.292850: step 9760, loss 1.43327, acc 0.59375\n",
            "2022-05-05T21:43:31.884363: step 9761, loss 1.2688, acc 0.59375\n",
            "2022-05-05T21:43:32.498796: step 9762, loss 1.23763, acc 0.625\n",
            "2022-05-05T21:43:33.088065: step 9763, loss 1.41364, acc 0.484375\n",
            "2022-05-05T21:43:33.692455: step 9764, loss 1.32908, acc 0.59375\n",
            "2022-05-05T21:43:34.280142: step 9765, loss 1.38934, acc 0.515625\n",
            "2022-05-05T21:43:34.896587: step 9766, loss 1.36085, acc 0.515625\n",
            "2022-05-05T21:43:35.495723: step 9767, loss 1.23403, acc 0.59375\n",
            "2022-05-05T21:43:36.096610: step 9768, loss 1.39444, acc 0.53125\n",
            "2022-05-05T21:43:36.708204: step 9769, loss 1.22868, acc 0.515625\n",
            "2022-05-05T21:43:37.298180: step 9770, loss 1.22467, acc 0.59375\n",
            "2022-05-05T21:43:37.902209: step 9771, loss 1.39078, acc 0.53125\n",
            "2022-05-05T21:43:38.515659: step 9772, loss 1.25502, acc 0.53125\n",
            "2022-05-05T21:43:39.121579: step 9773, loss 1.31015, acc 0.53125\n",
            "2022-05-05T21:43:39.729900: step 9774, loss 1.30581, acc 0.59375\n",
            "2022-05-05T21:43:40.323493: step 9775, loss 1.55964, acc 0.421875\n",
            "2022-05-05T21:43:40.934463: step 9776, loss 1.24493, acc 0.59375\n",
            "2022-05-05T21:43:41.547326: step 9777, loss 1.24802, acc 0.578125\n",
            "2022-05-05T21:43:42.147657: step 9778, loss 1.23745, acc 0.546875\n",
            "2022-05-05T21:43:42.755885: step 9779, loss 1.39566, acc 0.546875\n",
            "2022-05-05T21:43:43.340805: step 9780, loss 1.41588, acc 0.375\n",
            "2022-05-05T21:43:43.944037: step 9781, loss 1.33384, acc 0.546875\n",
            "2022-05-05T21:43:44.544838: step 9782, loss 1.48236, acc 0.4375\n",
            "2022-05-05T21:43:45.149977: step 9783, loss 1.352, acc 0.5\n",
            "2022-05-05T21:43:45.757077: step 9784, loss 1.20262, acc 0.609375\n",
            "2022-05-05T21:43:46.361089: step 9785, loss 1.31629, acc 0.546875\n",
            "2022-05-05T21:43:46.975084: step 9786, loss 1.6701, acc 0.4375\n",
            "2022-05-05T21:43:47.579634: step 9787, loss 1.32674, acc 0.46875\n",
            "2022-05-05T21:43:48.183965: step 9788, loss 1.19698, acc 0.625\n",
            "2022-05-05T21:43:48.791252: step 9789, loss 1.21438, acc 0.5\n",
            "2022-05-05T21:43:49.381673: step 9790, loss 1.30122, acc 0.484375\n",
            "2022-05-05T21:43:49.986341: step 9791, loss 1.51982, acc 0.40625\n",
            "2022-05-05T21:43:50.575576: step 9792, loss 1.30897, acc 0.53125\n",
            "2022-05-05T21:43:51.182701: step 9793, loss 1.4857, acc 0.484375\n",
            "2022-05-05T21:43:51.801844: step 9794, loss 1.29107, acc 0.546875\n",
            "2022-05-05T21:43:52.409372: step 9795, loss 1.13645, acc 0.59375\n",
            "2022-05-05T21:43:53.014982: step 9796, loss 1.25978, acc 0.578125\n",
            "2022-05-05T21:43:53.608949: step 9797, loss 1.42145, acc 0.5625\n",
            "2022-05-05T21:43:54.205265: step 9798, loss 1.49259, acc 0.453125\n",
            "2022-05-05T21:43:54.795388: step 9799, loss 1.567, acc 0.40625\n",
            "2022-05-05T21:43:55.394057: step 9800, loss 1.29931, acc 0.578125\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:43:55.540287: step 9800, loss 2.26881, acc 0.27\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-9800\n",
            "\n",
            "2022-05-05T21:43:56.354798: step 9801, loss 1.48945, acc 0.421875\n",
            "2022-05-05T21:43:56.947740: step 9802, loss 1.309, acc 0.546875\n",
            "2022-05-05T21:43:57.543478: step 9803, loss 1.42482, acc 0.5625\n",
            "2022-05-05T21:43:58.149804: step 9804, loss 1.47372, acc 0.375\n",
            "2022-05-05T21:43:58.753089: step 9805, loss 1.397, acc 0.53125\n",
            "2022-05-05T21:43:59.358593: step 9806, loss 1.43466, acc 0.421875\n",
            "2022-05-05T21:43:59.949480: step 9807, loss 1.42921, acc 0.484375\n",
            "2022-05-05T21:44:00.548614: step 9808, loss 1.47578, acc 0.453125\n",
            "2022-05-05T21:44:01.160746: step 9809, loss 1.43204, acc 0.484375\n",
            "2022-05-05T21:44:01.759145: step 9810, loss 1.27743, acc 0.515625\n",
            "2022-05-05T21:44:02.396190: step 9811, loss 1.48955, acc 0.421875\n",
            "2022-05-05T21:44:02.991392: step 9812, loss 1.27705, acc 0.5625\n",
            "2022-05-05T21:44:03.598590: step 9813, loss 1.33786, acc 0.515625\n",
            "2022-05-05T21:44:04.193904: step 9814, loss 1.4541, acc 0.484375\n",
            "2022-05-05T21:44:04.794792: step 9815, loss 1.31687, acc 0.5625\n",
            "2022-05-05T21:44:05.407125: step 9816, loss 1.11054, acc 0.609375\n",
            "2022-05-05T21:44:06.004366: step 9817, loss 1.36813, acc 0.5\n",
            "2022-05-05T21:44:06.619933: step 9818, loss 1.32887, acc 0.5\n",
            "2022-05-05T21:44:07.260995: step 9819, loss 1.64813, acc 0.421875\n",
            "2022-05-05T21:44:07.873028: step 9820, loss 1.38182, acc 0.5\n",
            "2022-05-05T21:44:08.471444: step 9821, loss 1.539, acc 0.453125\n",
            "2022-05-05T21:44:09.062727: step 9822, loss 1.47288, acc 0.5\n",
            "2022-05-05T21:44:09.669117: step 9823, loss 1.26604, acc 0.609375\n",
            "2022-05-05T21:44:10.265543: step 9824, loss 1.12104, acc 0.578125\n",
            "2022-05-05T21:44:10.877945: step 9825, loss 1.36258, acc 0.546875\n",
            "2022-05-05T21:44:11.374525: step 9826, loss 1.48951, acc 0.469388\n",
            "2022-05-05T21:44:12.005454: step 9827, loss 1.14894, acc 0.65625\n",
            "2022-05-05T21:44:12.646122: step 9828, loss 1.30124, acc 0.484375\n",
            "2022-05-05T21:44:13.274112: step 9829, loss 1.21997, acc 0.5625\n",
            "2022-05-05T21:44:13.877230: step 9830, loss 1.35088, acc 0.53125\n",
            "2022-05-05T21:44:14.471368: step 9831, loss 1.35891, acc 0.5\n",
            "2022-05-05T21:44:15.073373: step 9832, loss 1.24717, acc 0.640625\n",
            "2022-05-05T21:44:15.682903: step 9833, loss 1.11928, acc 0.625\n",
            "2022-05-05T21:44:16.279314: step 9834, loss 1.19686, acc 0.625\n",
            "2022-05-05T21:44:16.887696: step 9835, loss 1.2746, acc 0.5625\n",
            "2022-05-05T21:44:17.476878: step 9836, loss 1.21956, acc 0.578125\n",
            "2022-05-05T21:44:18.078153: step 9837, loss 1.18842, acc 0.53125\n",
            "2022-05-05T21:44:18.678280: step 9838, loss 1.23909, acc 0.65625\n",
            "2022-05-05T21:44:19.284032: step 9839, loss 1.03658, acc 0.671875\n",
            "2022-05-05T21:44:19.890421: step 9840, loss 1.1741, acc 0.640625\n",
            "2022-05-05T21:44:20.488807: step 9841, loss 1.15341, acc 0.671875\n",
            "2022-05-05T21:44:21.101737: step 9842, loss 1.24791, acc 0.625\n",
            "2022-05-05T21:44:21.704244: step 9843, loss 1.24037, acc 0.640625\n",
            "2022-05-05T21:44:22.313269: step 9844, loss 1.08137, acc 0.640625\n",
            "2022-05-05T21:44:22.952036: step 9845, loss 1.21542, acc 0.5\n",
            "2022-05-05T21:44:23.558299: step 9846, loss 1.24739, acc 0.59375\n",
            "2022-05-05T21:44:24.169626: step 9847, loss 1.1966, acc 0.59375\n",
            "2022-05-05T21:44:24.787235: step 9848, loss 1.28237, acc 0.546875\n",
            "2022-05-05T21:44:25.401018: step 9849, loss 1.23262, acc 0.46875\n",
            "2022-05-05T21:44:26.016179: step 9850, loss 1.35584, acc 0.46875\n",
            "2022-05-05T21:44:26.633293: step 9851, loss 1.06997, acc 0.609375\n",
            "2022-05-05T21:44:27.243520: step 9852, loss 1.07419, acc 0.625\n",
            "2022-05-05T21:44:27.847083: step 9853, loss 1.22563, acc 0.59375\n",
            "2022-05-05T21:44:28.459367: step 9854, loss 1.32655, acc 0.578125\n",
            "2022-05-05T21:44:29.078318: step 9855, loss 1.15807, acc 0.578125\n",
            "2022-05-05T21:44:29.687559: step 9856, loss 1.28918, acc 0.5625\n",
            "2022-05-05T21:44:30.293679: step 9857, loss 1.12999, acc 0.578125\n",
            "2022-05-05T21:44:30.888241: step 9858, loss 1.16427, acc 0.609375\n",
            "2022-05-05T21:44:31.511531: step 9859, loss 1.14382, acc 0.53125\n",
            "2022-05-05T21:44:32.123981: step 9860, loss 1.24118, acc 0.546875\n",
            "2022-05-05T21:44:32.726644: step 9861, loss 1.24321, acc 0.546875\n",
            "2022-05-05T21:44:33.359571: step 9862, loss 1.31153, acc 0.5\n",
            "2022-05-05T21:44:33.979663: step 9863, loss 1.3591, acc 0.578125\n",
            "2022-05-05T21:44:34.591451: step 9864, loss 1.1759, acc 0.625\n",
            "2022-05-05T21:44:35.211254: step 9865, loss 1.34652, acc 0.53125\n",
            "2022-05-05T21:44:35.807144: step 9866, loss 1.32565, acc 0.53125\n",
            "2022-05-05T21:44:36.418229: step 9867, loss 1.19373, acc 0.578125\n",
            "2022-05-05T21:44:37.015511: step 9868, loss 1.03473, acc 0.65625\n",
            "2022-05-05T21:44:37.619863: step 9869, loss 1.53634, acc 0.46875\n",
            "2022-05-05T21:44:38.216768: step 9870, loss 1.38989, acc 0.484375\n",
            "2022-05-05T21:44:38.837315: step 9871, loss 1.33858, acc 0.5625\n",
            "2022-05-05T21:44:39.448938: step 9872, loss 1.44904, acc 0.546875\n",
            "2022-05-05T21:44:40.047342: step 9873, loss 1.23191, acc 0.625\n",
            "2022-05-05T21:44:40.640965: step 9874, loss 1.35653, acc 0.4375\n",
            "2022-05-05T21:44:41.225805: step 9875, loss 1.24819, acc 0.59375\n",
            "2022-05-05T21:44:41.831445: step 9876, loss 1.27506, acc 0.453125\n",
            "2022-05-05T21:44:42.434297: step 9877, loss 1.3665, acc 0.5\n",
            "2022-05-05T21:44:43.029452: step 9878, loss 1.34116, acc 0.5\n",
            "2022-05-05T21:44:43.665623: step 9879, loss 1.22631, acc 0.546875\n",
            "2022-05-05T21:44:44.257547: step 9880, loss 1.28421, acc 0.46875\n",
            "2022-05-05T21:44:44.859447: step 9881, loss 1.21236, acc 0.578125\n",
            "2022-05-05T21:44:45.442548: step 9882, loss 1.13417, acc 0.5625\n",
            "2022-05-05T21:44:46.047639: step 9883, loss 0.975547, acc 0.6875\n",
            "2022-05-05T21:44:46.649671: step 9884, loss 1.22028, acc 0.59375\n",
            "2022-05-05T21:44:47.254025: step 9885, loss 1.354, acc 0.53125\n",
            "2022-05-05T21:44:47.861862: step 9886, loss 1.29685, acc 0.59375\n",
            "2022-05-05T21:44:48.456960: step 9887, loss 1.25455, acc 0.5625\n",
            "2022-05-05T21:44:49.060164: step 9888, loss 1.28393, acc 0.5625\n",
            "2022-05-05T21:44:49.655751: step 9889, loss 1.26372, acc 0.59375\n",
            "2022-05-05T21:44:50.245610: step 9890, loss 1.20305, acc 0.59375\n",
            "2022-05-05T21:44:50.844247: step 9891, loss 1.0926, acc 0.609375\n",
            "2022-05-05T21:44:51.425661: step 9892, loss 1.27314, acc 0.546875\n",
            "2022-05-05T21:44:52.033316: step 9893, loss 1.41119, acc 0.484375\n",
            "2022-05-05T21:44:52.637676: step 9894, loss 1.31126, acc 0.609375\n",
            "2022-05-05T21:44:53.244077: step 9895, loss 1.21223, acc 0.5\n",
            "2022-05-05T21:44:53.874661: step 9896, loss 1.27692, acc 0.453125\n",
            "2022-05-05T21:44:54.488534: step 9897, loss 1.30425, acc 0.5625\n",
            "2022-05-05T21:44:55.106148: step 9898, loss 1.26827, acc 0.578125\n",
            "2022-05-05T21:44:55.706376: step 9899, loss 1.27622, acc 0.515625\n",
            "2022-05-05T21:44:56.315298: step 9900, loss 1.14084, acc 0.578125\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:44:56.456720: step 9900, loss 2.02975, acc 0.31\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-9900\n",
            "\n",
            "2022-05-05T21:44:57.267330: step 9901, loss 1.17956, acc 0.640625\n",
            "2022-05-05T21:44:57.849858: step 9902, loss 1.45137, acc 0.46875\n",
            "2022-05-05T21:44:58.458101: step 9903, loss 1.05753, acc 0.6875\n",
            "2022-05-05T21:44:59.054860: step 9904, loss 1.19603, acc 0.640625\n",
            "2022-05-05T21:44:59.653673: step 9905, loss 1.15594, acc 0.578125\n",
            "2022-05-05T21:45:00.324486: step 9906, loss 1.30384, acc 0.59375\n",
            "2022-05-05T21:45:00.917103: step 9907, loss 1.21729, acc 0.546875\n",
            "2022-05-05T21:45:01.543627: step 9908, loss 1.18461, acc 0.609375\n",
            "2022-05-05T21:45:02.156682: step 9909, loss 1.18485, acc 0.5625\n",
            "2022-05-05T21:45:02.757805: step 9910, loss 1.49985, acc 0.421875\n",
            "2022-05-05T21:45:03.361174: step 9911, loss 1.16049, acc 0.578125\n",
            "2022-05-05T21:45:03.979277: step 9912, loss 1.25958, acc 0.578125\n",
            "2022-05-05T21:45:04.601588: step 9913, loss 1.30086, acc 0.609375\n",
            "2022-05-05T21:45:05.212246: step 9914, loss 1.33249, acc 0.5\n",
            "2022-05-05T21:45:05.820211: step 9915, loss 1.15639, acc 0.59375\n",
            "2022-05-05T21:45:06.419862: step 9916, loss 1.06084, acc 0.6875\n",
            "2022-05-05T21:45:07.014709: step 9917, loss 1.33111, acc 0.53125\n",
            "2022-05-05T21:45:07.622021: step 9918, loss 1.22214, acc 0.625\n",
            "2022-05-05T21:45:08.216898: step 9919, loss 1.49189, acc 0.515625\n",
            "2022-05-05T21:45:08.811140: step 9920, loss 1.33544, acc 0.5625\n",
            "2022-05-05T21:45:09.410460: step 9921, loss 1.11532, acc 0.609375\n",
            "2022-05-05T21:45:10.004317: step 9922, loss 1.36006, acc 0.53125\n",
            "2022-05-05T21:45:10.606073: step 9923, loss 1.25961, acc 0.5625\n",
            "2022-05-05T21:45:11.199286: step 9924, loss 1.21279, acc 0.5625\n",
            "2022-05-05T21:45:11.807831: step 9925, loss 1.26145, acc 0.484375\n",
            "2022-05-05T21:45:12.406502: step 9926, loss 1.21747, acc 0.640625\n",
            "2022-05-05T21:45:13.003905: step 9927, loss 1.26535, acc 0.5625\n",
            "2022-05-05T21:45:13.616777: step 9928, loss 1.20127, acc 0.5625\n",
            "2022-05-05T21:45:14.244018: step 9929, loss 1.31204, acc 0.484375\n",
            "2022-05-05T21:45:14.856421: step 9930, loss 1.42829, acc 0.484375\n",
            "2022-05-05T21:45:15.451932: step 9931, loss 1.15023, acc 0.609375\n",
            "2022-05-05T21:45:16.050507: step 9932, loss 1.18631, acc 0.59375\n",
            "2022-05-05T21:45:16.662337: step 9933, loss 1.23234, acc 0.578125\n",
            "2022-05-05T21:45:17.258508: step 9934, loss 1.15717, acc 0.65625\n",
            "2022-05-05T21:45:17.869868: step 9935, loss 1.15659, acc 0.59375\n",
            "2022-05-05T21:45:18.464116: step 9936, loss 1.23684, acc 0.625\n",
            "2022-05-05T21:45:19.070247: step 9937, loss 1.23621, acc 0.609375\n",
            "2022-05-05T21:45:19.683035: step 9938, loss 1.36291, acc 0.59375\n",
            "2022-05-05T21:45:20.286675: step 9939, loss 1.33993, acc 0.53125\n",
            "2022-05-05T21:45:20.899694: step 9940, loss 1.02709, acc 0.65625\n",
            "2022-05-05T21:45:21.508883: step 9941, loss 1.26708, acc 0.515625\n",
            "2022-05-05T21:45:22.132199: step 9942, loss 1.21558, acc 0.59375\n",
            "2022-05-05T21:45:22.753962: step 9943, loss 1.16212, acc 0.59375\n",
            "2022-05-05T21:45:23.364833: step 9944, loss 1.2032, acc 0.65625\n",
            "2022-05-05T21:45:23.975158: step 9945, loss 1.33366, acc 0.546875\n",
            "2022-05-05T21:45:24.609924: step 9946, loss 1.00422, acc 0.6875\n",
            "2022-05-05T21:45:25.210592: step 9947, loss 1.27928, acc 0.59375\n",
            "2022-05-05T21:45:25.817148: step 9948, loss 1.29112, acc 0.546875\n",
            "2022-05-05T21:45:26.423710: step 9949, loss 1.10159, acc 0.609375\n",
            "2022-05-05T21:45:27.054269: step 9950, loss 1.30208, acc 0.578125\n",
            "2022-05-05T21:45:27.657616: step 9951, loss 1.18885, acc 0.65625\n",
            "2022-05-05T21:45:28.260198: step 9952, loss 1.16127, acc 0.578125\n",
            "2022-05-05T21:45:28.868772: step 9953, loss 1.42543, acc 0.453125\n",
            "2022-05-05T21:45:29.457959: step 9954, loss 1.37607, acc 0.5\n",
            "2022-05-05T21:45:30.077808: step 9955, loss 1.25103, acc 0.546875\n",
            "2022-05-05T21:45:30.685122: step 9956, loss 1.28817, acc 0.53125\n",
            "2022-05-05T21:45:31.302424: step 9957, loss 1.36821, acc 0.546875\n",
            "2022-05-05T21:45:31.913372: step 9958, loss 1.48175, acc 0.484375\n",
            "2022-05-05T21:45:32.512451: step 9959, loss 1.20827, acc 0.59375\n",
            "2022-05-05T21:45:33.130870: step 9960, loss 1.22229, acc 0.53125\n",
            "2022-05-05T21:45:33.735848: step 9961, loss 1.30015, acc 0.5625\n",
            "2022-05-05T21:45:34.342023: step 9962, loss 1.37577, acc 0.53125\n",
            "2022-05-05T21:45:34.959090: step 9963, loss 1.29641, acc 0.546875\n",
            "2022-05-05T21:45:35.568859: step 9964, loss 1.49159, acc 0.53125\n",
            "2022-05-05T21:45:36.179487: step 9965, loss 1.13087, acc 0.5625\n",
            "2022-05-05T21:45:36.786339: step 9966, loss 1.39115, acc 0.484375\n",
            "2022-05-05T21:45:37.388948: step 9967, loss 1.01232, acc 0.703125\n",
            "2022-05-05T21:45:37.983669: step 9968, loss 0.94891, acc 0.734375\n",
            "2022-05-05T21:45:38.588623: step 9969, loss 1.2063, acc 0.5625\n",
            "2022-05-05T21:45:39.205095: step 9970, loss 1.16176, acc 0.53125\n",
            "2022-05-05T21:45:39.792393: step 9971, loss 1.38308, acc 0.421875\n",
            "2022-05-05T21:45:40.407750: step 9972, loss 1.21503, acc 0.53125\n",
            "2022-05-05T21:45:41.006072: step 9973, loss 1.37565, acc 0.53125\n",
            "2022-05-05T21:45:41.615804: step 9974, loss 1.50072, acc 0.484375\n",
            "2022-05-05T21:45:42.216395: step 9975, loss 1.46013, acc 0.515625\n",
            "2022-05-05T21:45:42.810293: step 9976, loss 1.28153, acc 0.59375\n",
            "2022-05-05T21:45:43.418504: step 9977, loss 1.45274, acc 0.515625\n",
            "2022-05-05T21:45:44.023070: step 9978, loss 1.29803, acc 0.578125\n",
            "2022-05-05T21:45:44.635602: step 9979, loss 1.49777, acc 0.5\n",
            "2022-05-05T21:45:45.257025: step 9980, loss 1.32806, acc 0.515625\n",
            "2022-05-05T21:45:45.868544: step 9981, loss 1.24963, acc 0.65625\n",
            "2022-05-05T21:45:46.480676: step 9982, loss 1.22021, acc 0.53125\n",
            "2022-05-05T21:45:47.085156: step 9983, loss 1.16726, acc 0.59375\n",
            "2022-05-05T21:45:47.695009: step 9984, loss 1.44394, acc 0.5625\n",
            "2022-05-05T21:45:48.298640: step 9985, loss 1.26635, acc 0.515625\n",
            "2022-05-05T21:45:48.917653: step 9986, loss 1.20338, acc 0.609375\n",
            "2022-05-05T21:45:49.534425: step 9987, loss 1.24545, acc 0.484375\n",
            "2022-05-05T21:45:50.139665: step 9988, loss 1.2643, acc 0.578125\n",
            "2022-05-05T21:45:50.755725: step 9989, loss 1.26198, acc 0.5\n",
            "2022-05-05T21:45:51.367021: step 9990, loss 1.29233, acc 0.515625\n",
            "2022-05-05T21:45:51.990598: step 9991, loss 1.22433, acc 0.5625\n",
            "2022-05-05T21:45:52.610878: step 9992, loss 1.40132, acc 0.59375\n",
            "2022-05-05T21:45:53.212246: step 9993, loss 1.48115, acc 0.453125\n",
            "2022-05-05T21:45:53.831332: step 9994, loss 1.17597, acc 0.53125\n",
            "2022-05-05T21:45:54.441168: step 9995, loss 1.14513, acc 0.640625\n",
            "2022-05-05T21:45:55.046135: step 9996, loss 1.26661, acc 0.53125\n",
            "2022-05-05T21:45:55.692840: step 9997, loss 0.942135, acc 0.703125\n",
            "2022-05-05T21:45:56.295647: step 9998, loss 1.24789, acc 0.65625\n",
            "2022-05-05T21:45:56.908694: step 9999, loss 1.16756, acc 0.546875\n",
            "2022-05-05T21:45:57.517835: step 10000, loss 1.28976, acc 0.578125\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:45:57.673980: step 10000, loss 1.99484, acc 0.28\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-10000\n",
            "\n",
            "2022-05-05T21:45:58.474019: step 10001, loss 1.17225, acc 0.59375\n",
            "2022-05-05T21:45:59.092420: step 10002, loss 1.15758, acc 0.46875\n",
            "2022-05-05T21:45:59.697460: step 10003, loss 1.15456, acc 0.609375\n",
            "2022-05-05T21:46:00.312176: step 10004, loss 1.21862, acc 0.484375\n",
            "2022-05-05T21:46:00.928615: step 10005, loss 1.16249, acc 0.625\n",
            "2022-05-05T21:46:01.537648: step 10006, loss 1.36802, acc 0.5\n",
            "2022-05-05T21:46:02.157339: step 10007, loss 1.1853, acc 0.625\n",
            "2022-05-05T21:46:02.763323: step 10008, loss 1.37902, acc 0.5625\n",
            "2022-05-05T21:46:03.388832: step 10009, loss 1.48473, acc 0.515625\n",
            "2022-05-05T21:46:04.000717: step 10010, loss 1.31652, acc 0.515625\n",
            "2022-05-05T21:46:04.599878: step 10011, loss 1.43651, acc 0.484375\n",
            "2022-05-05T21:46:05.211939: step 10012, loss 1.41114, acc 0.515625\n",
            "2022-05-05T21:46:05.845703: step 10013, loss 1.42489, acc 0.53125\n",
            "2022-05-05T21:46:06.450479: step 10014, loss 1.2304, acc 0.625\n",
            "2022-05-05T21:46:07.058376: step 10015, loss 1.19889, acc 0.546875\n",
            "2022-05-05T21:46:07.661873: step 10016, loss 1.36905, acc 0.5625\n",
            "2022-05-05T21:46:08.264079: step 10017, loss 1.1663, acc 0.609375\n",
            "2022-05-05T21:46:08.858169: step 10018, loss 1.43545, acc 0.484375\n",
            "2022-05-05T21:46:09.470307: step 10019, loss 1.44021, acc 0.5\n",
            "2022-05-05T21:46:10.088338: step 10020, loss 1.2297, acc 0.53125\n",
            "2022-05-05T21:46:10.695442: step 10021, loss 1.18822, acc 0.609375\n",
            "2022-05-05T21:46:11.311099: step 10022, loss 1.44225, acc 0.453125\n",
            "2022-05-05T21:46:11.907422: step 10023, loss 1.33588, acc 0.4375\n",
            "2022-05-05T21:46:12.546346: step 10024, loss 1.22865, acc 0.59375\n",
            "2022-05-05T21:46:13.176827: step 10025, loss 1.33938, acc 0.546875\n",
            "2022-05-05T21:46:13.782748: step 10026, loss 1.32477, acc 0.59375\n",
            "2022-05-05T21:46:14.400802: step 10027, loss 1.39309, acc 0.46875\n",
            "2022-05-05T21:46:14.999455: step 10028, loss 1.20686, acc 0.640625\n",
            "2022-05-05T21:46:15.607069: step 10029, loss 1.3956, acc 0.46875\n",
            "2022-05-05T21:46:16.191488: step 10030, loss 1.3836, acc 0.484375\n",
            "2022-05-05T21:46:16.798215: step 10031, loss 1.34022, acc 0.609375\n",
            "2022-05-05T21:46:17.392293: step 10032, loss 1.42288, acc 0.484375\n",
            "2022-05-05T21:46:18.006434: step 10033, loss 1.25361, acc 0.53125\n",
            "2022-05-05T21:46:18.608348: step 10034, loss 1.16218, acc 0.59375\n",
            "2022-05-05T21:46:19.197052: step 10035, loss 1.49409, acc 0.421875\n",
            "2022-05-05T21:46:19.793055: step 10036, loss 1.39726, acc 0.53125\n",
            "2022-05-05T21:46:20.382732: step 10037, loss 1.39342, acc 0.53125\n",
            "2022-05-05T21:46:20.962826: step 10038, loss 1.20692, acc 0.53125\n",
            "2022-05-05T21:46:21.555935: step 10039, loss 1.24348, acc 0.53125\n",
            "2022-05-05T21:46:22.145525: step 10040, loss 1.25237, acc 0.640625\n",
            "2022-05-05T21:46:22.751217: step 10041, loss 1.39145, acc 0.5\n",
            "2022-05-05T21:46:23.343210: step 10042, loss 1.2768, acc 0.5\n",
            "2022-05-05T21:46:23.936970: step 10043, loss 1.17375, acc 0.546875\n",
            "2022-05-05T21:46:24.536334: step 10044, loss 1.00003, acc 0.671875\n",
            "2022-05-05T21:46:25.131155: step 10045, loss 1.42573, acc 0.46875\n",
            "2022-05-05T21:46:25.733313: step 10046, loss 1.33765, acc 0.53125\n",
            "2022-05-05T21:46:26.322683: step 10047, loss 1.08584, acc 0.671875\n",
            "2022-05-05T21:46:26.920175: step 10048, loss 1.4602, acc 0.5\n",
            "2022-05-05T21:46:27.503313: step 10049, loss 1.32282, acc 0.53125\n",
            "2022-05-05T21:46:28.104327: step 10050, loss 1.2178, acc 0.53125\n",
            "2022-05-05T21:46:28.709714: step 10051, loss 1.35337, acc 0.5\n",
            "2022-05-05T21:46:29.300875: step 10052, loss 1.29714, acc 0.609375\n",
            "2022-05-05T21:46:29.901535: step 10053, loss 1.21839, acc 0.59375\n",
            "2022-05-05T21:46:30.487054: step 10054, loss 1.68783, acc 0.375\n",
            "2022-05-05T21:46:31.085712: step 10055, loss 1.29314, acc 0.578125\n",
            "2022-05-05T21:46:31.682704: step 10056, loss 1.40159, acc 0.46875\n",
            "2022-05-05T21:46:32.291532: step 10057, loss 1.32229, acc 0.53125\n",
            "2022-05-05T21:46:32.894507: step 10058, loss 1.28687, acc 0.484375\n",
            "2022-05-05T21:46:33.488816: step 10059, loss 1.24483, acc 0.546875\n",
            "2022-05-05T21:46:34.092564: step 10060, loss 1.39371, acc 0.453125\n",
            "2022-05-05T21:46:34.688953: step 10061, loss 1.13393, acc 0.5625\n",
            "2022-05-05T21:46:35.292822: step 10062, loss 1.2371, acc 0.546875\n",
            "2022-05-05T21:46:35.889979: step 10063, loss 1.25612, acc 0.515625\n",
            "2022-05-05T21:46:36.471366: step 10064, loss 1.00893, acc 0.65625\n",
            "2022-05-05T21:46:37.073176: step 10065, loss 1.22648, acc 0.609375\n",
            "2022-05-05T21:46:37.663136: step 10066, loss 1.21791, acc 0.546875\n",
            "2022-05-05T21:46:38.260562: step 10067, loss 1.35796, acc 0.5\n",
            "2022-05-05T21:46:38.857237: step 10068, loss 1.27753, acc 0.5625\n",
            "2022-05-05T21:46:39.458015: step 10069, loss 1.16925, acc 0.59375\n",
            "2022-05-05T21:46:40.064063: step 10070, loss 1.28831, acc 0.53125\n",
            "2022-05-05T21:46:40.669568: step 10071, loss 1.43001, acc 0.46875\n",
            "2022-05-05T21:46:41.288975: step 10072, loss 1.5127, acc 0.484375\n",
            "2022-05-05T21:46:41.891791: step 10073, loss 1.10307, acc 0.53125\n",
            "2022-05-05T21:46:42.531541: step 10074, loss 1.2556, acc 0.515625\n",
            "2022-05-05T21:46:43.139850: step 10075, loss 1.40437, acc 0.5\n",
            "2022-05-05T21:46:43.739314: step 10076, loss 1.39552, acc 0.515625\n",
            "2022-05-05T21:46:44.357367: step 10077, loss 1.26646, acc 0.5625\n",
            "2022-05-05T21:46:44.947640: step 10078, loss 1.37577, acc 0.515625\n",
            "2022-05-05T21:46:45.551095: step 10079, loss 1.21033, acc 0.546875\n",
            "2022-05-05T21:46:46.140978: step 10080, loss 1.05217, acc 0.5625\n",
            "2022-05-05T21:46:46.743091: step 10081, loss 1.27102, acc 0.546875\n",
            "2022-05-05T21:46:47.338106: step 10082, loss 1.48168, acc 0.4375\n",
            "2022-05-05T21:46:47.930102: step 10083, loss 1.55759, acc 0.515625\n",
            "2022-05-05T21:46:48.530774: step 10084, loss 1.16786, acc 0.546875\n",
            "2022-05-05T21:46:49.124817: step 10085, loss 1.28583, acc 0.5\n",
            "2022-05-05T21:46:49.735889: step 10086, loss 1.31276, acc 0.59375\n",
            "2022-05-05T21:46:50.335862: step 10087, loss 1.1937, acc 0.609375\n",
            "2022-05-05T21:46:50.952980: step 10088, loss 1.39076, acc 0.421875\n",
            "2022-05-05T21:46:51.553487: step 10089, loss 1.32725, acc 0.5\n",
            "2022-05-05T21:46:52.149316: step 10090, loss 1.26085, acc 0.578125\n",
            "2022-05-05T21:46:52.779400: step 10091, loss 1.12923, acc 0.640625\n",
            "2022-05-05T21:46:53.368649: step 10092, loss 1.18894, acc 0.625\n",
            "2022-05-05T21:46:53.973154: step 10093, loss 1.22756, acc 0.546875\n",
            "2022-05-05T21:46:54.571495: step 10094, loss 1.23247, acc 0.484375\n",
            "2022-05-05T21:46:55.168037: step 10095, loss 1.40997, acc 0.53125\n",
            "2022-05-05T21:46:55.774671: step 10096, loss 1.29576, acc 0.5625\n",
            "2022-05-05T21:46:56.376715: step 10097, loss 1.48354, acc 0.421875\n",
            "2022-05-05T21:46:56.982904: step 10098, loss 1.22349, acc 0.625\n",
            "2022-05-05T21:46:57.582686: step 10099, loss 1.20561, acc 0.5625\n",
            "2022-05-05T21:46:58.184056: step 10100, loss 1.49099, acc 0.484375\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:46:58.327190: step 10100, loss 1.97445, acc 0.35\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-10100\n",
            "\n",
            "2022-05-05T21:46:59.191101: step 10101, loss 1.2099, acc 0.5625\n",
            "2022-05-05T21:46:59.801870: step 10102, loss 1.35656, acc 0.53125\n",
            "2022-05-05T21:47:00.410510: step 10103, loss 1.2788, acc 0.5\n",
            "2022-05-05T21:47:01.022328: step 10104, loss 1.40669, acc 0.484375\n",
            "2022-05-05T21:47:01.623214: step 10105, loss 1.34388, acc 0.53125\n",
            "2022-05-05T21:47:02.221779: step 10106, loss 1.32445, acc 0.546875\n",
            "2022-05-05T21:47:02.831025: step 10107, loss 1.2531, acc 0.5625\n",
            "2022-05-05T21:47:03.447982: step 10108, loss 1.32932, acc 0.421875\n",
            "2022-05-05T21:47:04.045056: step 10109, loss 1.25041, acc 0.53125\n",
            "2022-05-05T21:47:04.670071: step 10110, loss 1.47286, acc 0.53125\n",
            "2022-05-05T21:47:05.278599: step 10111, loss 1.15414, acc 0.625\n",
            "2022-05-05T21:47:05.886262: step 10112, loss 1.37772, acc 0.546875\n",
            "2022-05-05T21:47:06.488244: step 10113, loss 1.35175, acc 0.53125\n",
            "2022-05-05T21:47:07.100806: step 10114, loss 1.26132, acc 0.5\n",
            "2022-05-05T21:47:07.698320: step 10115, loss 1.30008, acc 0.515625\n",
            "2022-05-05T21:47:08.301195: step 10116, loss 1.4385, acc 0.4375\n",
            "2022-05-05T21:47:08.896556: step 10117, loss 1.42157, acc 0.5\n",
            "2022-05-05T21:47:09.509692: step 10118, loss 1.30058, acc 0.625\n",
            "2022-05-05T21:47:10.112483: step 10119, loss 1.23813, acc 0.59375\n",
            "2022-05-05T21:47:10.718457: step 10120, loss 1.38339, acc 0.546875\n",
            "2022-05-05T21:47:11.322145: step 10121, loss 1.30575, acc 0.5\n",
            "2022-05-05T21:47:11.920438: step 10122, loss 1.17803, acc 0.625\n",
            "2022-05-05T21:47:12.532534: step 10123, loss 1.28339, acc 0.515625\n",
            "2022-05-05T21:47:13.148717: step 10124, loss 1.20971, acc 0.5625\n",
            "2022-05-05T21:47:13.777840: step 10125, loss 1.30491, acc 0.484375\n",
            "2022-05-05T21:47:14.385215: step 10126, loss 1.3997, acc 0.53125\n",
            "2022-05-05T21:47:14.984345: step 10127, loss 1.18254, acc 0.5\n",
            "2022-05-05T21:47:15.594827: step 10128, loss 1.40203, acc 0.578125\n",
            "2022-05-05T21:47:16.195663: step 10129, loss 1.30252, acc 0.484375\n",
            "2022-05-05T21:47:16.797171: step 10130, loss 1.31396, acc 0.609375\n",
            "2022-05-05T21:47:17.407748: step 10131, loss 1.35166, acc 0.5\n",
            "2022-05-05T21:47:18.005279: step 10132, loss 1.53761, acc 0.40625\n",
            "2022-05-05T21:47:18.622919: step 10133, loss 1.27835, acc 0.578125\n",
            "2022-05-05T21:47:19.224483: step 10134, loss 1.28312, acc 0.59375\n",
            "2022-05-05T21:47:19.823233: step 10135, loss 1.214, acc 0.546875\n",
            "2022-05-05T21:47:20.427739: step 10136, loss 1.01828, acc 0.65625\n",
            "2022-05-05T21:47:21.026766: step 10137, loss 1.4049, acc 0.5\n",
            "2022-05-05T21:47:21.635370: step 10138, loss 1.29561, acc 0.53125\n",
            "2022-05-05T21:47:22.232900: step 10139, loss 1.24583, acc 0.59375\n",
            "2022-05-05T21:47:22.846775: step 10140, loss 1.39223, acc 0.515625\n",
            "2022-05-05T21:47:23.449260: step 10141, loss 1.31513, acc 0.484375\n",
            "2022-05-05T21:47:24.074537: step 10142, loss 1.32361, acc 0.53125\n",
            "2022-05-05T21:47:24.674154: step 10143, loss 1.46081, acc 0.515625\n",
            "2022-05-05T21:47:25.271920: step 10144, loss 1.22975, acc 0.484375\n",
            "2022-05-05T21:47:25.889865: step 10145, loss 1.28691, acc 0.515625\n",
            "2022-05-05T21:47:26.503746: step 10146, loss 1.52705, acc 0.46875\n",
            "2022-05-05T21:47:27.101612: step 10147, loss 1.33398, acc 0.515625\n",
            "2022-05-05T21:47:27.720274: step 10148, loss 1.34943, acc 0.453125\n",
            "2022-05-05T21:47:28.313645: step 10149, loss 1.18472, acc 0.59375\n",
            "2022-05-05T21:47:28.932116: step 10150, loss 1.1474, acc 0.5625\n",
            "2022-05-05T21:47:29.546272: step 10151, loss 1.17946, acc 0.625\n",
            "2022-05-05T21:47:30.152483: step 10152, loss 1.09535, acc 0.609375\n",
            "2022-05-05T21:47:30.758480: step 10153, loss 1.20387, acc 0.546875\n",
            "2022-05-05T21:47:31.371375: step 10154, loss 1.28964, acc 0.53125\n",
            "2022-05-05T21:47:31.989165: step 10155, loss 1.2729, acc 0.46875\n",
            "2022-05-05T21:47:32.594381: step 10156, loss 1.24456, acc 0.578125\n",
            "2022-05-05T21:47:33.214345: step 10157, loss 1.27618, acc 0.5\n",
            "2022-05-05T21:47:33.833707: step 10158, loss 1.37103, acc 0.578125\n",
            "2022-05-05T21:47:34.441688: step 10159, loss 1.37063, acc 0.5\n",
            "2022-05-05T21:47:35.058173: step 10160, loss 1.37007, acc 0.546875\n",
            "2022-05-05T21:47:35.669347: step 10161, loss 1.39176, acc 0.453125\n",
            "2022-05-05T21:47:36.278874: step 10162, loss 1.32012, acc 0.5625\n",
            "2022-05-05T21:47:36.888046: step 10163, loss 1.29894, acc 0.546875\n",
            "2022-05-05T21:47:37.483446: step 10164, loss 1.37772, acc 0.515625\n",
            "2022-05-05T21:47:38.089093: step 10165, loss 1.13849, acc 0.53125\n",
            "2022-05-05T21:47:38.696915: step 10166, loss 1.22802, acc 0.59375\n",
            "2022-05-05T21:47:39.307438: step 10167, loss 1.21632, acc 0.59375\n",
            "2022-05-05T21:47:39.921833: step 10168, loss 1.28773, acc 0.515625\n",
            "2022-05-05T21:47:40.537542: step 10169, loss 1.31368, acc 0.53125\n",
            "2022-05-05T21:47:41.151564: step 10170, loss 1.24897, acc 0.59375\n",
            "2022-05-05T21:47:41.752736: step 10171, loss 1.1693, acc 0.5625\n",
            "2022-05-05T21:47:42.361478: step 10172, loss 1.30319, acc 0.53125\n",
            "2022-05-05T21:47:42.959419: step 10173, loss 1.26702, acc 0.53125\n",
            "2022-05-05T21:47:43.549768: step 10174, loss 1.16583, acc 0.609375\n",
            "2022-05-05T21:47:44.166801: step 10175, loss 1.44512, acc 0.4375\n",
            "2022-05-05T21:47:44.768514: step 10176, loss 1.38047, acc 0.46875\n",
            "2022-05-05T21:47:45.370662: step 10177, loss 1.20634, acc 0.546875\n",
            "2022-05-05T21:47:45.966119: step 10178, loss 1.37597, acc 0.5\n",
            "2022-05-05T21:47:46.575665: step 10179, loss 1.48675, acc 0.40625\n",
            "2022-05-05T21:47:47.180133: step 10180, loss 1.13906, acc 0.59375\n",
            "2022-05-05T21:47:47.786766: step 10181, loss 1.20761, acc 0.609375\n",
            "2022-05-05T21:47:48.393524: step 10182, loss 1.35262, acc 0.53125\n",
            "2022-05-05T21:47:48.990030: step 10183, loss 1.05086, acc 0.65625\n",
            "2022-05-05T21:47:49.591923: step 10184, loss 1.35539, acc 0.46875\n",
            "2022-05-05T21:47:50.191825: step 10185, loss 1.30405, acc 0.609375\n",
            "2022-05-05T21:47:50.794582: step 10186, loss 1.0489, acc 0.65625\n",
            "2022-05-05T21:47:51.402331: step 10187, loss 1.41677, acc 0.53125\n",
            "2022-05-05T21:47:52.008201: step 10188, loss 1.18601, acc 0.5625\n",
            "2022-05-05T21:47:52.616859: step 10189, loss 1.15424, acc 0.578125\n",
            "2022-05-05T21:47:53.212653: step 10190, loss 1.50675, acc 0.421875\n",
            "2022-05-05T21:47:53.822087: step 10191, loss 1.18348, acc 0.546875\n",
            "2022-05-05T21:47:54.424751: step 10192, loss 1.24437, acc 0.546875\n",
            "2022-05-05T21:47:55.051062: step 10193, loss 1.2356, acc 0.640625\n",
            "2022-05-05T21:47:55.663357: step 10194, loss 1.32399, acc 0.546875\n",
            "2022-05-05T21:47:56.257715: step 10195, loss 1.51543, acc 0.5\n",
            "2022-05-05T21:47:56.881714: step 10196, loss 1.32072, acc 0.609375\n",
            "2022-05-05T21:47:57.486852: step 10197, loss 1.30878, acc 0.5625\n",
            "2022-05-05T21:47:58.090174: step 10198, loss 1.50449, acc 0.34375\n",
            "2022-05-05T21:47:58.698325: step 10199, loss 1.11035, acc 0.578125\n",
            "2022-05-05T21:47:59.295380: step 10200, loss 1.34705, acc 0.484375\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:47:59.451265: step 10200, loss 2.03532, acc 0.29\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-10200\n",
            "\n",
            "2022-05-05T21:48:00.228753: step 10201, loss 1.32096, acc 0.5625\n",
            "2022-05-05T21:48:00.840183: step 10202, loss 1.32196, acc 0.484375\n",
            "2022-05-05T21:48:01.436548: step 10203, loss 1.43852, acc 0.484375\n",
            "2022-05-05T21:48:02.051649: step 10204, loss 1.33569, acc 0.46875\n",
            "2022-05-05T21:48:02.665769: step 10205, loss 1.28119, acc 0.546875\n",
            "2022-05-05T21:48:03.259559: step 10206, loss 1.1474, acc 0.609375\n",
            "2022-05-05T21:48:03.868675: step 10207, loss 1.34493, acc 0.4375\n",
            "2022-05-05T21:48:04.464003: step 10208, loss 1.32821, acc 0.53125\n",
            "2022-05-05T21:48:05.092307: step 10209, loss 1.18434, acc 0.640625\n",
            "2022-05-05T21:48:05.694453: step 10210, loss 1.28936, acc 0.578125\n",
            "2022-05-05T21:48:06.331508: step 10211, loss 1.21049, acc 0.59375\n",
            "2022-05-05T21:48:06.944429: step 10212, loss 1.52661, acc 0.359375\n",
            "2022-05-05T21:48:07.549594: step 10213, loss 1.31317, acc 0.515625\n",
            "2022-05-05T21:48:08.163525: step 10214, loss 1.35453, acc 0.46875\n",
            "2022-05-05T21:48:08.766144: step 10215, loss 1.24378, acc 0.5\n",
            "2022-05-05T21:48:09.365081: step 10216, loss 1.49927, acc 0.5\n",
            "2022-05-05T21:48:09.977201: step 10217, loss 1.28823, acc 0.515625\n",
            "2022-05-05T21:48:10.589205: step 10218, loss 1.51022, acc 0.515625\n",
            "2022-05-05T21:48:11.202690: step 10219, loss 1.20091, acc 0.59375\n",
            "2022-05-05T21:48:11.839783: step 10220, loss 1.20265, acc 0.53125\n",
            "2022-05-05T21:48:12.433396: step 10221, loss 1.3421, acc 0.5625\n",
            "2022-05-05T21:48:13.050538: step 10222, loss 1.26413, acc 0.5\n",
            "2022-05-05T21:48:13.646961: step 10223, loss 1.22322, acc 0.578125\n",
            "2022-05-05T21:48:14.257714: step 10224, loss 1.21332, acc 0.578125\n",
            "2022-05-05T21:48:14.846384: step 10225, loss 1.17392, acc 0.546875\n",
            "2022-05-05T21:48:15.480676: step 10226, loss 1.34333, acc 0.53125\n",
            "2022-05-05T21:48:16.085567: step 10227, loss 1.18722, acc 0.578125\n",
            "2022-05-05T21:48:16.677816: step 10228, loss 1.10363, acc 0.65625\n",
            "2022-05-05T21:48:17.281005: step 10229, loss 1.35567, acc 0.578125\n",
            "2022-05-05T21:48:17.895143: step 10230, loss 1.5583, acc 0.4375\n",
            "2022-05-05T21:48:18.500011: step 10231, loss 1.22882, acc 0.5\n",
            "2022-05-05T21:48:19.105044: step 10232, loss 1.294, acc 0.515625\n",
            "2022-05-05T21:48:19.692652: step 10233, loss 1.2415, acc 0.515625\n",
            "2022-05-05T21:48:20.299809: step 10234, loss 1.2838, acc 0.53125\n",
            "2022-05-05T21:48:20.903341: step 10235, loss 1.27223, acc 0.609375\n",
            "2022-05-05T21:48:21.504136: step 10236, loss 1.52442, acc 0.515625\n",
            "2022-05-05T21:48:22.121488: step 10237, loss 1.24123, acc 0.453125\n",
            "2022-05-05T21:48:22.726688: step 10238, loss 1.34011, acc 0.578125\n",
            "2022-05-05T21:48:23.330859: step 10239, loss 1.57211, acc 0.453125\n",
            "2022-05-05T21:48:23.928831: step 10240, loss 1.3988, acc 0.53125\n",
            "2022-05-05T21:48:24.536705: step 10241, loss 1.39948, acc 0.5\n",
            "2022-05-05T21:48:25.135352: step 10242, loss 1.29178, acc 0.59375\n",
            "2022-05-05T21:48:25.766757: step 10243, loss 1.3024, acc 0.546875\n",
            "2022-05-05T21:48:26.382444: step 10244, loss 1.26809, acc 0.46875\n",
            "2022-05-05T21:48:26.978705: step 10245, loss 1.53161, acc 0.4375\n",
            "2022-05-05T21:48:27.584622: step 10246, loss 1.3777, acc 0.53125\n",
            "2022-05-05T21:48:28.176215: step 10247, loss 1.39569, acc 0.421875\n",
            "2022-05-05T21:48:28.792054: step 10248, loss 1.06419, acc 0.640625\n",
            "2022-05-05T21:48:29.392134: step 10249, loss 1.15281, acc 0.546875\n",
            "2022-05-05T21:48:29.987369: step 10250, loss 1.16694, acc 0.515625\n",
            "2022-05-05T21:48:30.587337: step 10251, loss 1.33721, acc 0.578125\n",
            "2022-05-05T21:48:31.180751: step 10252, loss 1.20199, acc 0.609375\n",
            "2022-05-05T21:48:31.789744: step 10253, loss 1.38334, acc 0.515625\n",
            "2022-05-05T21:48:32.393071: step 10254, loss 1.19138, acc 0.65625\n",
            "2022-05-05T21:48:32.994996: step 10255, loss 1.30681, acc 0.609375\n",
            "2022-05-05T21:48:33.600327: step 10256, loss 1.2303, acc 0.625\n",
            "2022-05-05T21:48:34.187853: step 10257, loss 1.14878, acc 0.625\n",
            "2022-05-05T21:48:34.789540: step 10258, loss 1.31683, acc 0.53125\n",
            "2022-05-05T21:48:35.388809: step 10259, loss 1.3548, acc 0.484375\n",
            "2022-05-05T21:48:36.016271: step 10260, loss 1.36533, acc 0.546875\n",
            "2022-05-05T21:48:36.637479: step 10261, loss 1.28144, acc 0.546875\n",
            "2022-05-05T21:48:37.238404: step 10262, loss 1.45405, acc 0.4375\n",
            "2022-05-05T21:48:37.837750: step 10263, loss 1.29121, acc 0.59375\n",
            "2022-05-05T21:48:38.432869: step 10264, loss 1.34673, acc 0.515625\n",
            "2022-05-05T21:48:39.044125: step 10265, loss 1.18955, acc 0.578125\n",
            "2022-05-05T21:48:39.650131: step 10266, loss 1.28694, acc 0.53125\n",
            "2022-05-05T21:48:40.238522: step 10267, loss 1.23262, acc 0.5\n",
            "2022-05-05T21:48:40.886763: step 10268, loss 1.39916, acc 0.4375\n",
            "2022-05-05T21:48:41.474380: step 10269, loss 1.12806, acc 0.71875\n",
            "2022-05-05T21:48:42.081112: step 10270, loss 1.22463, acc 0.578125\n",
            "2022-05-05T21:48:42.672725: step 10271, loss 1.34691, acc 0.546875\n",
            "2022-05-05T21:48:43.270563: step 10272, loss 1.22982, acc 0.546875\n",
            "2022-05-05T21:48:43.880974: step 10273, loss 1.19484, acc 0.59375\n",
            "2022-05-05T21:48:44.482230: step 10274, loss 1.0908, acc 0.609375\n",
            "2022-05-05T21:48:45.085053: step 10275, loss 1.31288, acc 0.578125\n",
            "2022-05-05T21:48:45.675567: step 10276, loss 1.43441, acc 0.53125\n",
            "2022-05-05T21:48:46.306233: step 10277, loss 1.16247, acc 0.578125\n",
            "2022-05-05T21:48:46.919184: step 10278, loss 1.16224, acc 0.578125\n",
            "2022-05-05T21:48:47.525545: step 10279, loss 1.32778, acc 0.59375\n",
            "2022-05-05T21:48:48.140980: step 10280, loss 1.28394, acc 0.5625\n",
            "2022-05-05T21:48:48.745148: step 10281, loss 1.23825, acc 0.578125\n",
            "2022-05-05T21:48:49.358108: step 10282, loss 1.1065, acc 0.609375\n",
            "2022-05-05T21:48:49.962785: step 10283, loss 1.28083, acc 0.578125\n",
            "2022-05-05T21:48:50.553387: step 10284, loss 1.20309, acc 0.625\n",
            "2022-05-05T21:48:51.163570: step 10285, loss 1.08478, acc 0.640625\n",
            "2022-05-05T21:48:51.759843: step 10286, loss 1.20397, acc 0.53125\n",
            "2022-05-05T21:48:52.373693: step 10287, loss 1.27198, acc 0.484375\n",
            "2022-05-05T21:48:52.968437: step 10288, loss 1.23959, acc 0.578125\n",
            "2022-05-05T21:48:53.568266: step 10289, loss 1.24672, acc 0.53125\n",
            "2022-05-05T21:48:54.181591: step 10290, loss 1.23558, acc 0.53125\n",
            "2022-05-05T21:48:54.770230: step 10291, loss 1.38107, acc 0.546875\n",
            "2022-05-05T21:48:55.378685: step 10292, loss 1.35746, acc 0.5\n",
            "2022-05-05T21:48:55.978946: step 10293, loss 1.18675, acc 0.53125\n",
            "2022-05-05T21:48:56.601005: step 10294, loss 1.46033, acc 0.453125\n",
            "2022-05-05T21:48:57.206624: step 10295, loss 1.20589, acc 0.5625\n",
            "2022-05-05T21:48:57.789281: step 10296, loss 1.24408, acc 0.5625\n",
            "2022-05-05T21:48:58.397024: step 10297, loss 1.40036, acc 0.53125\n",
            "2022-05-05T21:48:58.997472: step 10298, loss 1.40786, acc 0.546875\n",
            "2022-05-05T21:48:59.594487: step 10299, loss 1.44599, acc 0.46875\n",
            "2022-05-05T21:49:00.177862: step 10300, loss 1.38057, acc 0.5625\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:49:00.330408: step 10300, loss 2.17662, acc 0.35\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-10300\n",
            "\n",
            "2022-05-05T21:49:01.113900: step 10301, loss 1.3382, acc 0.578125\n",
            "2022-05-05T21:49:01.712072: step 10302, loss 1.41901, acc 0.484375\n",
            "2022-05-05T21:49:02.316049: step 10303, loss 1.29827, acc 0.59375\n",
            "2022-05-05T21:49:02.914259: step 10304, loss 1.21115, acc 0.53125\n",
            "2022-05-05T21:49:03.516958: step 10305, loss 1.21637, acc 0.59375\n",
            "2022-05-05T21:49:04.101815: step 10306, loss 1.48594, acc 0.5\n",
            "2022-05-05T21:49:04.707328: step 10307, loss 1.03772, acc 0.640625\n",
            "2022-05-05T21:49:05.292465: step 10308, loss 1.29326, acc 0.515625\n",
            "2022-05-05T21:49:05.890692: step 10309, loss 1.41209, acc 0.5\n",
            "2022-05-05T21:49:06.481692: step 10310, loss 1.35614, acc 0.46875\n",
            "2022-05-05T21:49:07.096880: step 10311, loss 1.33641, acc 0.5\n",
            "2022-05-05T21:49:07.695356: step 10312, loss 1.30224, acc 0.484375\n",
            "2022-05-05T21:49:08.270392: step 10313, loss 1.63956, acc 0.421875\n",
            "2022-05-05T21:49:08.868218: step 10314, loss 1.28711, acc 0.515625\n",
            "2022-05-05T21:49:09.462810: step 10315, loss 1.23071, acc 0.5625\n",
            "2022-05-05T21:49:10.056176: step 10316, loss 1.18854, acc 0.578125\n",
            "2022-05-05T21:49:10.651930: step 10317, loss 1.48052, acc 0.4375\n",
            "2022-05-05T21:49:11.242256: step 10318, loss 1.21149, acc 0.609375\n",
            "2022-05-05T21:49:11.853264: step 10319, loss 1.64341, acc 0.375\n",
            "2022-05-05T21:49:12.445342: step 10320, loss 1.23315, acc 0.546875\n",
            "2022-05-05T21:49:13.070275: step 10321, loss 1.24456, acc 0.609375\n",
            "2022-05-05T21:49:13.708522: step 10322, loss 1.35899, acc 0.5625\n",
            "2022-05-05T21:49:14.309785: step 10323, loss 1.4579, acc 0.453125\n",
            "2022-05-05T21:49:14.910301: step 10324, loss 1.13052, acc 0.625\n",
            "2022-05-05T21:49:15.500585: step 10325, loss 1.55728, acc 0.4375\n",
            "2022-05-05T21:49:16.110961: step 10326, loss 1.40536, acc 0.578125\n",
            "2022-05-05T21:49:16.722111: step 10327, loss 1.47149, acc 0.5\n",
            "2022-05-05T21:49:17.343808: step 10328, loss 1.21816, acc 0.5625\n",
            "2022-05-05T21:49:17.959479: step 10329, loss 1.45217, acc 0.46875\n",
            "2022-05-05T21:49:18.582286: step 10330, loss 1.32368, acc 0.5\n",
            "2022-05-05T21:49:19.199077: step 10331, loss 1.1474, acc 0.5625\n",
            "2022-05-05T21:49:19.797208: step 10332, loss 1.36999, acc 0.484375\n",
            "2022-05-05T21:49:20.410635: step 10333, loss 1.32806, acc 0.5625\n",
            "2022-05-05T21:49:21.016387: step 10334, loss 1.31744, acc 0.46875\n",
            "2022-05-05T21:49:21.619409: step 10335, loss 1.53147, acc 0.4375\n",
            "2022-05-05T21:49:22.228259: step 10336, loss 1.44142, acc 0.5\n",
            "2022-05-05T21:49:22.816525: step 10337, loss 1.32549, acc 0.53125\n",
            "2022-05-05T21:49:23.408247: step 10338, loss 1.26908, acc 0.53125\n",
            "2022-05-05T21:49:24.004827: step 10339, loss 1.47511, acc 0.515625\n",
            "2022-05-05T21:49:24.603630: step 10340, loss 1.35847, acc 0.625\n",
            "2022-05-05T21:49:25.215972: step 10341, loss 1.28465, acc 0.546875\n",
            "2022-05-05T21:49:25.801691: step 10342, loss 1.13386, acc 0.578125\n",
            "2022-05-05T21:49:26.420658: step 10343, loss 1.2216, acc 0.59375\n",
            "2022-05-05T21:49:27.015509: step 10344, loss 1.42365, acc 0.546875\n",
            "2022-05-05T21:49:27.657680: step 10345, loss 1.39073, acc 0.53125\n",
            "2022-05-05T21:49:28.248490: step 10346, loss 1.47039, acc 0.46875\n",
            "2022-05-05T21:49:28.843823: step 10347, loss 1.19512, acc 0.578125\n",
            "2022-05-05T21:49:29.442842: step 10348, loss 1.21194, acc 0.546875\n",
            "2022-05-05T21:49:30.062297: step 10349, loss 1.32076, acc 0.46875\n",
            "2022-05-05T21:49:30.662566: step 10350, loss 1.39421, acc 0.46875\n",
            "2022-05-05T21:49:31.276746: step 10351, loss 1.29252, acc 0.59375\n",
            "2022-05-05T21:49:31.868980: step 10352, loss 1.23677, acc 0.625\n",
            "2022-05-05T21:49:32.482824: step 10353, loss 1.44273, acc 0.484375\n",
            "2022-05-05T21:49:33.077188: step 10354, loss 1.55303, acc 0.328125\n",
            "2022-05-05T21:49:33.675586: step 10355, loss 1.27444, acc 0.578125\n",
            "2022-05-05T21:49:34.271149: step 10356, loss 1.40651, acc 0.5\n",
            "2022-05-05T21:49:34.871810: step 10357, loss 1.32787, acc 0.5625\n",
            "2022-05-05T21:49:35.477961: step 10358, loss 1.30159, acc 0.5625\n",
            "2022-05-05T21:49:36.074619: step 10359, loss 1.32418, acc 0.484375\n",
            "2022-05-05T21:49:36.680941: step 10360, loss 1.43442, acc 0.4375\n",
            "2022-05-05T21:49:37.287286: step 10361, loss 1.30131, acc 0.5625\n",
            "2022-05-05T21:49:37.918816: step 10362, loss 1.40613, acc 0.515625\n",
            "2022-05-05T21:49:38.531980: step 10363, loss 1.34501, acc 0.484375\n",
            "2022-05-05T21:49:39.129590: step 10364, loss 1.50527, acc 0.484375\n",
            "2022-05-05T21:49:39.735518: step 10365, loss 1.17911, acc 0.578125\n",
            "2022-05-05T21:49:40.332526: step 10366, loss 1.34978, acc 0.4375\n",
            "2022-05-05T21:49:40.947115: step 10367, loss 1.31155, acc 0.578125\n",
            "2022-05-05T21:49:41.554515: step 10368, loss 1.50791, acc 0.484375\n",
            "2022-05-05T21:49:42.150840: step 10369, loss 1.34656, acc 0.515625\n",
            "2022-05-05T21:49:42.752951: step 10370, loss 1.21944, acc 0.546875\n",
            "2022-05-05T21:49:43.337225: step 10371, loss 1.08772, acc 0.625\n",
            "2022-05-05T21:49:43.939340: step 10372, loss 1.21545, acc 0.546875\n",
            "2022-05-05T21:49:44.522479: step 10373, loss 1.53401, acc 0.390625\n",
            "2022-05-05T21:49:45.126949: step 10374, loss 1.30638, acc 0.515625\n",
            "2022-05-05T21:49:45.721478: step 10375, loss 1.46155, acc 0.453125\n",
            "2022-05-05T21:49:46.305661: step 10376, loss 1.41414, acc 0.5\n",
            "2022-05-05T21:49:46.911505: step 10377, loss 1.18124, acc 0.625\n",
            "2022-05-05T21:49:47.492293: step 10378, loss 1.30303, acc 0.59375\n",
            "2022-05-05T21:49:48.115958: step 10379, loss 1.23389, acc 0.53125\n",
            "2022-05-05T21:49:48.723096: step 10380, loss 1.39013, acc 0.515625\n",
            "2022-05-05T21:49:49.309506: step 10381, loss 1.16625, acc 0.59375\n",
            "2022-05-05T21:49:49.916560: step 10382, loss 1.36252, acc 0.515625\n",
            "2022-05-05T21:49:50.500911: step 10383, loss 1.38319, acc 0.46875\n",
            "2022-05-05T21:49:51.098717: step 10384, loss 1.22358, acc 0.5625\n",
            "2022-05-05T21:49:51.690681: step 10385, loss 1.13571, acc 0.5625\n",
            "2022-05-05T21:49:52.294800: step 10386, loss 1.30184, acc 0.515625\n",
            "2022-05-05T21:49:52.900181: step 10387, loss 1.37902, acc 0.484375\n",
            "2022-05-05T21:49:53.487647: step 10388, loss 1.1665, acc 0.546875\n",
            "2022-05-05T21:49:54.090501: step 10389, loss 1.49034, acc 0.453125\n",
            "2022-05-05T21:49:54.679424: step 10390, loss 1.25809, acc 0.484375\n",
            "2022-05-05T21:49:55.288804: step 10391, loss 1.19692, acc 0.59375\n",
            "2022-05-05T21:49:55.889365: step 10392, loss 1.28496, acc 0.5625\n",
            "2022-05-05T21:49:56.498141: step 10393, loss 1.30504, acc 0.53125\n",
            "2022-05-05T21:49:57.112718: step 10394, loss 1.14743, acc 0.546875\n",
            "2022-05-05T21:49:57.716732: step 10395, loss 1.37864, acc 0.5625\n",
            "2022-05-05T21:49:58.320870: step 10396, loss 1.25763, acc 0.5625\n",
            "2022-05-05T21:49:58.933885: step 10397, loss 1.24346, acc 0.578125\n",
            "2022-05-05T21:49:59.533534: step 10398, loss 1.18291, acc 0.546875\n",
            "2022-05-05T21:50:00.136080: step 10399, loss 1.42596, acc 0.453125\n",
            "2022-05-05T21:50:00.730803: step 10400, loss 1.18928, acc 0.59375\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:50:00.878213: step 10400, loss 2.04811, acc 0.33\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-10400\n",
            "\n",
            "2022-05-05T21:50:01.662685: step 10401, loss 1.28116, acc 0.546875\n",
            "2022-05-05T21:50:02.270879: step 10402, loss 1.31458, acc 0.515625\n",
            "2022-05-05T21:50:02.870655: step 10403, loss 1.33848, acc 0.515625\n",
            "2022-05-05T21:50:03.369337: step 10404, loss 1.38231, acc 0.489796\n",
            "2022-05-05T21:50:03.970227: step 10405, loss 1.12698, acc 0.671875\n",
            "2022-05-05T21:50:04.577176: step 10406, loss 1.11414, acc 0.578125\n",
            "2022-05-05T21:50:05.208833: step 10407, loss 1.08668, acc 0.640625\n",
            "2022-05-05T21:50:05.818319: step 10408, loss 1.22801, acc 0.625\n",
            "2022-05-05T21:50:06.420466: step 10409, loss 1.14919, acc 0.578125\n",
            "2022-05-05T21:50:07.017595: step 10410, loss 1.06442, acc 0.640625\n",
            "2022-05-05T21:50:07.618042: step 10411, loss 1.16707, acc 0.5625\n",
            "2022-05-05T21:50:08.210298: step 10412, loss 1.10864, acc 0.53125\n",
            "2022-05-05T21:50:08.842728: step 10413, loss 1.25654, acc 0.5625\n",
            "2022-05-05T21:50:09.455511: step 10414, loss 1.25595, acc 0.609375\n",
            "2022-05-05T21:50:10.050899: step 10415, loss 1.49283, acc 0.40625\n",
            "2022-05-05T21:50:10.681435: step 10416, loss 1.40961, acc 0.5\n",
            "2022-05-05T21:50:11.292364: step 10417, loss 1.35524, acc 0.59375\n",
            "2022-05-05T21:50:11.909715: step 10418, loss 1.3213, acc 0.53125\n",
            "2022-05-05T21:50:12.511767: step 10419, loss 1.17563, acc 0.609375\n",
            "2022-05-05T21:50:13.119618: step 10420, loss 1.14769, acc 0.640625\n",
            "2022-05-05T21:50:13.743218: step 10421, loss 1.23574, acc 0.5625\n",
            "2022-05-05T21:50:14.331942: step 10422, loss 1.10071, acc 0.59375\n",
            "2022-05-05T21:50:14.938818: step 10423, loss 1.22549, acc 0.53125\n",
            "2022-05-05T21:50:15.543982: step 10424, loss 1.31642, acc 0.5\n",
            "2022-05-05T21:50:16.137768: step 10425, loss 1.30263, acc 0.53125\n",
            "2022-05-05T21:50:16.751597: step 10426, loss 1.22022, acc 0.546875\n",
            "2022-05-05T21:50:17.348531: step 10427, loss 1.21753, acc 0.578125\n",
            "2022-05-05T21:50:17.960376: step 10428, loss 1.24548, acc 0.578125\n",
            "2022-05-05T21:50:18.547719: step 10429, loss 1.32132, acc 0.578125\n",
            "2022-05-05T21:50:19.178536: step 10430, loss 1.06256, acc 0.6875\n",
            "2022-05-05T21:50:19.781750: step 10431, loss 1.35303, acc 0.515625\n",
            "2022-05-05T21:50:20.383938: step 10432, loss 1.08858, acc 0.640625\n",
            "2022-05-05T21:50:21.076825: step 10433, loss 1.13696, acc 0.578125\n",
            "2022-05-05T21:50:21.894096: step 10434, loss 1.11964, acc 0.5625\n",
            "2022-05-05T21:50:22.513085: step 10435, loss 1.27106, acc 0.546875\n",
            "2022-05-05T21:50:23.128734: step 10436, loss 1.45626, acc 0.375\n",
            "2022-05-05T21:50:23.717771: step 10437, loss 1.15366, acc 0.625\n",
            "2022-05-05T21:50:24.317897: step 10438, loss 1.29662, acc 0.5\n",
            "2022-05-05T21:50:24.925377: step 10439, loss 1.17824, acc 0.609375\n",
            "2022-05-05T21:50:25.513122: step 10440, loss 1.34495, acc 0.484375\n",
            "2022-05-05T21:50:26.113752: step 10441, loss 1.30166, acc 0.5\n",
            "2022-05-05T21:50:26.726593: step 10442, loss 1.01385, acc 0.65625\n",
            "2022-05-05T21:50:27.334797: step 10443, loss 1.13725, acc 0.578125\n",
            "2022-05-05T21:50:27.932448: step 10444, loss 1.02508, acc 0.578125\n",
            "2022-05-05T21:50:28.533805: step 10445, loss 1.28808, acc 0.640625\n",
            "2022-05-05T21:50:29.146176: step 10446, loss 1.06972, acc 0.640625\n",
            "2022-05-05T21:50:29.770512: step 10447, loss 1.2616, acc 0.546875\n",
            "2022-05-05T21:50:30.381579: step 10448, loss 1.01304, acc 0.703125\n",
            "2022-05-05T21:50:30.977436: step 10449, loss 1.15204, acc 0.578125\n",
            "2022-05-05T21:50:31.592078: step 10450, loss 1.36747, acc 0.453125\n",
            "2022-05-05T21:50:32.197958: step 10451, loss 1.23832, acc 0.515625\n",
            "2022-05-05T21:50:32.798890: step 10452, loss 1.30945, acc 0.484375\n",
            "2022-05-05T21:50:33.409196: step 10453, loss 1.34368, acc 0.515625\n",
            "2022-05-05T21:50:34.010342: step 10454, loss 1.26151, acc 0.5625\n",
            "2022-05-05T21:50:34.613565: step 10455, loss 1.05909, acc 0.640625\n",
            "2022-05-05T21:50:35.217494: step 10456, loss 1.48138, acc 0.453125\n",
            "2022-05-05T21:50:35.813217: step 10457, loss 1.34094, acc 0.484375\n",
            "2022-05-05T21:50:36.421405: step 10458, loss 1.12747, acc 0.625\n",
            "2022-05-05T21:50:37.019937: step 10459, loss 1.0298, acc 0.546875\n",
            "2022-05-05T21:50:37.655280: step 10460, loss 1.27085, acc 0.546875\n",
            "2022-05-05T21:50:38.278452: step 10461, loss 1.25073, acc 0.546875\n",
            "2022-05-05T21:50:38.885087: step 10462, loss 1.15322, acc 0.65625\n",
            "2022-05-05T21:50:39.520933: step 10463, loss 1.16463, acc 0.640625\n",
            "2022-05-05T21:50:40.168718: step 10464, loss 1.01968, acc 0.6875\n",
            "2022-05-05T21:50:40.779727: step 10465, loss 1.42625, acc 0.5625\n",
            "2022-05-05T21:50:41.429096: step 10466, loss 1.41004, acc 0.453125\n",
            "2022-05-05T21:50:42.063632: step 10467, loss 1.37217, acc 0.546875\n",
            "2022-05-05T21:50:42.681935: step 10468, loss 0.880019, acc 0.734375\n",
            "2022-05-05T21:50:43.282122: step 10469, loss 1.28418, acc 0.515625\n",
            "2022-05-05T21:50:43.902369: step 10470, loss 1.27223, acc 0.5625\n",
            "2022-05-05T21:50:44.527595: step 10471, loss 1.03162, acc 0.5625\n",
            "2022-05-05T21:50:45.148387: step 10472, loss 1.15785, acc 0.59375\n",
            "2022-05-05T21:50:45.767008: step 10473, loss 1.1648, acc 0.59375\n",
            "2022-05-05T21:50:46.376220: step 10474, loss 1.18685, acc 0.5625\n",
            "2022-05-05T21:50:46.999339: step 10475, loss 1.18216, acc 0.59375\n",
            "2022-05-05T21:50:47.623960: step 10476, loss 1.04942, acc 0.671875\n",
            "2022-05-05T21:50:48.242597: step 10477, loss 1.19873, acc 0.59375\n",
            "2022-05-05T21:50:48.862727: step 10478, loss 1.2885, acc 0.5625\n",
            "2022-05-05T21:50:49.520478: step 10479, loss 1.34116, acc 0.53125\n",
            "2022-05-05T21:50:50.197880: step 10480, loss 1.33107, acc 0.625\n",
            "2022-05-05T21:50:50.822133: step 10481, loss 1.23057, acc 0.625\n",
            "2022-05-05T21:50:51.476768: step 10482, loss 1.29318, acc 0.5\n",
            "2022-05-05T21:50:52.120167: step 10483, loss 1.25256, acc 0.59375\n",
            "2022-05-05T21:50:52.749885: step 10484, loss 1.11592, acc 0.625\n",
            "2022-05-05T21:50:53.369621: step 10485, loss 1.24113, acc 0.609375\n",
            "2022-05-05T21:50:54.000322: step 10486, loss 1.25614, acc 0.515625\n",
            "2022-05-05T21:50:54.625681: step 10487, loss 1.16101, acc 0.578125\n",
            "2022-05-05T21:50:55.278690: step 10488, loss 1.23519, acc 0.578125\n",
            "2022-05-05T21:50:55.909543: step 10489, loss 1.34171, acc 0.484375\n",
            "2022-05-05T21:50:56.509610: step 10490, loss 1.17119, acc 0.59375\n",
            "2022-05-05T21:50:57.132928: step 10491, loss 0.887933, acc 0.734375\n",
            "2022-05-05T21:50:57.759183: step 10492, loss 1.23958, acc 0.59375\n",
            "2022-05-05T21:50:58.392272: step 10493, loss 1.16715, acc 0.578125\n",
            "2022-05-05T21:50:59.031658: step 10494, loss 1.25408, acc 0.640625\n",
            "2022-05-05T21:50:59.666978: step 10495, loss 1.2463, acc 0.546875\n",
            "2022-05-05T21:51:00.316887: step 10496, loss 1.28818, acc 0.59375\n",
            "2022-05-05T21:51:00.963476: step 10497, loss 1.28723, acc 0.5625\n",
            "2022-05-05T21:51:01.582105: step 10498, loss 1.11528, acc 0.609375\n",
            "2022-05-05T21:51:02.202071: step 10499, loss 1.25085, acc 0.578125\n",
            "2022-05-05T21:51:02.826366: step 10500, loss 0.869977, acc 0.65625\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:51:02.978654: step 10500, loss 2.08281, acc 0.34\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-10500\n",
            "\n",
            "2022-05-05T21:51:03.837667: step 10501, loss 1.06506, acc 0.59375\n",
            "2022-05-05T21:51:04.485471: step 10502, loss 1.11726, acc 0.625\n",
            "2022-05-05T21:51:05.136338: step 10503, loss 1.3715, acc 0.578125\n",
            "2022-05-05T21:51:05.773483: step 10504, loss 1.26302, acc 0.484375\n",
            "2022-05-05T21:51:06.409818: step 10505, loss 1.2737, acc 0.53125\n",
            "2022-05-05T21:51:07.036260: step 10506, loss 1.25821, acc 0.53125\n",
            "2022-05-05T21:51:07.687781: step 10507, loss 1.21327, acc 0.515625\n",
            "2022-05-05T21:51:08.324564: step 10508, loss 1.17699, acc 0.609375\n",
            "2022-05-05T21:51:08.946927: step 10509, loss 1.50471, acc 0.515625\n",
            "2022-05-05T21:51:09.581641: step 10510, loss 1.50055, acc 0.5\n",
            "2022-05-05T21:51:10.192933: step 10511, loss 0.987224, acc 0.671875\n",
            "2022-05-05T21:51:10.835834: step 10512, loss 1.2259, acc 0.5625\n",
            "2022-05-05T21:51:11.481061: step 10513, loss 1.13424, acc 0.578125\n",
            "2022-05-05T21:51:12.089548: step 10514, loss 1.128, acc 0.578125\n",
            "2022-05-05T21:51:12.730354: step 10515, loss 1.2136, acc 0.5625\n",
            "2022-05-05T21:51:13.356786: step 10516, loss 1.16587, acc 0.546875\n",
            "2022-05-05T21:51:13.971079: step 10517, loss 1.41338, acc 0.515625\n",
            "2022-05-05T21:51:14.588886: step 10518, loss 1.19791, acc 0.578125\n",
            "2022-05-05T21:51:15.205717: step 10519, loss 1.14567, acc 0.59375\n",
            "2022-05-05T21:51:15.824238: step 10520, loss 1.14671, acc 0.609375\n",
            "2022-05-05T21:51:16.452527: step 10521, loss 1.24306, acc 0.5625\n",
            "2022-05-05T21:51:17.064346: step 10522, loss 1.22478, acc 0.578125\n",
            "2022-05-05T21:51:17.692023: step 10523, loss 1.24351, acc 0.53125\n",
            "2022-05-05T21:51:18.303391: step 10524, loss 1.02815, acc 0.65625\n",
            "2022-05-05T21:51:18.930886: step 10525, loss 1.41044, acc 0.5625\n",
            "2022-05-05T21:51:19.557023: step 10526, loss 1.24367, acc 0.59375\n",
            "2022-05-05T21:51:20.173911: step 10527, loss 1.11946, acc 0.625\n",
            "2022-05-05T21:51:20.812571: step 10528, loss 0.955343, acc 0.6875\n",
            "2022-05-05T21:51:21.462065: step 10529, loss 1.23521, acc 0.59375\n",
            "2022-05-05T21:51:22.092895: step 10530, loss 1.24562, acc 0.578125\n",
            "2022-05-05T21:51:22.710348: step 10531, loss 1.31507, acc 0.59375\n",
            "2022-05-05T21:51:23.324384: step 10532, loss 1.34215, acc 0.46875\n",
            "2022-05-05T21:51:23.937838: step 10533, loss 1.32387, acc 0.484375\n",
            "2022-05-05T21:51:24.562741: step 10534, loss 1.13388, acc 0.625\n",
            "2022-05-05T21:51:25.195448: step 10535, loss 1.18438, acc 0.625\n",
            "2022-05-05T21:51:25.819233: step 10536, loss 1.11206, acc 0.6875\n",
            "2022-05-05T21:51:26.441477: step 10537, loss 1.25861, acc 0.515625\n",
            "2022-05-05T21:51:27.060725: step 10538, loss 1.24888, acc 0.59375\n",
            "2022-05-05T21:51:27.678029: step 10539, loss 1.20055, acc 0.5625\n",
            "2022-05-05T21:51:28.296899: step 10540, loss 1.21599, acc 0.578125\n",
            "2022-05-05T21:51:28.929940: step 10541, loss 1.28651, acc 0.546875\n",
            "2022-05-05T21:51:29.550338: step 10542, loss 1.27839, acc 0.5625\n",
            "2022-05-05T21:51:30.189052: step 10543, loss 1.39853, acc 0.546875\n",
            "2022-05-05T21:51:30.801799: step 10544, loss 1.25886, acc 0.5625\n",
            "2022-05-05T21:51:31.447887: step 10545, loss 1.25985, acc 0.53125\n",
            "2022-05-05T21:51:32.070091: step 10546, loss 1.21204, acc 0.453125\n",
            "2022-05-05T21:51:32.696247: step 10547, loss 1.20776, acc 0.59375\n",
            "2022-05-05T21:51:33.326539: step 10548, loss 1.15406, acc 0.609375\n",
            "2022-05-05T21:51:33.935694: step 10549, loss 1.32358, acc 0.578125\n",
            "2022-05-05T21:51:34.556074: step 10550, loss 1.33392, acc 0.46875\n",
            "2022-05-05T21:51:35.178365: step 10551, loss 1.27296, acc 0.484375\n",
            "2022-05-05T21:51:35.793532: step 10552, loss 1.34379, acc 0.515625\n",
            "2022-05-05T21:51:36.413749: step 10553, loss 1.2362, acc 0.4375\n",
            "2022-05-05T21:51:37.024849: step 10554, loss 0.929232, acc 0.734375\n",
            "2022-05-05T21:51:37.649821: step 10555, loss 0.971736, acc 0.703125\n",
            "2022-05-05T21:51:38.268634: step 10556, loss 1.26689, acc 0.546875\n",
            "2022-05-05T21:51:38.875798: step 10557, loss 1.13377, acc 0.625\n",
            "2022-05-05T21:51:39.501796: step 10558, loss 1.28463, acc 0.515625\n",
            "2022-05-05T21:51:40.104168: step 10559, loss 1.23651, acc 0.515625\n",
            "2022-05-05T21:51:40.721018: step 10560, loss 1.45896, acc 0.453125\n",
            "2022-05-05T21:51:41.356534: step 10561, loss 1.18733, acc 0.578125\n",
            "2022-05-05T21:51:41.980752: step 10562, loss 1.38271, acc 0.484375\n",
            "2022-05-05T21:51:42.608789: step 10563, loss 1.05475, acc 0.640625\n",
            "2022-05-05T21:51:43.215497: step 10564, loss 1.37491, acc 0.515625\n",
            "2022-05-05T21:51:43.841401: step 10565, loss 1.19086, acc 0.546875\n",
            "2022-05-05T21:51:44.468517: step 10566, loss 1.22642, acc 0.6875\n",
            "2022-05-05T21:51:45.075972: step 10567, loss 1.28147, acc 0.515625\n",
            "2022-05-05T21:51:45.708525: step 10568, loss 1.19071, acc 0.5625\n",
            "2022-05-05T21:51:46.319967: step 10569, loss 1.29562, acc 0.53125\n",
            "2022-05-05T21:51:46.938537: step 10570, loss 1.47601, acc 0.421875\n",
            "2022-05-05T21:51:47.552612: step 10571, loss 1.26723, acc 0.5\n",
            "2022-05-05T21:51:48.158755: step 10572, loss 1.13615, acc 0.578125\n",
            "2022-05-05T21:51:48.794322: step 10573, loss 1.25671, acc 0.546875\n",
            "2022-05-05T21:51:49.400425: step 10574, loss 1.22073, acc 0.578125\n",
            "2022-05-05T21:51:50.026656: step 10575, loss 1.22398, acc 0.5\n",
            "2022-05-05T21:51:50.650806: step 10576, loss 1.20468, acc 0.5625\n",
            "2022-05-05T21:51:51.256459: step 10577, loss 1.13467, acc 0.578125\n",
            "2022-05-05T21:51:51.875131: step 10578, loss 1.31906, acc 0.578125\n",
            "2022-05-05T21:51:52.512459: step 10579, loss 1.32012, acc 0.515625\n",
            "2022-05-05T21:51:53.147101: step 10580, loss 1.01599, acc 0.71875\n",
            "2022-05-05T21:51:53.758129: step 10581, loss 1.16807, acc 0.609375\n",
            "2022-05-05T21:51:54.369593: step 10582, loss 1.2381, acc 0.53125\n",
            "2022-05-05T21:51:54.986818: step 10583, loss 1.18705, acc 0.703125\n",
            "2022-05-05T21:51:55.597722: step 10584, loss 1.24356, acc 0.59375\n",
            "2022-05-05T21:51:56.213829: step 10585, loss 1.13537, acc 0.546875\n",
            "2022-05-05T21:51:56.832082: step 10586, loss 1.1557, acc 0.5625\n",
            "2022-05-05T21:51:57.439733: step 10587, loss 1.46006, acc 0.515625\n",
            "2022-05-05T21:51:58.050107: step 10588, loss 1.29887, acc 0.59375\n",
            "2022-05-05T21:51:58.658566: step 10589, loss 1.15752, acc 0.53125\n",
            "2022-05-05T21:51:59.282755: step 10590, loss 1.15725, acc 0.625\n",
            "2022-05-05T21:51:59.906087: step 10591, loss 1.30266, acc 0.5625\n",
            "2022-05-05T21:52:00.509026: step 10592, loss 1.24368, acc 0.609375\n",
            "2022-05-05T21:52:01.129614: step 10593, loss 1.27104, acc 0.578125\n",
            "2022-05-05T21:52:01.741237: step 10594, loss 1.1253, acc 0.640625\n",
            "2022-05-05T21:52:02.373333: step 10595, loss 1.20015, acc 0.53125\n",
            "2022-05-05T21:52:03.019184: step 10596, loss 1.13101, acc 0.578125\n",
            "2022-05-05T21:52:03.632375: step 10597, loss 1.35732, acc 0.5625\n",
            "2022-05-05T21:52:04.259519: step 10598, loss 1.21165, acc 0.609375\n",
            "2022-05-05T21:52:04.871967: step 10599, loss 1.10512, acc 0.609375\n",
            "2022-05-05T21:52:05.485698: step 10600, loss 1.08581, acc 0.578125\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:52:05.630150: step 10600, loss 2.10854, acc 0.31\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-10600\n",
            "\n",
            "2022-05-05T21:52:06.449945: step 10601, loss 1.34397, acc 0.484375\n",
            "2022-05-05T21:52:07.069452: step 10602, loss 1.23441, acc 0.515625\n",
            "2022-05-05T21:52:07.676559: step 10603, loss 1.18252, acc 0.609375\n",
            "2022-05-05T21:52:08.295805: step 10604, loss 1.25501, acc 0.546875\n",
            "2022-05-05T21:52:08.907807: step 10605, loss 1.15159, acc 0.65625\n",
            "2022-05-05T21:52:09.539852: step 10606, loss 1.34342, acc 0.5625\n",
            "2022-05-05T21:52:10.151830: step 10607, loss 1.13554, acc 0.59375\n",
            "2022-05-05T21:52:10.762533: step 10608, loss 1.24944, acc 0.5625\n",
            "2022-05-05T21:52:11.388485: step 10609, loss 1.221, acc 0.625\n",
            "2022-05-05T21:52:12.007404: step 10610, loss 1.18783, acc 0.59375\n",
            "2022-05-05T21:52:12.634218: step 10611, loss 1.23981, acc 0.625\n",
            "2022-05-05T21:52:13.280152: step 10612, loss 1.1694, acc 0.515625\n",
            "2022-05-05T21:52:13.899234: step 10613, loss 1.29913, acc 0.484375\n",
            "2022-05-05T21:52:14.550716: step 10614, loss 1.03696, acc 0.65625\n",
            "2022-05-05T21:52:15.162392: step 10615, loss 1.411, acc 0.5\n",
            "2022-05-05T21:52:15.783067: step 10616, loss 1.37218, acc 0.578125\n",
            "2022-05-05T21:52:16.409712: step 10617, loss 1.13245, acc 0.65625\n",
            "2022-05-05T21:52:17.021710: step 10618, loss 1.3004, acc 0.546875\n",
            "2022-05-05T21:52:17.646308: step 10619, loss 1.37773, acc 0.53125\n",
            "2022-05-05T21:52:18.243563: step 10620, loss 1.21776, acc 0.5625\n",
            "2022-05-05T21:52:18.869559: step 10621, loss 1.38173, acc 0.484375\n",
            "2022-05-05T21:52:19.488729: step 10622, loss 1.22898, acc 0.59375\n",
            "2022-05-05T21:52:20.129605: step 10623, loss 1.12156, acc 0.625\n",
            "2022-05-05T21:52:20.746098: step 10624, loss 1.0358, acc 0.578125\n",
            "2022-05-05T21:52:21.356552: step 10625, loss 1.2621, acc 0.546875\n",
            "2022-05-05T21:52:21.982979: step 10626, loss 1.36274, acc 0.484375\n",
            "2022-05-05T21:52:22.614558: step 10627, loss 1.12627, acc 0.59375\n",
            "2022-05-05T21:52:23.261306: step 10628, loss 1.29833, acc 0.59375\n",
            "2022-05-05T21:52:23.881602: step 10629, loss 1.31321, acc 0.515625\n",
            "2022-05-05T21:52:24.492899: step 10630, loss 1.33685, acc 0.453125\n",
            "2022-05-05T21:52:25.128694: step 10631, loss 1.21475, acc 0.53125\n",
            "2022-05-05T21:52:25.753784: step 10632, loss 1.23591, acc 0.546875\n",
            "2022-05-05T21:52:26.373188: step 10633, loss 1.29132, acc 0.53125\n",
            "2022-05-05T21:52:27.003189: step 10634, loss 1.24992, acc 0.578125\n",
            "2022-05-05T21:52:27.619885: step 10635, loss 1.20756, acc 0.578125\n",
            "2022-05-05T21:52:28.243548: step 10636, loss 1.2456, acc 0.53125\n",
            "2022-05-05T21:52:28.866310: step 10637, loss 1.25229, acc 0.5\n",
            "2022-05-05T21:52:29.488568: step 10638, loss 1.28798, acc 0.53125\n",
            "2022-05-05T21:52:30.111181: step 10639, loss 1.19207, acc 0.59375\n",
            "2022-05-05T21:52:30.720533: step 10640, loss 1.24245, acc 0.5\n",
            "2022-05-05T21:52:31.356364: step 10641, loss 1.26902, acc 0.625\n",
            "2022-05-05T21:52:31.990414: step 10642, loss 1.41414, acc 0.546875\n",
            "2022-05-05T21:52:32.610786: step 10643, loss 1.13049, acc 0.59375\n",
            "2022-05-05T21:52:33.249297: step 10644, loss 1.10782, acc 0.5625\n",
            "2022-05-05T21:52:33.895339: step 10645, loss 1.23088, acc 0.515625\n",
            "2022-05-05T21:52:34.517113: step 10646, loss 1.09697, acc 0.625\n",
            "2022-05-05T21:52:35.157653: step 10647, loss 1.11968, acc 0.625\n",
            "2022-05-05T21:52:35.775430: step 10648, loss 1.37435, acc 0.578125\n",
            "2022-05-05T21:52:36.400391: step 10649, loss 1.46402, acc 0.484375\n",
            "2022-05-05T21:52:37.036970: step 10650, loss 1.07241, acc 0.609375\n",
            "2022-05-05T21:52:37.660413: step 10651, loss 1.44237, acc 0.453125\n",
            "2022-05-05T21:52:38.291594: step 10652, loss 1.23799, acc 0.578125\n",
            "2022-05-05T21:52:38.900604: step 10653, loss 1.4496, acc 0.453125\n",
            "2022-05-05T21:52:39.538336: step 10654, loss 1.17859, acc 0.578125\n",
            "2022-05-05T21:52:40.168870: step 10655, loss 1.13171, acc 0.59375\n",
            "2022-05-05T21:52:40.782850: step 10656, loss 1.14911, acc 0.546875\n",
            "2022-05-05T21:52:41.418375: step 10657, loss 1.23536, acc 0.515625\n",
            "2022-05-05T21:52:42.029864: step 10658, loss 1.35773, acc 0.578125\n",
            "2022-05-05T21:52:42.647652: step 10659, loss 1.26383, acc 0.5\n",
            "2022-05-05T21:52:43.264442: step 10660, loss 1.26591, acc 0.484375\n",
            "2022-05-05T21:52:43.907614: step 10661, loss 1.18031, acc 0.515625\n",
            "2022-05-05T21:52:44.534043: step 10662, loss 1.25225, acc 0.609375\n",
            "2022-05-05T21:52:45.141969: step 10663, loss 1.35378, acc 0.5\n",
            "2022-05-05T21:52:45.777580: step 10664, loss 1.33588, acc 0.515625\n",
            "2022-05-05T21:52:46.401607: step 10665, loss 1.2501, acc 0.578125\n",
            "2022-05-05T21:52:47.016180: step 10666, loss 1.18773, acc 0.5625\n",
            "2022-05-05T21:52:47.640974: step 10667, loss 1.01214, acc 0.65625\n",
            "2022-05-05T21:52:48.245095: step 10668, loss 1.26828, acc 0.546875\n",
            "2022-05-05T21:52:48.864846: step 10669, loss 1.37472, acc 0.453125\n",
            "2022-05-05T21:52:49.484351: step 10670, loss 1.19426, acc 0.625\n",
            "2022-05-05T21:52:50.088003: step 10671, loss 1.19548, acc 0.5625\n",
            "2022-05-05T21:52:50.710816: step 10672, loss 1.44841, acc 0.578125\n",
            "2022-05-05T21:52:51.336275: step 10673, loss 1.05227, acc 0.6875\n",
            "2022-05-05T21:52:51.964429: step 10674, loss 1.25484, acc 0.546875\n",
            "2022-05-05T21:52:52.591868: step 10675, loss 1.55362, acc 0.46875\n",
            "2022-05-05T21:52:53.210473: step 10676, loss 1.12157, acc 0.6875\n",
            "2022-05-05T21:52:53.838027: step 10677, loss 1.27661, acc 0.59375\n",
            "2022-05-05T21:52:54.483499: step 10678, loss 1.06432, acc 0.640625\n",
            "2022-05-05T21:52:55.115282: step 10679, loss 1.44682, acc 0.484375\n",
            "2022-05-05T21:52:55.742250: step 10680, loss 1.08305, acc 0.5625\n",
            "2022-05-05T21:52:56.356415: step 10681, loss 1.41554, acc 0.5\n",
            "2022-05-05T21:52:56.980217: step 10682, loss 1.28953, acc 0.578125\n",
            "2022-05-05T21:52:57.591242: step 10683, loss 1.42078, acc 0.5\n",
            "2022-05-05T21:52:58.219426: step 10684, loss 1.2368, acc 0.5\n",
            "2022-05-05T21:52:58.845348: step 10685, loss 1.44874, acc 0.421875\n",
            "2022-05-05T21:52:59.468179: step 10686, loss 1.24412, acc 0.546875\n",
            "2022-05-05T21:53:00.102180: step 10687, loss 1.2702, acc 0.515625\n",
            "2022-05-05T21:53:00.718686: step 10688, loss 1.40781, acc 0.40625\n",
            "2022-05-05T21:53:01.339350: step 10689, loss 1.39677, acc 0.53125\n",
            "2022-05-05T21:53:01.968464: step 10690, loss 1.45819, acc 0.46875\n",
            "2022-05-05T21:53:02.573129: step 10691, loss 1.20771, acc 0.578125\n",
            "2022-05-05T21:53:03.194398: step 10692, loss 1.25186, acc 0.53125\n",
            "2022-05-05T21:53:03.803715: step 10693, loss 1.28733, acc 0.53125\n",
            "2022-05-05T21:53:04.442683: step 10694, loss 0.994119, acc 0.6875\n",
            "2022-05-05T21:53:05.068948: step 10695, loss 1.35839, acc 0.421875\n",
            "2022-05-05T21:53:05.682081: step 10696, loss 1.24482, acc 0.546875\n",
            "2022-05-05T21:53:06.313945: step 10697, loss 1.26422, acc 0.578125\n",
            "2022-05-05T21:53:06.929590: step 10698, loss 1.13709, acc 0.5625\n",
            "2022-05-05T21:53:07.570927: step 10699, loss 1.26074, acc 0.59375\n",
            "2022-05-05T21:53:08.189440: step 10700, loss 1.21827, acc 0.5625\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:53:08.333575: step 10700, loss 1.99579, acc 0.29\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-10700\n",
            "\n",
            "2022-05-05T21:53:09.804616: step 10701, loss 1.32738, acc 0.53125\n",
            "2022-05-05T21:53:10.521288: step 10702, loss 1.32932, acc 0.546875\n",
            "2022-05-05T21:53:11.258507: step 10703, loss 1.24794, acc 0.546875\n",
            "2022-05-05T21:53:11.960734: step 10704, loss 1.42554, acc 0.53125\n",
            "2022-05-05T21:53:12.675920: step 10705, loss 1.5181, acc 0.53125\n",
            "2022-05-05T21:53:13.403257: step 10706, loss 1.2059, acc 0.546875\n",
            "2022-05-05T21:53:14.179964: step 10707, loss 1.42314, acc 0.5625\n",
            "2022-05-05T21:53:15.957563: step 10708, loss 1.24018, acc 0.578125\n",
            "2022-05-05T21:53:17.598685: step 10709, loss 1.31599, acc 0.515625\n",
            "2022-05-05T21:53:18.597187: step 10710, loss 1.03909, acc 0.703125\n",
            "2022-05-05T21:53:19.493654: step 10711, loss 1.15675, acc 0.546875\n",
            "2022-05-05T21:53:20.123223: step 10712, loss 1.526, acc 0.375\n",
            "2022-05-05T21:53:20.749522: step 10713, loss 1.13283, acc 0.5625\n",
            "2022-05-05T21:53:21.381225: step 10714, loss 1.17373, acc 0.625\n",
            "2022-05-05T21:53:22.016099: step 10715, loss 1.21347, acc 0.546875\n",
            "2022-05-05T21:53:22.638548: step 10716, loss 1.33409, acc 0.5\n",
            "2022-05-05T21:53:23.274383: step 10717, loss 1.20673, acc 0.578125\n",
            "2022-05-05T21:53:23.948724: step 10718, loss 1.22812, acc 0.5625\n",
            "2022-05-05T21:53:24.633310: step 10719, loss 1.22173, acc 0.609375\n",
            "2022-05-05T21:53:25.357024: step 10720, loss 1.36866, acc 0.59375\n",
            "2022-05-05T21:53:26.058935: step 10721, loss 1.39509, acc 0.5\n",
            "2022-05-05T21:53:26.767242: step 10722, loss 1.26582, acc 0.546875\n",
            "2022-05-05T21:53:27.485592: step 10723, loss 1.38597, acc 0.5\n",
            "2022-05-05T21:53:28.183401: step 10724, loss 1.31949, acc 0.578125\n",
            "2022-05-05T21:53:28.871275: step 10725, loss 1.38946, acc 0.5625\n",
            "2022-05-05T21:53:29.565166: step 10726, loss 1.3777, acc 0.484375\n",
            "2022-05-05T21:53:30.272102: step 10727, loss 1.08818, acc 0.625\n",
            "2022-05-05T21:53:30.953105: step 10728, loss 1.17135, acc 0.5625\n",
            "2022-05-05T21:53:31.672364: step 10729, loss 1.23149, acc 0.53125\n",
            "2022-05-05T21:53:32.371947: step 10730, loss 1.178, acc 0.5625\n",
            "2022-05-05T21:53:33.082636: step 10731, loss 1.25057, acc 0.53125\n",
            "2022-05-05T21:53:33.801226: step 10732, loss 1.24959, acc 0.5\n",
            "2022-05-05T21:53:34.512202: step 10733, loss 1.29414, acc 0.515625\n",
            "2022-05-05T21:53:35.226628: step 10734, loss 1.39184, acc 0.5\n",
            "2022-05-05T21:53:35.959139: step 10735, loss 1.39973, acc 0.53125\n",
            "2022-05-05T21:53:36.668481: step 10736, loss 1.21698, acc 0.53125\n",
            "2022-05-05T21:53:37.406661: step 10737, loss 1.1231, acc 0.625\n",
            "2022-05-05T21:53:38.113161: step 10738, loss 1.34248, acc 0.546875\n",
            "2022-05-05T21:53:38.823499: step 10739, loss 1.13303, acc 0.625\n",
            "2022-05-05T21:53:39.548735: step 10740, loss 1.1761, acc 0.53125\n",
            "2022-05-05T21:53:40.265288: step 10741, loss 1.34702, acc 0.53125\n",
            "2022-05-05T21:53:40.976878: step 10742, loss 1.44167, acc 0.5\n",
            "2022-05-05T21:53:41.719023: step 10743, loss 1.3509, acc 0.46875\n",
            "2022-05-05T21:53:42.434493: step 10744, loss 1.10754, acc 0.5625\n",
            "2022-05-05T21:53:43.152405: step 10745, loss 1.42034, acc 0.578125\n",
            "2022-05-05T21:53:43.887780: step 10746, loss 1.33455, acc 0.53125\n",
            "2022-05-05T21:53:44.588402: step 10747, loss 1.05788, acc 0.625\n",
            "2022-05-05T21:53:45.303048: step 10748, loss 1.31104, acc 0.546875\n",
            "2022-05-05T21:53:46.045261: step 10749, loss 1.38702, acc 0.4375\n",
            "2022-05-05T21:53:46.761407: step 10750, loss 1.27471, acc 0.65625\n",
            "2022-05-05T21:53:47.469222: step 10751, loss 1.39442, acc 0.453125\n",
            "2022-05-05T21:53:48.196259: step 10752, loss 1.48337, acc 0.46875\n",
            "2022-05-05T21:53:48.910644: step 10753, loss 1.37367, acc 0.515625\n",
            "2022-05-05T21:53:49.624638: step 10754, loss 1.4913, acc 0.484375\n",
            "2022-05-05T21:53:50.379286: step 10755, loss 1.33472, acc 0.578125\n",
            "2022-05-05T21:53:51.116709: step 10756, loss 1.40618, acc 0.4375\n",
            "2022-05-05T21:53:51.826847: step 10757, loss 1.15518, acc 0.609375\n",
            "2022-05-05T21:53:52.565475: step 10758, loss 1.27351, acc 0.53125\n",
            "2022-05-05T21:53:53.302126: step 10759, loss 1.47077, acc 0.484375\n",
            "2022-05-05T21:53:54.002750: step 10760, loss 1.43339, acc 0.53125\n",
            "2022-05-05T21:53:54.748649: step 10761, loss 1.37787, acc 0.546875\n",
            "2022-05-05T21:53:55.497962: step 10762, loss 1.38856, acc 0.53125\n",
            "2022-05-05T21:53:56.291147: step 10763, loss 1.30521, acc 0.53125\n",
            "2022-05-05T21:53:57.009438: step 10764, loss 1.21372, acc 0.609375\n",
            "2022-05-05T21:53:57.758819: step 10765, loss 1.38239, acc 0.484375\n",
            "2022-05-05T21:53:58.469641: step 10766, loss 1.20464, acc 0.578125\n",
            "2022-05-05T21:53:59.177848: step 10767, loss 1.20365, acc 0.609375\n",
            "2022-05-05T21:53:59.890244: step 10768, loss 0.940928, acc 0.6875\n",
            "2022-05-05T21:54:00.627591: step 10769, loss 1.19485, acc 0.5625\n",
            "2022-05-05T21:54:01.340839: step 10770, loss 1.53097, acc 0.453125\n",
            "2022-05-05T21:54:02.066101: step 10771, loss 1.08983, acc 0.609375\n",
            "2022-05-05T21:54:02.793423: step 10772, loss 1.05709, acc 0.640625\n",
            "2022-05-05T21:54:03.539229: step 10773, loss 1.56567, acc 0.4375\n",
            "2022-05-05T21:54:04.253696: step 10774, loss 1.30808, acc 0.609375\n",
            "2022-05-05T21:54:04.977365: step 10775, loss 1.29411, acc 0.5625\n",
            "2022-05-05T21:54:05.682900: step 10776, loss 1.4677, acc 0.546875\n",
            "2022-05-05T21:54:06.374494: step 10777, loss 1.22519, acc 0.546875\n",
            "2022-05-05T21:54:07.153845: step 10778, loss 1.38967, acc 0.53125\n",
            "2022-05-05T21:54:07.907436: step 10779, loss 1.27558, acc 0.53125\n",
            "2022-05-05T21:54:08.629298: step 10780, loss 1.30441, acc 0.625\n",
            "2022-05-05T21:54:09.352691: step 10781, loss 1.18586, acc 0.609375\n",
            "2022-05-05T21:54:10.093343: step 10782, loss 1.25521, acc 0.546875\n",
            "2022-05-05T21:54:10.883641: step 10783, loss 1.22924, acc 0.59375\n",
            "2022-05-05T21:54:11.630543: step 10784, loss 1.51992, acc 0.4375\n",
            "2022-05-05T21:54:12.396532: step 10785, loss 1.50431, acc 0.40625\n",
            "2022-05-05T21:54:13.141894: step 10786, loss 1.3858, acc 0.484375\n",
            "2022-05-05T21:54:13.908134: step 10787, loss 1.3019, acc 0.484375\n",
            "2022-05-05T21:54:14.668524: step 10788, loss 1.30134, acc 0.625\n",
            "2022-05-05T21:54:15.442774: step 10789, loss 1.32134, acc 0.546875\n",
            "2022-05-05T21:54:16.182362: step 10790, loss 1.35175, acc 0.453125\n",
            "2022-05-05T21:54:16.907323: step 10791, loss 1.32688, acc 0.53125\n",
            "2022-05-05T21:54:17.649455: step 10792, loss 1.34887, acc 0.46875\n",
            "2022-05-05T21:54:18.380171: step 10793, loss 1.35626, acc 0.578125\n",
            "2022-05-05T21:54:19.098550: step 10794, loss 1.19855, acc 0.578125\n",
            "2022-05-05T21:54:19.807150: step 10795, loss 1.15526, acc 0.59375\n",
            "2022-05-05T21:54:20.543843: step 10796, loss 1.35417, acc 0.546875\n",
            "2022-05-05T21:54:21.249785: step 10797, loss 1.39497, acc 0.484375\n",
            "2022-05-05T21:54:21.995010: step 10798, loss 1.31313, acc 0.59375\n",
            "2022-05-05T21:54:22.727340: step 10799, loss 1.14461, acc 0.53125\n",
            "2022-05-05T21:54:23.465173: step 10800, loss 1.34101, acc 0.53125\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:54:23.632330: step 10800, loss 2.09938, acc 0.28\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-10800\n",
            "\n",
            "2022-05-05T21:54:24.598496: step 10801, loss 1.1777, acc 0.65625\n",
            "2022-05-05T21:54:25.347032: step 10802, loss 1.2286, acc 0.59375\n",
            "2022-05-05T21:54:26.081620: step 10803, loss 1.21766, acc 0.578125\n",
            "2022-05-05T21:54:26.817431: step 10804, loss 1.2102, acc 0.59375\n",
            "2022-05-05T21:54:27.667265: step 10805, loss 1.35557, acc 0.484375\n",
            "2022-05-05T21:54:28.428392: step 10806, loss 1.15023, acc 0.59375\n",
            "2022-05-05T21:54:29.266738: step 10807, loss 1.3295, acc 0.484375\n",
            "2022-05-05T21:54:30.080038: step 10808, loss 1.27344, acc 0.546875\n",
            "2022-05-05T21:54:30.879743: step 10809, loss 1.26301, acc 0.59375\n",
            "2022-05-05T21:54:31.742926: step 10810, loss 1.25624, acc 0.578125\n",
            "2022-05-05T21:54:32.493572: step 10811, loss 1.44092, acc 0.46875\n",
            "2022-05-05T21:54:33.266498: step 10812, loss 1.30939, acc 0.46875\n",
            "2022-05-05T21:54:34.019747: step 10813, loss 1.25036, acc 0.5\n",
            "2022-05-05T21:54:34.821982: step 10814, loss 1.32306, acc 0.515625\n",
            "2022-05-05T21:54:35.543272: step 10815, loss 1.25835, acc 0.5\n",
            "2022-05-05T21:54:36.263564: step 10816, loss 1.27076, acc 0.5625\n",
            "2022-05-05T21:54:36.993473: step 10817, loss 1.39598, acc 0.53125\n",
            "2022-05-05T21:54:37.791877: step 10818, loss 1.2344, acc 0.59375\n",
            "2022-05-05T21:54:38.538583: step 10819, loss 1.42646, acc 0.5\n",
            "2022-05-05T21:54:39.276700: step 10820, loss 1.18151, acc 0.53125\n",
            "2022-05-05T21:54:40.084721: step 10821, loss 1.16482, acc 0.578125\n",
            "2022-05-05T21:54:40.833294: step 10822, loss 1.34967, acc 0.546875\n",
            "2022-05-05T21:54:41.549455: step 10823, loss 1.53244, acc 0.515625\n",
            "2022-05-05T21:54:42.295138: step 10824, loss 1.1892, acc 0.5625\n",
            "2022-05-05T21:54:43.032254: step 10825, loss 1.27416, acc 0.59375\n",
            "2022-05-05T21:54:43.767264: step 10826, loss 1.56252, acc 0.515625\n",
            "2022-05-05T21:54:44.532811: step 10827, loss 1.31483, acc 0.546875\n",
            "2022-05-05T21:54:45.269809: step 10828, loss 1.18881, acc 0.546875\n",
            "2022-05-05T21:54:45.992372: step 10829, loss 1.39712, acc 0.46875\n",
            "2022-05-05T21:54:46.768023: step 10830, loss 1.32145, acc 0.53125\n",
            "2022-05-05T21:54:47.505916: step 10831, loss 1.11904, acc 0.609375\n",
            "2022-05-05T21:54:48.255616: step 10832, loss 1.22686, acc 0.53125\n",
            "2022-05-05T21:54:48.965088: step 10833, loss 1.27145, acc 0.5\n",
            "2022-05-05T21:54:49.706020: step 10834, loss 1.1976, acc 0.609375\n",
            "2022-05-05T21:54:50.425244: step 10835, loss 1.45127, acc 0.53125\n",
            "2022-05-05T21:54:51.059709: step 10836, loss 1.46053, acc 0.46875\n",
            "2022-05-05T21:54:51.724349: step 10837, loss 1.31556, acc 0.453125\n",
            "2022-05-05T21:54:52.372518: step 10838, loss 1.2602, acc 0.546875\n",
            "2022-05-05T21:54:53.009380: step 10839, loss 1.07132, acc 0.6875\n",
            "2022-05-05T21:54:53.658277: step 10840, loss 1.37177, acc 0.515625\n",
            "2022-05-05T21:54:54.280191: step 10841, loss 1.19042, acc 0.671875\n",
            "2022-05-05T21:54:54.913626: step 10842, loss 1.39322, acc 0.53125\n",
            "2022-05-05T21:54:55.550201: step 10843, loss 1.36115, acc 0.5625\n",
            "2022-05-05T21:54:56.168247: step 10844, loss 1.29181, acc 0.546875\n",
            "2022-05-05T21:54:56.796662: step 10845, loss 1.39373, acc 0.515625\n",
            "2022-05-05T21:54:57.416165: step 10846, loss 1.2673, acc 0.515625\n",
            "2022-05-05T21:54:58.056486: step 10847, loss 1.02413, acc 0.65625\n",
            "2022-05-05T21:54:58.727023: step 10848, loss 1.35381, acc 0.5\n",
            "2022-05-05T21:54:59.340229: step 10849, loss 1.15265, acc 0.5625\n",
            "2022-05-05T21:54:59.986640: step 10850, loss 1.3965, acc 0.4375\n",
            "2022-05-05T21:55:00.605713: step 10851, loss 1.33051, acc 0.609375\n",
            "2022-05-05T21:55:01.239543: step 10852, loss 1.32187, acc 0.546875\n",
            "2022-05-05T21:55:01.878438: step 10853, loss 1.28609, acc 0.5\n",
            "2022-05-05T21:55:02.492058: step 10854, loss 1.18117, acc 0.5625\n",
            "2022-05-05T21:55:03.126668: step 10855, loss 1.21267, acc 0.59375\n",
            "2022-05-05T21:55:03.785479: step 10856, loss 1.27391, acc 0.546875\n",
            "2022-05-05T21:55:04.409171: step 10857, loss 1.24159, acc 0.546875\n",
            "2022-05-05T21:55:05.039157: step 10858, loss 1.33028, acc 0.546875\n",
            "2022-05-05T21:55:05.658889: step 10859, loss 1.38116, acc 0.46875\n",
            "2022-05-05T21:55:06.302027: step 10860, loss 1.34241, acc 0.484375\n",
            "2022-05-05T21:55:06.945147: step 10861, loss 1.55205, acc 0.453125\n",
            "2022-05-05T21:55:07.559612: step 10862, loss 1.34846, acc 0.484375\n",
            "2022-05-05T21:55:08.178146: step 10863, loss 1.24293, acc 0.546875\n",
            "2022-05-05T21:55:08.818083: step 10864, loss 1.3117, acc 0.5625\n",
            "2022-05-05T21:55:09.452117: step 10865, loss 1.31339, acc 0.484375\n",
            "2022-05-05T21:55:10.085141: step 10866, loss 1.35779, acc 0.5\n",
            "2022-05-05T21:55:10.701256: step 10867, loss 1.20091, acc 0.609375\n",
            "2022-05-05T21:55:11.330869: step 10868, loss 1.41117, acc 0.5\n",
            "2022-05-05T21:55:11.947717: step 10869, loss 1.35881, acc 0.484375\n",
            "2022-05-05T21:55:12.586933: step 10870, loss 1.31834, acc 0.515625\n",
            "2022-05-05T21:55:13.209982: step 10871, loss 1.45348, acc 0.578125\n",
            "2022-05-05T21:55:13.823419: step 10872, loss 1.37048, acc 0.609375\n",
            "2022-05-05T21:55:14.470780: step 10873, loss 1.34726, acc 0.515625\n",
            "2022-05-05T21:55:15.083489: step 10874, loss 1.31905, acc 0.5625\n",
            "2022-05-05T21:55:15.712679: step 10875, loss 1.47748, acc 0.421875\n",
            "2022-05-05T21:55:16.343007: step 10876, loss 1.16095, acc 0.59375\n",
            "2022-05-05T21:55:16.968513: step 10877, loss 1.21564, acc 0.53125\n",
            "2022-05-05T21:55:17.593107: step 10878, loss 1.17258, acc 0.671875\n",
            "2022-05-05T21:55:18.207430: step 10879, loss 1.48811, acc 0.484375\n",
            "2022-05-05T21:55:18.827227: step 10880, loss 1.44027, acc 0.5\n",
            "2022-05-05T21:55:19.480129: step 10881, loss 1.35565, acc 0.484375\n",
            "2022-05-05T21:55:20.096388: step 10882, loss 1.27004, acc 0.546875\n",
            "2022-05-05T21:55:20.733969: step 10883, loss 1.46553, acc 0.515625\n",
            "2022-05-05T21:55:21.365873: step 10884, loss 1.46008, acc 0.4375\n",
            "2022-05-05T21:55:21.990468: step 10885, loss 1.23837, acc 0.578125\n",
            "2022-05-05T21:55:22.621212: step 10886, loss 1.17811, acc 0.640625\n",
            "2022-05-05T21:55:23.238567: step 10887, loss 1.40296, acc 0.5\n",
            "2022-05-05T21:55:23.864220: step 10888, loss 1.28185, acc 0.609375\n",
            "2022-05-05T21:55:24.492249: step 10889, loss 1.36142, acc 0.515625\n",
            "2022-05-05T21:55:25.111764: step 10890, loss 1.42094, acc 0.53125\n",
            "2022-05-05T21:55:25.752529: step 10891, loss 1.29949, acc 0.546875\n",
            "2022-05-05T21:55:26.381709: step 10892, loss 1.33362, acc 0.453125\n",
            "2022-05-05T21:55:27.016024: step 10893, loss 1.03288, acc 0.640625\n",
            "2022-05-05T21:55:27.657937: step 10894, loss 1.24237, acc 0.515625\n",
            "2022-05-05T21:55:28.268166: step 10895, loss 1.18225, acc 0.59375\n",
            "2022-05-05T21:55:28.902774: step 10896, loss 1.49292, acc 0.421875\n",
            "2022-05-05T21:55:29.543639: step 10897, loss 1.2353, acc 0.546875\n",
            "2022-05-05T21:55:30.178836: step 10898, loss 1.21301, acc 0.5625\n",
            "2022-05-05T21:55:30.810169: step 10899, loss 1.26814, acc 0.515625\n",
            "2022-05-05T21:55:31.447377: step 10900, loss 1.20714, acc 0.609375\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:55:31.600973: step 10900, loss 2.10317, acc 0.3\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-10900\n",
            "\n",
            "2022-05-05T21:55:32.464263: step 10901, loss 1.36475, acc 0.4375\n",
            "2022-05-05T21:55:33.089100: step 10902, loss 1.13552, acc 0.625\n",
            "2022-05-05T21:55:33.700391: step 10903, loss 1.2495, acc 0.5625\n",
            "2022-05-05T21:55:34.341095: step 10904, loss 1.16948, acc 0.5625\n",
            "2022-05-05T21:55:34.974280: step 10905, loss 1.33293, acc 0.5\n",
            "2022-05-05T21:55:35.650349: step 10906, loss 1.26518, acc 0.46875\n",
            "2022-05-05T21:55:36.288295: step 10907, loss 1.40981, acc 0.484375\n",
            "2022-05-05T21:55:36.926758: step 10908, loss 1.31729, acc 0.5625\n",
            "2022-05-05T21:55:37.541627: step 10909, loss 1.34301, acc 0.5\n",
            "2022-05-05T21:55:38.168905: step 10910, loss 1.23822, acc 0.515625\n",
            "2022-05-05T21:55:38.787714: step 10911, loss 1.62566, acc 0.4375\n",
            "2022-05-05T21:55:39.417380: step 10912, loss 1.28726, acc 0.5\n",
            "2022-05-05T21:55:40.066164: step 10913, loss 1.19733, acc 0.5\n",
            "2022-05-05T21:55:40.685529: step 10914, loss 1.36514, acc 0.484375\n",
            "2022-05-05T21:55:41.321437: step 10915, loss 1.50034, acc 0.4375\n",
            "2022-05-05T21:55:41.961606: step 10916, loss 1.27399, acc 0.484375\n",
            "2022-05-05T21:55:42.595688: step 10917, loss 1.1821, acc 0.515625\n",
            "2022-05-05T21:55:43.238371: step 10918, loss 1.23187, acc 0.671875\n",
            "2022-05-05T21:55:43.858837: step 10919, loss 1.62749, acc 0.46875\n",
            "2022-05-05T21:55:44.487301: step 10920, loss 1.30896, acc 0.484375\n",
            "2022-05-05T21:55:45.114566: step 10921, loss 1.34947, acc 0.484375\n",
            "2022-05-05T21:55:45.750034: step 10922, loss 1.43087, acc 0.484375\n",
            "2022-05-05T21:55:46.383746: step 10923, loss 1.18836, acc 0.546875\n",
            "2022-05-05T21:55:47.019166: step 10924, loss 1.38198, acc 0.515625\n",
            "2022-05-05T21:55:47.648098: step 10925, loss 1.29201, acc 0.59375\n",
            "2022-05-05T21:55:48.278187: step 10926, loss 1.50332, acc 0.515625\n",
            "2022-05-05T21:55:48.908481: step 10927, loss 1.3737, acc 0.484375\n",
            "2022-05-05T21:55:49.561348: step 10928, loss 1.1731, acc 0.578125\n",
            "2022-05-05T21:55:50.220911: step 10929, loss 1.21495, acc 0.59375\n",
            "2022-05-05T21:55:50.866026: step 10930, loss 1.21806, acc 0.5625\n",
            "2022-05-05T21:55:51.506636: step 10931, loss 1.42111, acc 0.46875\n",
            "2022-05-05T21:55:52.160074: step 10932, loss 1.22781, acc 0.546875\n",
            "2022-05-05T21:55:52.810792: step 10933, loss 1.41604, acc 0.546875\n",
            "2022-05-05T21:55:53.450220: step 10934, loss 1.26776, acc 0.578125\n",
            "2022-05-05T21:55:54.085685: step 10935, loss 1.50043, acc 0.484375\n",
            "2022-05-05T21:55:54.744282: step 10936, loss 1.16509, acc 0.5625\n",
            "2022-05-05T21:55:55.376758: step 10937, loss 1.50357, acc 0.484375\n",
            "2022-05-05T21:55:56.016326: step 10938, loss 1.41141, acc 0.5\n",
            "2022-05-05T21:55:56.663272: step 10939, loss 1.26005, acc 0.5625\n",
            "2022-05-05T21:55:57.291291: step 10940, loss 1.30536, acc 0.609375\n",
            "2022-05-05T21:55:57.945350: step 10941, loss 1.3419, acc 0.5\n",
            "2022-05-05T21:55:58.585436: step 10942, loss 1.10243, acc 0.640625\n",
            "2022-05-05T21:55:59.211686: step 10943, loss 1.47027, acc 0.515625\n",
            "2022-05-05T21:55:59.879095: step 10944, loss 1.4043, acc 0.515625\n",
            "2022-05-05T21:56:00.543202: step 10945, loss 1.50483, acc 0.5\n",
            "2022-05-05T21:56:01.192487: step 10946, loss 1.51936, acc 0.515625\n",
            "2022-05-05T21:56:01.847763: step 10947, loss 1.38935, acc 0.484375\n",
            "2022-05-05T21:56:02.483566: step 10948, loss 1.21412, acc 0.609375\n",
            "2022-05-05T21:56:03.200411: step 10949, loss 1.22881, acc 0.5625\n",
            "2022-05-05T21:56:03.895667: step 10950, loss 1.34134, acc 0.53125\n",
            "2022-05-05T21:56:04.569913: step 10951, loss 1.36255, acc 0.515625\n",
            "2022-05-05T21:56:05.301132: step 10952, loss 1.22881, acc 0.578125\n",
            "2022-05-05T21:56:06.005298: step 10953, loss 1.37548, acc 0.46875\n",
            "2022-05-05T21:56:06.669389: step 10954, loss 1.46173, acc 0.5\n",
            "2022-05-05T21:56:07.372246: step 10955, loss 1.52772, acc 0.4375\n",
            "2022-05-05T21:56:08.057787: step 10956, loss 1.17651, acc 0.578125\n",
            "2022-05-05T21:56:08.697943: step 10957, loss 1.48838, acc 0.453125\n",
            "2022-05-05T21:56:09.376907: step 10958, loss 1.31483, acc 0.5625\n",
            "2022-05-05T21:56:10.059561: step 10959, loss 1.14015, acc 0.609375\n",
            "2022-05-05T21:56:10.776686: step 10960, loss 1.25153, acc 0.5625\n",
            "2022-05-05T21:56:11.466763: step 10961, loss 1.21755, acc 0.5\n",
            "2022-05-05T21:56:12.161617: step 10962, loss 1.12344, acc 0.65625\n",
            "2022-05-05T21:56:12.854574: step 10963, loss 1.29759, acc 0.5\n",
            "2022-05-05T21:56:13.579535: step 10964, loss 1.28856, acc 0.5625\n",
            "2022-05-05T21:56:14.297442: step 10965, loss 1.37235, acc 0.578125\n",
            "2022-05-05T21:56:15.002795: step 10966, loss 1.41771, acc 0.4375\n",
            "2022-05-05T21:56:15.691637: step 10967, loss 1.22745, acc 0.625\n",
            "2022-05-05T21:56:16.361823: step 10968, loss 1.22861, acc 0.5625\n",
            "2022-05-05T21:56:17.004181: step 10969, loss 1.22758, acc 0.5625\n",
            "2022-05-05T21:56:17.652011: step 10970, loss 1.48304, acc 0.484375\n",
            "2022-05-05T21:56:18.318839: step 10971, loss 1.28319, acc 0.453125\n",
            "2022-05-05T21:56:18.982180: step 10972, loss 1.18609, acc 0.546875\n",
            "2022-05-05T21:56:19.652956: step 10973, loss 1.10442, acc 0.59375\n",
            "2022-05-05T21:56:20.302303: step 10974, loss 1.35616, acc 0.515625\n",
            "2022-05-05T21:56:20.991244: step 10975, loss 1.27219, acc 0.53125\n",
            "2022-05-05T21:56:21.657477: step 10976, loss 1.30403, acc 0.453125\n",
            "2022-05-05T21:56:22.313592: step 10977, loss 1.23708, acc 0.640625\n",
            "2022-05-05T21:56:22.960723: step 10978, loss 1.66627, acc 0.40625\n",
            "2022-05-05T21:56:23.586435: step 10979, loss 1.26794, acc 0.578125\n",
            "2022-05-05T21:56:24.200450: step 10980, loss 1.40285, acc 0.484375\n",
            "2022-05-05T21:56:24.831126: step 10981, loss 1.47675, acc 0.46875\n",
            "2022-05-05T21:56:25.348857: step 10982, loss 1.31269, acc 0.632653\n",
            "2022-05-05T21:56:25.989442: step 10983, loss 1.27961, acc 0.515625\n",
            "2022-05-05T21:56:26.629445: step 10984, loss 1.28886, acc 0.515625\n",
            "2022-05-05T21:56:27.240465: step 10985, loss 1.15273, acc 0.65625\n",
            "2022-05-05T21:56:27.866594: step 10986, loss 1.31151, acc 0.59375\n",
            "2022-05-05T21:56:28.479182: step 10987, loss 1.10319, acc 0.625\n",
            "2022-05-05T21:56:29.103161: step 10988, loss 1.21824, acc 0.5\n",
            "2022-05-05T21:56:29.756942: step 10989, loss 1.19062, acc 0.5625\n",
            "2022-05-05T21:56:30.369886: step 10990, loss 1.25774, acc 0.546875\n",
            "2022-05-05T21:56:31.000116: step 10991, loss 1.21825, acc 0.5625\n",
            "2022-05-05T21:56:31.639138: step 10992, loss 1.03256, acc 0.578125\n",
            "2022-05-05T21:56:32.274544: step 10993, loss 1.07068, acc 0.578125\n",
            "2022-05-05T21:56:32.919433: step 10994, loss 1.36857, acc 0.515625\n",
            "2022-05-05T21:56:33.537233: step 10995, loss 1.309, acc 0.453125\n",
            "2022-05-05T21:56:34.171223: step 10996, loss 1.24777, acc 0.5\n",
            "2022-05-05T21:56:34.791415: step 10997, loss 1.16499, acc 0.59375\n",
            "2022-05-05T21:56:35.430442: step 10998, loss 1.05796, acc 0.5625\n",
            "2022-05-05T21:56:36.066183: step 10999, loss 1.08502, acc 0.6875\n",
            "2022-05-05T21:56:36.686718: step 11000, loss 1.23028, acc 0.484375\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:56:36.829369: step 11000, loss 1.94925, acc 0.36\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-11000\n",
            "\n",
            "2022-05-05T21:56:37.615229: step 11001, loss 1.2597, acc 0.59375\n",
            "2022-05-05T21:56:38.244983: step 11002, loss 1.21792, acc 0.609375\n",
            "2022-05-05T21:56:38.858373: step 11003, loss 1.10993, acc 0.609375\n",
            "2022-05-05T21:56:39.480657: step 11004, loss 1.01284, acc 0.6875\n",
            "2022-05-05T21:56:40.103068: step 11005, loss 1.3, acc 0.546875\n",
            "2022-05-05T21:56:40.724227: step 11006, loss 1.03504, acc 0.625\n",
            "2022-05-05T21:56:41.344875: step 11007, loss 1.18733, acc 0.578125\n",
            "2022-05-05T21:56:41.996328: step 11008, loss 1.26563, acc 0.515625\n",
            "2022-05-05T21:56:42.619105: step 11009, loss 1.19107, acc 0.609375\n",
            "2022-05-05T21:56:43.241696: step 11010, loss 1.15046, acc 0.609375\n",
            "2022-05-05T21:56:43.856930: step 11011, loss 1.08418, acc 0.703125\n",
            "2022-05-05T21:56:44.495841: step 11012, loss 1.15547, acc 0.53125\n",
            "2022-05-05T21:56:45.108162: step 11013, loss 1.34065, acc 0.484375\n",
            "2022-05-05T21:56:45.730073: step 11014, loss 1.1884, acc 0.5625\n",
            "2022-05-05T21:56:46.355570: step 11015, loss 1.2137, acc 0.59375\n",
            "2022-05-05T21:56:46.977948: step 11016, loss 1.19668, acc 0.515625\n",
            "2022-05-05T21:56:47.626696: step 11017, loss 1.22053, acc 0.546875\n",
            "2022-05-05T21:56:48.278244: step 11018, loss 1.05693, acc 0.640625\n",
            "2022-05-05T21:56:48.894218: step 11019, loss 1.10955, acc 0.609375\n",
            "2022-05-05T21:56:49.512717: step 11020, loss 1.08537, acc 0.640625\n",
            "2022-05-05T21:56:50.143895: step 11021, loss 1.20359, acc 0.578125\n",
            "2022-05-05T21:56:50.780094: step 11022, loss 1.09355, acc 0.59375\n",
            "2022-05-05T21:56:51.400732: step 11023, loss 1.2412, acc 0.578125\n",
            "2022-05-05T21:56:52.042106: step 11024, loss 0.916026, acc 0.765625\n",
            "2022-05-05T21:56:52.673111: step 11025, loss 1.1648, acc 0.53125\n",
            "2022-05-05T21:56:53.305378: step 11026, loss 1.1933, acc 0.546875\n",
            "2022-05-05T21:56:53.938360: step 11027, loss 1.17244, acc 0.59375\n",
            "2022-05-05T21:56:54.564081: step 11028, loss 1.16337, acc 0.625\n",
            "2022-05-05T21:56:55.188067: step 11029, loss 1.15148, acc 0.625\n",
            "2022-05-05T21:56:55.811046: step 11030, loss 1.24407, acc 0.5\n",
            "2022-05-05T21:56:56.427871: step 11031, loss 1.06651, acc 0.546875\n",
            "2022-05-05T21:56:57.048339: step 11032, loss 1.25733, acc 0.5625\n",
            "2022-05-05T21:56:57.673665: step 11033, loss 1.08824, acc 0.625\n",
            "2022-05-05T21:56:58.287319: step 11034, loss 1.22297, acc 0.515625\n",
            "2022-05-05T21:56:58.919602: step 11035, loss 0.955725, acc 0.6875\n",
            "2022-05-05T21:56:59.528439: step 11036, loss 1.16877, acc 0.5\n",
            "2022-05-05T21:57:00.165908: step 11037, loss 1.31098, acc 0.5\n",
            "2022-05-05T21:57:00.799901: step 11038, loss 1.15945, acc 0.625\n",
            "2022-05-05T21:57:01.415796: step 11039, loss 1.19824, acc 0.515625\n",
            "2022-05-05T21:57:02.059372: step 11040, loss 1.21403, acc 0.515625\n",
            "2022-05-05T21:57:02.667803: step 11041, loss 1.08604, acc 0.5625\n",
            "2022-05-05T21:57:03.303438: step 11042, loss 1.18459, acc 0.546875\n",
            "2022-05-05T21:57:03.919927: step 11043, loss 0.985896, acc 0.703125\n",
            "2022-05-05T21:57:04.532708: step 11044, loss 1.23387, acc 0.546875\n",
            "2022-05-05T21:57:05.154977: step 11045, loss 1.1352, acc 0.578125\n",
            "2022-05-05T21:57:05.766228: step 11046, loss 1.42824, acc 0.484375\n",
            "2022-05-05T21:57:06.392573: step 11047, loss 1.28106, acc 0.484375\n",
            "2022-05-05T21:57:07.017660: step 11048, loss 1.21284, acc 0.546875\n",
            "2022-05-05T21:57:07.633727: step 11049, loss 1.22109, acc 0.625\n",
            "2022-05-05T21:57:08.258564: step 11050, loss 1.15744, acc 0.5625\n",
            "2022-05-05T21:57:08.872820: step 11051, loss 1.06875, acc 0.640625\n",
            "2022-05-05T21:57:09.506717: step 11052, loss 1.10299, acc 0.546875\n",
            "2022-05-05T21:57:10.132893: step 11053, loss 1.30976, acc 0.515625\n",
            "2022-05-05T21:57:10.749187: step 11054, loss 1.30273, acc 0.5\n",
            "2022-05-05T21:57:11.369076: step 11055, loss 1.19961, acc 0.640625\n",
            "2022-05-05T21:57:11.990709: step 11056, loss 1.13847, acc 0.609375\n",
            "2022-05-05T21:57:12.630696: step 11057, loss 1.06249, acc 0.546875\n",
            "2022-05-05T21:57:13.261184: step 11058, loss 1.2637, acc 0.578125\n",
            "2022-05-05T21:57:13.873846: step 11059, loss 1.14862, acc 0.625\n",
            "2022-05-05T21:57:14.492603: step 11060, loss 1.29466, acc 0.59375\n",
            "2022-05-05T21:57:15.095912: step 11061, loss 1.153, acc 0.609375\n",
            "2022-05-05T21:57:15.725237: step 11062, loss 1.01947, acc 0.671875\n",
            "2022-05-05T21:57:16.341839: step 11063, loss 1.10766, acc 0.65625\n",
            "2022-05-05T21:57:16.951966: step 11064, loss 1.16149, acc 0.515625\n",
            "2022-05-05T21:57:17.566125: step 11065, loss 1.28944, acc 0.5\n",
            "2022-05-05T21:57:18.181675: step 11066, loss 1.16639, acc 0.625\n",
            "2022-05-05T21:57:18.805548: step 11067, loss 1.1489, acc 0.5\n",
            "2022-05-05T21:57:19.420601: step 11068, loss 1.41951, acc 0.53125\n",
            "2022-05-05T21:57:20.031618: step 11069, loss 1.04138, acc 0.734375\n",
            "2022-05-05T21:57:20.657744: step 11070, loss 1.33506, acc 0.53125\n",
            "2022-05-05T21:57:21.268916: step 11071, loss 1.02018, acc 0.734375\n",
            "2022-05-05T21:57:21.894332: step 11072, loss 1.35763, acc 0.515625\n",
            "2022-05-05T21:57:22.523187: step 11073, loss 1.02722, acc 0.703125\n",
            "2022-05-05T21:57:23.150466: step 11074, loss 1.18272, acc 0.59375\n",
            "2022-05-05T21:57:23.796337: step 11075, loss 1.13417, acc 0.59375\n",
            "2022-05-05T21:57:24.406050: step 11076, loss 1.20416, acc 0.515625\n",
            "2022-05-05T21:57:25.026665: step 11077, loss 1.14513, acc 0.671875\n",
            "2022-05-05T21:57:25.644800: step 11078, loss 1.2188, acc 0.59375\n",
            "2022-05-05T21:57:26.268086: step 11079, loss 1.09926, acc 0.625\n",
            "2022-05-05T21:57:26.889653: step 11080, loss 1.10429, acc 0.609375\n",
            "2022-05-05T21:57:27.495233: step 11081, loss 1.36602, acc 0.46875\n",
            "2022-05-05T21:57:28.118410: step 11082, loss 1.06371, acc 0.671875\n",
            "2022-05-05T21:57:28.742051: step 11083, loss 1.12086, acc 0.5625\n",
            "2022-05-05T21:57:29.361288: step 11084, loss 1.41961, acc 0.421875\n",
            "2022-05-05T21:57:29.987463: step 11085, loss 1.31984, acc 0.5\n",
            "2022-05-05T21:57:30.604769: step 11086, loss 1.18392, acc 0.625\n",
            "2022-05-05T21:57:31.225518: step 11087, loss 1.12128, acc 0.578125\n",
            "2022-05-05T21:57:31.843504: step 11088, loss 1.27106, acc 0.546875\n",
            "2022-05-05T21:57:32.454542: step 11089, loss 1.3199, acc 0.5625\n",
            "2022-05-05T21:57:33.069240: step 11090, loss 1.21567, acc 0.484375\n",
            "2022-05-05T21:57:33.699032: step 11091, loss 1.06083, acc 0.65625\n",
            "2022-05-05T21:57:34.320741: step 11092, loss 1.20402, acc 0.546875\n",
            "2022-05-05T21:57:34.941161: step 11093, loss 1.09976, acc 0.625\n",
            "2022-05-05T21:57:35.547518: step 11094, loss 1.33796, acc 0.53125\n",
            "2022-05-05T21:57:36.176316: step 11095, loss 1.04108, acc 0.625\n",
            "2022-05-05T21:57:36.772534: step 11096, loss 1.23727, acc 0.515625\n",
            "2022-05-05T21:57:37.412044: step 11097, loss 1.21837, acc 0.625\n",
            "2022-05-05T21:57:38.032967: step 11098, loss 1.25785, acc 0.609375\n",
            "2022-05-05T21:57:38.641629: step 11099, loss 1.0556, acc 0.640625\n",
            "2022-05-05T21:57:39.256420: step 11100, loss 1.44067, acc 0.578125\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:57:39.411817: step 11100, loss 2.07621, acc 0.24\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-11100\n",
            "\n",
            "2022-05-05T21:57:40.227903: step 11101, loss 1.0473, acc 0.625\n",
            "2022-05-05T21:57:40.833869: step 11102, loss 0.991087, acc 0.671875\n",
            "2022-05-05T21:57:41.460368: step 11103, loss 1.11347, acc 0.59375\n",
            "2022-05-05T21:57:42.095139: step 11104, loss 1.41497, acc 0.375\n",
            "2022-05-05T21:57:42.711940: step 11105, loss 1.2662, acc 0.578125\n",
            "2022-05-05T21:57:43.334345: step 11106, loss 1.17301, acc 0.546875\n",
            "2022-05-05T21:57:43.953169: step 11107, loss 1.11922, acc 0.59375\n",
            "2022-05-05T21:57:44.593182: step 11108, loss 1.06373, acc 0.609375\n",
            "2022-05-05T21:57:45.218133: step 11109, loss 0.997274, acc 0.671875\n",
            "2022-05-05T21:57:45.825621: step 11110, loss 1.29348, acc 0.5625\n",
            "2022-05-05T21:57:46.463424: step 11111, loss 1.18383, acc 0.546875\n",
            "2022-05-05T21:57:47.071743: step 11112, loss 1.25787, acc 0.625\n",
            "2022-05-05T21:57:47.701454: step 11113, loss 1.15901, acc 0.640625\n",
            "2022-05-05T21:57:48.324522: step 11114, loss 1.24089, acc 0.5625\n",
            "2022-05-05T21:57:48.936027: step 11115, loss 1.38594, acc 0.5\n",
            "2022-05-05T21:57:49.553180: step 11116, loss 1.09735, acc 0.578125\n",
            "2022-05-05T21:57:50.155915: step 11117, loss 1.00353, acc 0.65625\n",
            "2022-05-05T21:57:50.779529: step 11118, loss 1.09536, acc 0.65625\n",
            "2022-05-05T21:57:51.402595: step 11119, loss 1.17069, acc 0.578125\n",
            "2022-05-05T21:57:52.010561: step 11120, loss 1.11419, acc 0.609375\n",
            "2022-05-05T21:57:52.638209: step 11121, loss 1.23104, acc 0.640625\n",
            "2022-05-05T21:57:53.248083: step 11122, loss 1.41853, acc 0.53125\n",
            "2022-05-05T21:57:53.873137: step 11123, loss 1.29918, acc 0.53125\n",
            "2022-05-05T21:57:54.492126: step 11124, loss 1.16021, acc 0.609375\n",
            "2022-05-05T21:57:55.156195: step 11125, loss 1.29656, acc 0.5625\n",
            "2022-05-05T21:57:55.780885: step 11126, loss 1.45359, acc 0.53125\n",
            "2022-05-05T21:57:56.390808: step 11127, loss 1.23411, acc 0.609375\n",
            "2022-05-05T21:57:57.013834: step 11128, loss 1.0533, acc 0.640625\n",
            "2022-05-05T21:57:57.648231: step 11129, loss 1.32532, acc 0.578125\n",
            "2022-05-05T21:57:58.280112: step 11130, loss 1.06179, acc 0.65625\n",
            "2022-05-05T21:57:58.924617: step 11131, loss 1.16261, acc 0.625\n",
            "2022-05-05T21:57:59.542470: step 11132, loss 1.15045, acc 0.578125\n",
            "2022-05-05T21:58:00.184273: step 11133, loss 1.322, acc 0.453125\n",
            "2022-05-05T21:58:00.811289: step 11134, loss 1.00555, acc 0.71875\n",
            "2022-05-05T21:58:01.416631: step 11135, loss 1.17104, acc 0.65625\n",
            "2022-05-05T21:58:02.039721: step 11136, loss 0.987711, acc 0.625\n",
            "2022-05-05T21:58:02.653878: step 11137, loss 1.15735, acc 0.640625\n",
            "2022-05-05T21:58:03.278168: step 11138, loss 1.32304, acc 0.578125\n",
            "2022-05-05T21:58:03.907519: step 11139, loss 1.08907, acc 0.59375\n",
            "2022-05-05T21:58:04.523144: step 11140, loss 1.35734, acc 0.546875\n",
            "2022-05-05T21:58:05.166534: step 11141, loss 1.35349, acc 0.5\n",
            "2022-05-05T21:58:05.773287: step 11142, loss 1.0449, acc 0.640625\n",
            "2022-05-05T21:58:06.401339: step 11143, loss 1.26944, acc 0.546875\n",
            "2022-05-05T21:58:07.013919: step 11144, loss 1.08, acc 0.5625\n",
            "2022-05-05T21:58:07.631653: step 11145, loss 1.33453, acc 0.5625\n",
            "2022-05-05T21:58:08.250825: step 11146, loss 1.37385, acc 0.53125\n",
            "2022-05-05T21:58:08.853427: step 11147, loss 1.32106, acc 0.546875\n",
            "2022-05-05T21:58:09.469174: step 11148, loss 1.31404, acc 0.546875\n",
            "2022-05-05T21:58:10.091851: step 11149, loss 1.38286, acc 0.390625\n",
            "2022-05-05T21:58:10.718328: step 11150, loss 1.32507, acc 0.546875\n",
            "2022-05-05T21:58:11.336134: step 11151, loss 1.08764, acc 0.59375\n",
            "2022-05-05T21:58:11.943879: step 11152, loss 1.40399, acc 0.4375\n",
            "2022-05-05T21:58:12.575085: step 11153, loss 1.31116, acc 0.53125\n",
            "2022-05-05T21:58:13.196865: step 11154, loss 1.12189, acc 0.59375\n",
            "2022-05-05T21:58:13.800936: step 11155, loss 1.12575, acc 0.59375\n",
            "2022-05-05T21:58:14.413448: step 11156, loss 1.47456, acc 0.390625\n",
            "2022-05-05T21:58:15.020255: step 11157, loss 1.27898, acc 0.5\n",
            "2022-05-05T21:58:15.672820: step 11158, loss 1.23825, acc 0.5\n",
            "2022-05-05T21:58:16.294594: step 11159, loss 1.1518, acc 0.625\n",
            "2022-05-05T21:58:16.899194: step 11160, loss 1.27199, acc 0.546875\n",
            "2022-05-05T21:58:17.522872: step 11161, loss 1.20774, acc 0.65625\n",
            "2022-05-05T21:58:18.131146: step 11162, loss 1.18647, acc 0.5625\n",
            "2022-05-05T21:58:18.749915: step 11163, loss 1.24903, acc 0.515625\n",
            "2022-05-05T21:58:19.367479: step 11164, loss 1.28806, acc 0.484375\n",
            "2022-05-05T21:58:19.980362: step 11165, loss 1.02013, acc 0.625\n",
            "2022-05-05T21:58:20.599140: step 11166, loss 1.30904, acc 0.59375\n",
            "2022-05-05T21:58:21.213376: step 11167, loss 1.24767, acc 0.609375\n",
            "2022-05-05T21:58:21.835903: step 11168, loss 1.24749, acc 0.625\n",
            "2022-05-05T21:58:22.454247: step 11169, loss 1.30635, acc 0.546875\n",
            "2022-05-05T21:58:23.059689: step 11170, loss 1.20948, acc 0.53125\n",
            "2022-05-05T21:58:23.678108: step 11171, loss 1.25188, acc 0.578125\n",
            "2022-05-05T21:58:24.280189: step 11172, loss 1.29654, acc 0.625\n",
            "2022-05-05T21:58:24.913058: step 11173, loss 1.25173, acc 0.578125\n",
            "2022-05-05T21:58:25.539534: step 11174, loss 1.26138, acc 0.5\n",
            "2022-05-05T21:58:26.180492: step 11175, loss 1.30575, acc 0.5625\n",
            "2022-05-05T21:58:26.812357: step 11176, loss 1.19792, acc 0.640625\n",
            "2022-05-05T21:58:27.432292: step 11177, loss 1.12972, acc 0.59375\n",
            "2022-05-05T21:58:28.059966: step 11178, loss 1.09545, acc 0.59375\n",
            "2022-05-05T21:58:28.692057: step 11179, loss 1.29464, acc 0.515625\n",
            "2022-05-05T21:58:29.306401: step 11180, loss 1.00647, acc 0.625\n",
            "2022-05-05T21:58:29.936820: step 11181, loss 1.14786, acc 0.609375\n",
            "2022-05-05T21:58:30.553200: step 11182, loss 1.19511, acc 0.53125\n",
            "2022-05-05T21:58:31.176590: step 11183, loss 1.33976, acc 0.515625\n",
            "2022-05-05T21:58:31.807112: step 11184, loss 1.39307, acc 0.546875\n",
            "2022-05-05T21:58:32.426273: step 11185, loss 1.21411, acc 0.59375\n",
            "2022-05-05T21:58:33.062568: step 11186, loss 1.28457, acc 0.5\n",
            "2022-05-05T21:58:33.680106: step 11187, loss 1.22565, acc 0.5\n",
            "2022-05-05T21:58:34.297891: step 11188, loss 1.25226, acc 0.53125\n",
            "2022-05-05T21:58:34.924926: step 11189, loss 1.33224, acc 0.515625\n",
            "2022-05-05T21:58:35.542847: step 11190, loss 1.30284, acc 0.484375\n",
            "2022-05-05T21:58:36.189825: step 11191, loss 1.18464, acc 0.578125\n",
            "2022-05-05T21:58:36.792467: step 11192, loss 1.32628, acc 0.46875\n",
            "2022-05-05T21:58:37.425762: step 11193, loss 1.16161, acc 0.578125\n",
            "2022-05-05T21:58:38.051065: step 11194, loss 1.22313, acc 0.5625\n",
            "2022-05-05T21:58:38.663969: step 11195, loss 1.3207, acc 0.5\n",
            "2022-05-05T21:58:39.289455: step 11196, loss 1.52598, acc 0.515625\n",
            "2022-05-05T21:58:39.899297: step 11197, loss 1.10917, acc 0.5625\n",
            "2022-05-05T21:58:40.528497: step 11198, loss 1.13303, acc 0.546875\n",
            "2022-05-05T21:58:41.162631: step 11199, loss 1.13194, acc 0.59375\n",
            "2022-05-05T21:58:41.776337: step 11200, loss 1.11186, acc 0.578125\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:58:41.919276: step 11200, loss 1.89152, acc 0.39\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-11200\n",
            "\n",
            "2022-05-05T21:58:42.706622: step 11201, loss 1.17791, acc 0.625\n",
            "2022-05-05T21:58:43.326413: step 11202, loss 1.38628, acc 0.421875\n",
            "2022-05-05T21:58:43.935813: step 11203, loss 1.33455, acc 0.546875\n",
            "2022-05-05T21:58:44.569174: step 11204, loss 1.29509, acc 0.5625\n",
            "2022-05-05T21:58:45.201114: step 11205, loss 1.24129, acc 0.578125\n",
            "2022-05-05T21:58:45.817861: step 11206, loss 1.23632, acc 0.484375\n",
            "2022-05-05T21:58:46.456718: step 11207, loss 1.27895, acc 0.59375\n",
            "2022-05-05T21:58:47.097317: step 11208, loss 1.18811, acc 0.578125\n",
            "2022-05-05T21:58:47.727321: step 11209, loss 1.09456, acc 0.640625\n",
            "2022-05-05T21:58:48.352349: step 11210, loss 1.43642, acc 0.46875\n",
            "2022-05-05T21:58:48.964352: step 11211, loss 1.20469, acc 0.609375\n",
            "2022-05-05T21:58:49.593256: step 11212, loss 1.3301, acc 0.578125\n",
            "2022-05-05T21:58:50.204274: step 11213, loss 1.17227, acc 0.546875\n",
            "2022-05-05T21:58:50.827371: step 11214, loss 1.17164, acc 0.578125\n",
            "2022-05-05T21:58:51.447106: step 11215, loss 1.31171, acc 0.546875\n",
            "2022-05-05T21:58:52.079430: step 11216, loss 1.30519, acc 0.515625\n",
            "2022-05-05T21:58:52.706162: step 11217, loss 1.4274, acc 0.515625\n",
            "2022-05-05T21:58:53.313210: step 11218, loss 1.2559, acc 0.625\n",
            "2022-05-05T21:58:53.938570: step 11219, loss 1.13104, acc 0.59375\n",
            "2022-05-05T21:58:54.575612: step 11220, loss 1.32034, acc 0.515625\n",
            "2022-05-05T21:58:55.197116: step 11221, loss 1.13049, acc 0.625\n",
            "2022-05-05T21:58:55.819804: step 11222, loss 1.02777, acc 0.625\n",
            "2022-05-05T21:58:56.432470: step 11223, loss 1.37242, acc 0.421875\n",
            "2022-05-05T21:58:57.096072: step 11224, loss 1.07272, acc 0.609375\n",
            "2022-05-05T21:58:57.718738: step 11225, loss 1.2154, acc 0.546875\n",
            "2022-05-05T21:58:58.337631: step 11226, loss 1.12556, acc 0.578125\n",
            "2022-05-05T21:58:58.984247: step 11227, loss 1.31974, acc 0.5625\n",
            "2022-05-05T21:58:59.593433: step 11228, loss 1.04786, acc 0.625\n",
            "2022-05-05T21:59:00.212968: step 11229, loss 1.3818, acc 0.4375\n",
            "2022-05-05T21:59:00.839262: step 11230, loss 1.28205, acc 0.578125\n",
            "2022-05-05T21:59:01.444827: step 11231, loss 1.38688, acc 0.453125\n",
            "2022-05-05T21:59:02.061443: step 11232, loss 1.25581, acc 0.546875\n",
            "2022-05-05T21:59:02.820951: step 11233, loss 1.19388, acc 0.546875\n",
            "2022-05-05T21:59:03.650102: step 11234, loss 1.19594, acc 0.53125\n",
            "2022-05-05T21:59:04.487530: step 11235, loss 1.34411, acc 0.46875\n",
            "2022-05-05T21:59:05.144496: step 11236, loss 1.28111, acc 0.578125\n",
            "2022-05-05T21:59:05.789187: step 11237, loss 1.25433, acc 0.5625\n",
            "2022-05-05T21:59:06.454881: step 11238, loss 1.21469, acc 0.53125\n",
            "2022-05-05T21:59:07.138353: step 11239, loss 1.02387, acc 0.703125\n",
            "2022-05-05T21:59:07.780408: step 11240, loss 1.40202, acc 0.46875\n",
            "2022-05-05T21:59:08.434737: step 11241, loss 1.18578, acc 0.578125\n",
            "2022-05-05T21:59:09.108321: step 11242, loss 1.05949, acc 0.671875\n",
            "2022-05-05T21:59:09.839768: step 11243, loss 1.39954, acc 0.5\n",
            "2022-05-05T21:59:10.900303: step 11244, loss 1.37744, acc 0.5625\n",
            "2022-05-05T21:59:11.623783: step 11245, loss 1.16546, acc 0.609375\n",
            "2022-05-05T21:59:12.313222: step 11246, loss 1.09932, acc 0.625\n",
            "2022-05-05T21:59:12.965970: step 11247, loss 1.39946, acc 0.59375\n",
            "2022-05-05T21:59:13.632536: step 11248, loss 1.40325, acc 0.40625\n",
            "2022-05-05T21:59:14.289673: step 11249, loss 1.17816, acc 0.578125\n",
            "2022-05-05T21:59:14.956850: step 11250, loss 1.04791, acc 0.65625\n",
            "2022-05-05T21:59:15.623006: step 11251, loss 1.12089, acc 0.609375\n",
            "2022-05-05T21:59:16.288030: step 11252, loss 1.18223, acc 0.5625\n",
            "2022-05-05T21:59:16.959618: step 11253, loss 1.21471, acc 0.578125\n",
            "2022-05-05T21:59:17.697424: step 11254, loss 1.18244, acc 0.515625\n",
            "2022-05-05T21:59:18.347415: step 11255, loss 1.06962, acc 0.65625\n",
            "2022-05-05T21:59:19.013666: step 11256, loss 1.22771, acc 0.5625\n",
            "2022-05-05T21:59:19.685133: step 11257, loss 1.28389, acc 0.53125\n",
            "2022-05-05T21:59:20.345810: step 11258, loss 1.01795, acc 0.671875\n",
            "2022-05-05T21:59:21.009307: step 11259, loss 1.09957, acc 0.59375\n",
            "2022-05-05T21:59:21.672348: step 11260, loss 1.35015, acc 0.578125\n",
            "2022-05-05T21:59:22.314654: step 11261, loss 1.23008, acc 0.484375\n",
            "2022-05-05T21:59:22.976276: step 11262, loss 1.07522, acc 0.578125\n",
            "2022-05-05T21:59:23.625856: step 11263, loss 1.37756, acc 0.546875\n",
            "2022-05-05T21:59:24.293151: step 11264, loss 0.935195, acc 0.75\n",
            "2022-05-05T21:59:24.956198: step 11265, loss 1.21897, acc 0.546875\n",
            "2022-05-05T21:59:25.615953: step 11266, loss 1.14235, acc 0.640625\n",
            "2022-05-05T21:59:26.287684: step 11267, loss 1.24366, acc 0.546875\n",
            "2022-05-05T21:59:26.943053: step 11268, loss 1.38265, acc 0.453125\n",
            "2022-05-05T21:59:27.594380: step 11269, loss 1.62435, acc 0.4375\n",
            "2022-05-05T21:59:28.301023: step 11270, loss 1.31939, acc 0.546875\n",
            "2022-05-05T21:59:28.958767: step 11271, loss 1.22706, acc 0.515625\n",
            "2022-05-05T21:59:29.600732: step 11272, loss 1.09837, acc 0.484375\n",
            "2022-05-05T21:59:30.269464: step 11273, loss 1.20085, acc 0.53125\n",
            "2022-05-05T21:59:30.913568: step 11274, loss 1.43769, acc 0.390625\n",
            "2022-05-05T21:59:31.575973: step 11275, loss 1.3495, acc 0.53125\n",
            "2022-05-05T21:59:32.256170: step 11276, loss 1.22457, acc 0.546875\n",
            "2022-05-05T21:59:32.909329: step 11277, loss 1.20963, acc 0.515625\n",
            "2022-05-05T21:59:33.571681: step 11278, loss 1.28673, acc 0.578125\n",
            "2022-05-05T21:59:34.230420: step 11279, loss 1.52413, acc 0.421875\n",
            "2022-05-05T21:59:34.882148: step 11280, loss 1.19747, acc 0.578125\n",
            "2022-05-05T21:59:35.543478: step 11281, loss 1.21468, acc 0.546875\n",
            "2022-05-05T21:59:36.196251: step 11282, loss 1.22385, acc 0.578125\n",
            "2022-05-05T21:59:36.843730: step 11283, loss 1.35813, acc 0.515625\n",
            "2022-05-05T21:59:37.504284: step 11284, loss 1.3845, acc 0.5\n",
            "2022-05-05T21:59:38.187770: step 11285, loss 1.27956, acc 0.53125\n",
            "2022-05-05T21:59:38.859603: step 11286, loss 1.18971, acc 0.640625\n",
            "2022-05-05T21:59:39.525973: step 11287, loss 1.21358, acc 0.53125\n",
            "2022-05-05T21:59:40.183338: step 11288, loss 1.29996, acc 0.5625\n",
            "2022-05-05T21:59:40.846617: step 11289, loss 1.28323, acc 0.53125\n",
            "2022-05-05T21:59:41.503114: step 11290, loss 1.09729, acc 0.578125\n",
            "2022-05-05T21:59:42.151130: step 11291, loss 1.37267, acc 0.546875\n",
            "2022-05-05T21:59:42.829712: step 11292, loss 1.10092, acc 0.65625\n",
            "2022-05-05T21:59:43.483019: step 11293, loss 1.44107, acc 0.5625\n",
            "2022-05-05T21:59:44.141748: step 11294, loss 1.18926, acc 0.5625\n",
            "2022-05-05T21:59:44.838525: step 11295, loss 1.29848, acc 0.53125\n",
            "2022-05-05T21:59:45.502370: step 11296, loss 1.05931, acc 0.65625\n",
            "2022-05-05T21:59:46.154755: step 11297, loss 1.27109, acc 0.609375\n",
            "2022-05-05T21:59:46.816274: step 11298, loss 1.35092, acc 0.515625\n",
            "2022-05-05T21:59:47.464921: step 11299, loss 1.2152, acc 0.546875\n",
            "2022-05-05T21:59:48.131564: step 11300, loss 1.07917, acc 0.6875\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T21:59:48.286039: step 11300, loss 2.25754, acc 0.31\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-11300\n",
            "\n",
            "2022-05-05T21:59:49.203436: step 11301, loss 1.15192, acc 0.578125\n",
            "2022-05-05T21:59:49.871309: step 11302, loss 1.44761, acc 0.53125\n",
            "2022-05-05T21:59:50.511105: step 11303, loss 1.1812, acc 0.640625\n",
            "2022-05-05T21:59:51.180597: step 11304, loss 1.09159, acc 0.703125\n",
            "2022-05-05T21:59:51.850844: step 11305, loss 1.47409, acc 0.5625\n",
            "2022-05-05T21:59:52.508357: step 11306, loss 1.11693, acc 0.5625\n",
            "2022-05-05T21:59:53.170701: step 11307, loss 1.3157, acc 0.40625\n",
            "2022-05-05T21:59:53.871135: step 11308, loss 1.44331, acc 0.421875\n",
            "2022-05-05T21:59:54.533047: step 11309, loss 1.21626, acc 0.609375\n",
            "2022-05-05T21:59:55.205166: step 11310, loss 1.12134, acc 0.671875\n",
            "2022-05-05T21:59:55.868679: step 11311, loss 1.43439, acc 0.4375\n",
            "2022-05-05T21:59:56.541873: step 11312, loss 1.14505, acc 0.59375\n",
            "2022-05-05T21:59:57.208791: step 11313, loss 1.38794, acc 0.453125\n",
            "2022-05-05T21:59:57.874060: step 11314, loss 1.1764, acc 0.546875\n",
            "2022-05-05T21:59:58.546200: step 11315, loss 1.30941, acc 0.46875\n",
            "2022-05-05T21:59:59.267185: step 11316, loss 1.13266, acc 0.625\n",
            "2022-05-05T21:59:59.929971: step 11317, loss 1.33668, acc 0.53125\n",
            "2022-05-05T22:00:00.598138: step 11318, loss 1.40716, acc 0.53125\n",
            "2022-05-05T22:00:01.271450: step 11319, loss 1.25931, acc 0.515625\n",
            "2022-05-05T22:00:01.933508: step 11320, loss 1.35431, acc 0.546875\n",
            "2022-05-05T22:00:02.595425: step 11321, loss 1.10538, acc 0.578125\n",
            "2022-05-05T22:00:03.260644: step 11322, loss 1.11748, acc 0.59375\n",
            "2022-05-05T22:00:03.928971: step 11323, loss 1.21404, acc 0.5625\n",
            "2022-05-05T22:00:04.605779: step 11324, loss 1.28211, acc 0.5\n",
            "2022-05-05T22:00:05.265893: step 11325, loss 1.15657, acc 0.578125\n",
            "2022-05-05T22:00:05.932484: step 11326, loss 1.34047, acc 0.546875\n",
            "2022-05-05T22:00:06.606942: step 11327, loss 1.28859, acc 0.484375\n",
            "2022-05-05T22:00:07.256853: step 11328, loss 1.4565, acc 0.40625\n",
            "2022-05-05T22:00:07.935452: step 11329, loss 1.36423, acc 0.453125\n",
            "2022-05-05T22:00:08.599753: step 11330, loss 1.24987, acc 0.53125\n",
            "2022-05-05T22:00:09.291860: step 11331, loss 1.2829, acc 0.515625\n",
            "2022-05-05T22:00:09.989411: step 11332, loss 1.32779, acc 0.484375\n",
            "2022-05-05T22:00:10.681411: step 11333, loss 1.28077, acc 0.484375\n",
            "2022-05-05T22:00:11.339531: step 11334, loss 0.916581, acc 0.65625\n",
            "2022-05-05T22:00:12.015198: step 11335, loss 1.19134, acc 0.59375\n",
            "2022-05-05T22:00:12.688729: step 11336, loss 1.34006, acc 0.5625\n",
            "2022-05-05T22:00:13.331916: step 11337, loss 1.26409, acc 0.546875\n",
            "2022-05-05T22:00:14.003356: step 11338, loss 1.18961, acc 0.609375\n",
            "2022-05-05T22:00:14.663696: step 11339, loss 1.19002, acc 0.609375\n",
            "2022-05-05T22:00:15.385797: step 11340, loss 1.09448, acc 0.59375\n",
            "2022-05-05T22:00:16.116544: step 11341, loss 1.18975, acc 0.609375\n",
            "2022-05-05T22:00:16.815919: step 11342, loss 1.35012, acc 0.59375\n",
            "2022-05-05T22:00:17.464847: step 11343, loss 1.27587, acc 0.53125\n",
            "2022-05-05T22:00:18.120410: step 11344, loss 1.07668, acc 0.640625\n",
            "2022-05-05T22:00:18.768976: step 11345, loss 1.18703, acc 0.625\n",
            "2022-05-05T22:00:19.469152: step 11346, loss 1.20802, acc 0.5625\n",
            "2022-05-05T22:00:20.132445: step 11347, loss 1.23497, acc 0.5625\n",
            "2022-05-05T22:00:20.788121: step 11348, loss 1.30874, acc 0.46875\n",
            "2022-05-05T22:00:21.458915: step 11349, loss 1.05685, acc 0.640625\n",
            "2022-05-05T22:00:22.128903: step 11350, loss 1.37852, acc 0.421875\n",
            "2022-05-05T22:00:22.794059: step 11351, loss 1.15613, acc 0.59375\n",
            "2022-05-05T22:00:23.469907: step 11352, loss 1.23035, acc 0.515625\n",
            "2022-05-05T22:00:24.273124: step 11353, loss 1.27955, acc 0.46875\n",
            "2022-05-05T22:00:24.998700: step 11354, loss 1.13063, acc 0.6875\n",
            "2022-05-05T22:00:25.680745: step 11355, loss 1.01512, acc 0.65625\n",
            "2022-05-05T22:00:26.436828: step 11356, loss 1.25719, acc 0.578125\n",
            "2022-05-05T22:00:27.093665: step 11357, loss 1.2894, acc 0.53125\n",
            "2022-05-05T22:00:27.766480: step 11358, loss 1.66918, acc 0.390625\n",
            "2022-05-05T22:00:28.461846: step 11359, loss 1.00087, acc 0.65625\n",
            "2022-05-05T22:00:29.113389: step 11360, loss 1.42622, acc 0.40625\n",
            "2022-05-05T22:00:29.784900: step 11361, loss 1.09329, acc 0.5625\n",
            "2022-05-05T22:00:30.440391: step 11362, loss 1.18885, acc 0.578125\n",
            "2022-05-05T22:00:31.102481: step 11363, loss 1.28741, acc 0.421875\n",
            "2022-05-05T22:00:31.819659: step 11364, loss 1.12297, acc 0.59375\n",
            "2022-05-05T22:00:32.521900: step 11365, loss 1.49187, acc 0.46875\n",
            "2022-05-05T22:00:33.192047: step 11366, loss 1.34351, acc 0.578125\n",
            "2022-05-05T22:00:33.858815: step 11367, loss 1.26714, acc 0.46875\n",
            "2022-05-05T22:00:34.526008: step 11368, loss 1.38579, acc 0.421875\n",
            "2022-05-05T22:00:35.191482: step 11369, loss 1.40059, acc 0.453125\n",
            "2022-05-05T22:00:35.877965: step 11370, loss 1.112, acc 0.625\n",
            "2022-05-05T22:00:36.539984: step 11371, loss 1.33443, acc 0.484375\n",
            "2022-05-05T22:00:37.209359: step 11372, loss 1.34929, acc 0.515625\n",
            "2022-05-05T22:00:37.894458: step 11373, loss 1.38623, acc 0.46875\n",
            "2022-05-05T22:00:38.551610: step 11374, loss 1.23282, acc 0.5625\n",
            "2022-05-05T22:00:39.223292: step 11375, loss 1.38583, acc 0.578125\n",
            "2022-05-05T22:00:39.901829: step 11376, loss 1.18963, acc 0.65625\n",
            "2022-05-05T22:00:40.556908: step 11377, loss 1.12238, acc 0.5625\n",
            "2022-05-05T22:00:41.227241: step 11378, loss 1.31462, acc 0.484375\n",
            "2022-05-05T22:00:41.919655: step 11379, loss 1.31836, acc 0.546875\n",
            "2022-05-05T22:00:42.593703: step 11380, loss 1.13608, acc 0.59375\n",
            "2022-05-05T22:00:43.269476: step 11381, loss 1.12244, acc 0.625\n",
            "2022-05-05T22:00:43.945628: step 11382, loss 1.41307, acc 0.453125\n",
            "2022-05-05T22:00:44.605951: step 11383, loss 1.15302, acc 0.671875\n",
            "2022-05-05T22:00:45.286645: step 11384, loss 1.37718, acc 0.453125\n",
            "2022-05-05T22:00:45.944786: step 11385, loss 1.34301, acc 0.59375\n",
            "2022-05-05T22:00:46.638276: step 11386, loss 1.30088, acc 0.484375\n",
            "2022-05-05T22:00:47.310137: step 11387, loss 1.27044, acc 0.515625\n",
            "2022-05-05T22:00:47.971530: step 11388, loss 1.26167, acc 0.546875\n",
            "2022-05-05T22:00:48.651780: step 11389, loss 1.30471, acc 0.5625\n",
            "2022-05-05T22:00:49.336068: step 11390, loss 1.33834, acc 0.5625\n",
            "2022-05-05T22:00:49.992055: step 11391, loss 1.17458, acc 0.609375\n",
            "2022-05-05T22:00:50.691050: step 11392, loss 1.25436, acc 0.5625\n",
            "2022-05-05T22:00:51.365143: step 11393, loss 1.33074, acc 0.640625\n",
            "2022-05-05T22:00:52.022108: step 11394, loss 1.0621, acc 0.59375\n",
            "2022-05-05T22:00:52.710215: step 11395, loss 1.27629, acc 0.5625\n",
            "2022-05-05T22:00:53.396585: step 11396, loss 1.35419, acc 0.515625\n",
            "2022-05-05T22:00:54.050158: step 11397, loss 1.24886, acc 0.546875\n",
            "2022-05-05T22:00:54.730153: step 11398, loss 1.4447, acc 0.421875\n",
            "2022-05-05T22:00:55.410346: step 11399, loss 1.32082, acc 0.515625\n",
            "2022-05-05T22:00:56.082205: step 11400, loss 1.23012, acc 0.578125\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T22:00:56.246324: step 11400, loss 2.08226, acc 0.34\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-11400\n",
            "\n",
            "2022-05-05T22:00:57.221807: step 11401, loss 1.30243, acc 0.546875\n",
            "2022-05-05T22:00:57.892878: step 11402, loss 1.21211, acc 0.625\n",
            "2022-05-05T22:00:58.583570: step 11403, loss 1.3263, acc 0.515625\n",
            "2022-05-05T22:00:59.239465: step 11404, loss 1.67553, acc 0.40625\n",
            "2022-05-05T22:00:59.908266: step 11405, loss 1.41356, acc 0.5\n",
            "2022-05-05T22:01:00.601819: step 11406, loss 1.3254, acc 0.46875\n",
            "2022-05-05T22:01:01.266249: step 11407, loss 1.48508, acc 0.453125\n",
            "2022-05-05T22:01:01.989031: step 11408, loss 1.38951, acc 0.46875\n",
            "2022-05-05T22:01:02.650509: step 11409, loss 1.28051, acc 0.484375\n",
            "2022-05-05T22:01:03.326774: step 11410, loss 1.28622, acc 0.578125\n",
            "2022-05-05T22:01:04.005665: step 11411, loss 1.22607, acc 0.578125\n",
            "2022-05-05T22:01:04.663333: step 11412, loss 1.30157, acc 0.484375\n",
            "2022-05-05T22:01:05.332509: step 11413, loss 1.29579, acc 0.5625\n",
            "2022-05-05T22:01:06.003038: step 11414, loss 1.2069, acc 0.5625\n",
            "2022-05-05T22:01:06.695856: step 11415, loss 1.3967, acc 0.515625\n",
            "2022-05-05T22:01:07.364155: step 11416, loss 1.16382, acc 0.625\n",
            "2022-05-05T22:01:08.050261: step 11417, loss 1.34731, acc 0.46875\n",
            "2022-05-05T22:01:08.730802: step 11418, loss 1.3618, acc 0.46875\n",
            "2022-05-05T22:01:09.412554: step 11419, loss 1.28764, acc 0.578125\n",
            "2022-05-05T22:01:10.090339: step 11420, loss 1.25468, acc 0.546875\n",
            "2022-05-05T22:01:10.759287: step 11421, loss 1.35768, acc 0.453125\n",
            "2022-05-05T22:01:11.433908: step 11422, loss 1.51105, acc 0.453125\n",
            "2022-05-05T22:01:12.100525: step 11423, loss 1.04845, acc 0.671875\n",
            "2022-05-05T22:01:12.782618: step 11424, loss 1.33105, acc 0.515625\n",
            "2022-05-05T22:01:13.462244: step 11425, loss 1.45983, acc 0.5\n",
            "2022-05-05T22:01:14.167301: step 11426, loss 1.37795, acc 0.515625\n",
            "2022-05-05T22:01:14.829058: step 11427, loss 1.14925, acc 0.671875\n",
            "2022-05-05T22:01:15.502809: step 11428, loss 1.16806, acc 0.625\n",
            "2022-05-05T22:01:16.193761: step 11429, loss 1.18316, acc 0.546875\n",
            "2022-05-05T22:01:16.859138: step 11430, loss 1.34487, acc 0.515625\n",
            "2022-05-05T22:01:17.548427: step 11431, loss 1.19179, acc 0.578125\n",
            "2022-05-05T22:01:18.233242: step 11432, loss 1.11654, acc 0.578125\n",
            "2022-05-05T22:01:18.897737: step 11433, loss 1.05124, acc 0.578125\n",
            "2022-05-05T22:01:19.575636: step 11434, loss 1.14551, acc 0.546875\n",
            "2022-05-05T22:01:20.230867: step 11435, loss 1.1566, acc 0.578125\n",
            "2022-05-05T22:01:20.901946: step 11436, loss 1.09161, acc 0.609375\n",
            "2022-05-05T22:01:21.585541: step 11437, loss 1.34234, acc 0.484375\n",
            "2022-05-05T22:01:22.251561: step 11438, loss 1.34251, acc 0.453125\n",
            "2022-05-05T22:01:22.946129: step 11439, loss 1.27417, acc 0.515625\n",
            "2022-05-05T22:01:23.619441: step 11440, loss 1.06738, acc 0.703125\n",
            "2022-05-05T22:01:24.273627: step 11441, loss 1.30122, acc 0.5625\n",
            "2022-05-05T22:01:24.950493: step 11442, loss 1.38662, acc 0.5\n",
            "2022-05-05T22:01:25.632782: step 11443, loss 1.28064, acc 0.5\n",
            "2022-05-05T22:01:26.295970: step 11444, loss 1.59351, acc 0.4375\n",
            "2022-05-05T22:01:26.993172: step 11445, loss 1.28076, acc 0.5625\n",
            "2022-05-05T22:01:27.661539: step 11446, loss 1.3511, acc 0.515625\n",
            "2022-05-05T22:01:28.318249: step 11447, loss 1.12504, acc 0.609375\n",
            "2022-05-05T22:01:28.987616: step 11448, loss 1.3044, acc 0.546875\n",
            "2022-05-05T22:01:29.641857: step 11449, loss 1.35155, acc 0.5625\n",
            "2022-05-05T22:01:30.313210: step 11450, loss 1.46093, acc 0.484375\n",
            "2022-05-05T22:01:30.994112: step 11451, loss 1.25543, acc 0.609375\n",
            "2022-05-05T22:01:31.654531: step 11452, loss 1.28084, acc 0.515625\n",
            "2022-05-05T22:01:32.320435: step 11453, loss 1.41923, acc 0.421875\n",
            "2022-05-05T22:01:32.995319: step 11454, loss 1.31226, acc 0.53125\n",
            "2022-05-05T22:01:33.653458: step 11455, loss 1.28179, acc 0.546875\n",
            "2022-05-05T22:01:34.324132: step 11456, loss 1.39841, acc 0.46875\n",
            "2022-05-05T22:01:34.998321: step 11457, loss 1.39964, acc 0.46875\n",
            "2022-05-05T22:01:35.645219: step 11458, loss 1.42489, acc 0.515625\n",
            "2022-05-05T22:01:36.310050: step 11459, loss 1.20459, acc 0.53125\n",
            "2022-05-05T22:01:36.976935: step 11460, loss 1.1025, acc 0.578125\n",
            "2022-05-05T22:01:37.625875: step 11461, loss 1.29809, acc 0.578125\n",
            "2022-05-05T22:01:38.302530: step 11462, loss 1.28214, acc 0.5625\n",
            "2022-05-05T22:01:38.952808: step 11463, loss 1.34879, acc 0.453125\n",
            "2022-05-05T22:01:39.637166: step 11464, loss 1.28725, acc 0.53125\n",
            "2022-05-05T22:01:40.311975: step 11465, loss 1.22621, acc 0.546875\n",
            "2022-05-05T22:01:40.969603: step 11466, loss 1.08853, acc 0.59375\n",
            "2022-05-05T22:01:41.656174: step 11467, loss 1.25198, acc 0.484375\n",
            "2022-05-05T22:01:42.317916: step 11468, loss 1.1141, acc 0.625\n",
            "2022-05-05T22:01:42.992671: step 11469, loss 1.1899, acc 0.5\n",
            "2022-05-05T22:01:43.658484: step 11470, loss 1.3432, acc 0.515625\n",
            "2022-05-05T22:01:44.318725: step 11471, loss 1.33794, acc 0.53125\n",
            "2022-05-05T22:01:44.979499: step 11472, loss 1.3788, acc 0.484375\n",
            "2022-05-05T22:01:45.649085: step 11473, loss 1.12052, acc 0.609375\n",
            "2022-05-05T22:01:46.322641: step 11474, loss 1.47235, acc 0.515625\n",
            "2022-05-05T22:01:46.999573: step 11475, loss 1.41947, acc 0.515625\n",
            "2022-05-05T22:01:47.662841: step 11476, loss 1.18762, acc 0.5\n",
            "2022-05-05T22:01:48.317238: step 11477, loss 1.39558, acc 0.46875\n",
            "2022-05-05T22:01:48.996261: step 11478, loss 1.39029, acc 0.421875\n",
            "2022-05-05T22:01:49.661670: step 11479, loss 1.44199, acc 0.46875\n",
            "2022-05-05T22:01:50.329505: step 11480, loss 1.24133, acc 0.53125\n",
            "2022-05-05T22:01:50.995817: step 11481, loss 1.53378, acc 0.390625\n",
            "2022-05-05T22:01:51.673328: step 11482, loss 1.25434, acc 0.53125\n",
            "2022-05-05T22:01:52.324889: step 11483, loss 1.29236, acc 0.484375\n",
            "2022-05-05T22:01:53.011523: step 11484, loss 1.43435, acc 0.5\n",
            "2022-05-05T22:01:53.678753: step 11485, loss 1.38097, acc 0.5\n",
            "2022-05-05T22:01:54.341498: step 11486, loss 1.38792, acc 0.53125\n",
            "2022-05-05T22:01:55.004354: step 11487, loss 1.28374, acc 0.5625\n",
            "2022-05-05T22:01:55.662156: step 11488, loss 1.37482, acc 0.53125\n",
            "2022-05-05T22:01:56.341025: step 11489, loss 1.18119, acc 0.578125\n",
            "2022-05-05T22:01:57.022292: step 11490, loss 1.22763, acc 0.53125\n",
            "2022-05-05T22:01:57.678617: step 11491, loss 1.54351, acc 0.484375\n",
            "2022-05-05T22:01:58.352051: step 11492, loss 1.44155, acc 0.484375\n",
            "2022-05-05T22:01:59.034471: step 11493, loss 1.36514, acc 0.484375\n",
            "2022-05-05T22:01:59.692844: step 11494, loss 1.17565, acc 0.546875\n",
            "2022-05-05T22:02:00.361666: step 11495, loss 1.31805, acc 0.515625\n",
            "2022-05-05T22:02:01.026322: step 11496, loss 1.44639, acc 0.453125\n",
            "2022-05-05T22:02:01.699306: step 11497, loss 1.17653, acc 0.5625\n",
            "2022-05-05T22:02:02.376594: step 11498, loss 1.41299, acc 0.578125\n",
            "2022-05-05T22:02:03.031836: step 11499, loss 1.46906, acc 0.5\n",
            "2022-05-05T22:02:03.723027: step 11500, loss 1.3152, acc 0.5\n",
            "\n",
            "Evaluation:\n",
            "2022-05-05T22:02:03.876789: step 11500, loss 2.07267, acc 0.33\n",
            "\n",
            "Saved model checkpoint to /content/runs/1651775402/checkpoints/model-11500\n",
            "\n",
            "2022-05-05T22:02:04.775498: step 11501, loss 1.40528, acc 0.390625\n",
            "2022-05-05T22:02:05.444074: step 11502, loss 1.4093, acc 0.546875\n",
            "2022-05-05T22:02:06.112504: step 11503, loss 1.25159, acc 0.515625\n",
            "2022-05-05T22:02:06.783591: step 11504, loss 1.39145, acc 0.484375\n",
            "2022-05-05T22:02:07.459807: step 11505, loss 1.23136, acc 0.59375\n",
            "2022-05-05T22:02:08.127705: step 11506, loss 1.30561, acc 0.546875\n",
            "2022-05-05T22:02:08.796938: step 11507, loss 1.22503, acc 0.609375\n",
            "2022-05-05T22:02:09.518700: step 11508, loss 1.47523, acc 0.40625\n",
            "2022-05-05T22:02:10.162680: step 11509, loss 1.30163, acc 0.5625\n",
            "2022-05-05T22:02:10.826737: step 11510, loss 1.16402, acc 0.59375\n",
            "2022-05-05T22:02:11.488169: step 11511, loss 1.46275, acc 0.5\n",
            "2022-05-05T22:02:12.151096: step 11512, loss 1.01563, acc 0.6875\n",
            "2022-05-05T22:02:12.843176: step 11513, loss 1.15627, acc 0.65625\n",
            "2022-05-05T22:02:13.500457: step 11514, loss 1.40392, acc 0.40625\n",
            "2022-05-05T22:02:14.174356: step 11515, loss 1.41151, acc 0.4375\n",
            "2022-05-05T22:02:14.843440: step 11516, loss 1.27361, acc 0.53125\n",
            "2022-05-05T22:02:15.499576: step 11517, loss 1.2949, acc 0.59375\n",
            "2022-05-05T22:02:16.155421: step 11518, loss 1.34333, acc 0.5\n",
            "2022-05-05T22:02:16.824942: step 11519, loss 1.50716, acc 0.40625\n",
            "2022-05-05T22:02:17.476120: step 11520, loss 1.2549, acc 0.53125\n",
            "2022-05-05T22:02:18.146722: step 11521, loss 1.21463, acc 0.515625\n",
            "2022-05-05T22:02:18.830158: step 11522, loss 1.37043, acc 0.484375\n",
            "2022-05-05T22:02:19.490781: step 11523, loss 1.60086, acc 0.515625\n",
            "2022-05-05T22:02:20.157411: step 11524, loss 1.31777, acc 0.484375\n",
            "2022-05-05T22:02:20.832336: step 11525, loss 1.29345, acc 0.53125\n",
            "2022-05-05T22:02:21.491112: step 11526, loss 0.944766, acc 0.671875\n",
            "2022-05-05T22:02:22.156414: step 11527, loss 1.35781, acc 0.484375\n",
            "2022-05-05T22:02:22.832877: step 11528, loss 1.45169, acc 0.421875\n",
            "2022-05-05T22:02:23.482249: step 11529, loss 1.21506, acc 0.515625\n",
            "2022-05-05T22:02:24.152452: step 11530, loss 1.20211, acc 0.5625\n",
            "2022-05-05T22:02:24.803984: step 11531, loss 1.32131, acc 0.5625\n",
            "2022-05-05T22:02:25.477435: step 11532, loss 1.11099, acc 0.578125\n",
            "2022-05-05T22:02:26.154704: step 11533, loss 1.1646, acc 0.5625\n",
            "2022-05-05T22:02:26.812189: step 11534, loss 1.24651, acc 0.59375\n",
            "2022-05-05T22:02:27.485532: step 11535, loss 1.24104, acc 0.484375\n",
            "2022-05-05T22:02:28.155035: step 11536, loss 1.28982, acc 0.5\n",
            "2022-05-05T22:02:28.805881: step 11537, loss 1.35365, acc 0.484375\n",
            "2022-05-05T22:02:29.470625: step 11538, loss 1.34539, acc 0.5625\n",
            "2022-05-05T22:02:30.144252: step 11539, loss 1.11503, acc 0.5625\n",
            "2022-05-05T22:02:30.802586: step 11540, loss 1.26555, acc 0.515625\n",
            "2022-05-05T22:02:31.471731: step 11541, loss 1.45481, acc 0.484375\n",
            "2022-05-05T22:02:32.131081: step 11542, loss 1.22336, acc 0.578125\n",
            "2022-05-05T22:02:32.803767: step 11543, loss 1.17506, acc 0.546875\n",
            "2022-05-05T22:02:33.483673: step 11544, loss 1.51019, acc 0.390625\n",
            "2022-05-05T22:02:34.148555: step 11545, loss 1.18849, acc 0.578125\n",
            "2022-05-05T22:02:34.812251: step 11546, loss 1.2219, acc 0.625\n",
            "2022-05-05T22:02:35.478860: step 11547, loss 1.37069, acc 0.515625\n",
            "2022-05-05T22:02:36.130907: step 11548, loss 1.4225, acc 0.46875\n",
            "2022-05-05T22:02:36.803797: step 11549, loss 1.21585, acc 0.453125\n",
            "2022-05-05T22:02:37.461576: step 11550, loss 1.5188, acc 0.484375\n",
            "2022-05-05T22:02:38.117931: step 11551, loss 1.39887, acc 0.46875\n",
            "2022-05-05T22:02:38.800201: step 11552, loss 1.20683, acc 0.546875\n",
            "2022-05-05T22:02:39.445628: step 11553, loss 1.3635, acc 0.453125\n",
            "2022-05-05T22:02:40.118809: step 11554, loss 1.34495, acc 0.5\n",
            "2022-05-05T22:02:40.820769: step 11555, loss 1.25531, acc 0.546875\n",
            "2022-05-05T22:02:41.470445: step 11556, loss 1.39098, acc 0.53125\n",
            "2022-05-05T22:02:42.142919: step 11557, loss 1.30297, acc 0.46875\n",
            "2022-05-05T22:02:42.827801: step 11558, loss 1.28853, acc 0.53125\n",
            "2022-05-05T22:02:43.479338: step 11559, loss 1.40185, acc 0.46875\n",
            "2022-05-05T22:02:44.041879: step 11560, loss 1.26796, acc 0.571429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uqSCdryEUHeg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}